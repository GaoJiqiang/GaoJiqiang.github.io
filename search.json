[{"title":"在加密数据上进行神经网络的训练","url":"/2020/11/18/%E5%9C%A8%E5%8A%A0%E5%AF%86%E6%95%B0%E6%8D%AE%E4%B8%8A%E8%BF%9B%E8%A1%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83/","content":"<h1 id=\"在加密数据上进行神经网络的训练\"><a href=\"#在加密数据上进行神经网络的训练\" class=\"headerlink\" title=\"在加密数据上进行神经网络的训练\"></a>在加密数据上进行神经网络的训练</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>本文介绍一下目前如何使用加密数据进行神经网络的训练，并简要介绍各种应用场景、已有工具框架等内容。</p>\n<h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>当前，基于云的神经网络服务部署逐渐成为主流，在这种情况下，数据和模型由不同方拥有。但是，MLaaS的场景下会产生许多数据隐私问题。举个简单例子来讲，第三方开发了一个深度学习预测模型，对患者的医学数据进行某种疾病的检测。由于法律法规和个人隐私需求的限制，医院无法直接传输明文医学数据给第三方用于模型输入，也不应将检测结果暴露给患者以外的第三方。通过同态加密（HE），医院可以发送加密数据，使得第三方在加密数据上运行模型，而无需透露任何基础信息。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20201118142642144.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Bkc3d6ZA==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>在过去一段时间，这种基于HE的机器学习方法逐渐成为研究重点，另一种技术主要是安全多方计算（MPC）。HE非常适合使用深度神经网络进行预测的任务。</p>\n<h2 id=\"一些应用场景\"><a href=\"#一些应用场景\" class=\"headerlink\" title=\"一些应用场景\"></a>一些应用场景</h2><p>数据拥有者（DO）和云模型提供者（Cloud）</p>\n<ol>\n<li>加密的数据，未加密的模型：DO将HE加密的数据发送到Cloud。然后，Cloud根据加密数据计算模型以产生加密输出，将输出发送给DO，DO使用私钥对其进行解密。</li>\n<li>未加密的数据，加密的模型：Cloud将已加密的模型发送给DO，然后DO在不现实任何数据的情况下在本地运行模型，以生成加密的输出。DO不会获得任何有关模型的任何信息，并且可以由密钥的所有者（如模型的所有者）解密输出。</li>\n<li>加密数据，加密模型：一个DO或多个DO将数据联合起来输入加密网络进行预测，Cloud返回加密预测结果，需要多个密钥才能解密。类似于联邦学习。</li>\n</ol>\n<p><a href=\"https://medium.com/swlh/faster-neural-networks-on-encrypted-data-with-intel-he-transformer-and-tensorflow-9fdc9eb1a888\">https://medium.com/swlh/faster-neural-networks-on-encrypted-data-with-intel-he-transformer-and-tensorflow-9fdc9eb1a888</a><br>第一个应用场景是最具有代表性的，因为它是MLaaS在同态加密下的直接应用。与MPC相比，HE的优势在于不需要维持通信来进行计算，但是缺点也同样明显，那就是计算量的代价、可计算函数的局限性、同态乘法的误差增长。总体而言，HE方案的主要瓶颈是计算能力，而MPC则是通信。</p>\n<h2 id=\"什么是同态加密\"><a href=\"#什么是同态加密\" class=\"headerlink\" title=\"什么是同态加密\"></a>什么是同态加密</h2><p>同态加密（HE，homomorphic encryption）是密码学里一种特殊的加密模式，同态加密使我们可以将加密后的密文发给任意的第三方进行计算，并且在计算前不需要解密，即：在密文上进行计算。 虽然同态加密的概念最早出现于30年前，但是第一个支持在密文上进行任意运算的<strong>全同态加密</strong>框架出现较晚，在2009年由Craig Gentry提出。</p>\n<p><strong>同态加密的分类</strong></p>\n<ol>\n<li>部分同态加密（<strong>PHE</strong>）指同态加密算法只对加法或乘法（其中一种）有同态的性质。<strong>PHE的优点是原理简单、易实现，缺点是仅支持一种运算（加法或乘法）</strong>。可以应用在<strong>联邦学习</strong>中服务器的聚合操作。</li>\n<li>层次同态加密算法（<strong>LHE</strong>）一般支持有限次数的加法和乘法运算。<strong>LHE的优点是同时支持加法和乘法，并且因为出现时间比PHE晚，所以技术更加成熟、一般效率比FHE要高很多、和PHE效率接近或高于PHE，缺点是支持的计算次数有限。</strong></li>\n<li>全同态加密算法（<strong>FHE</strong>）支持在密文上进行无限次数的、任意类型的计算。<strong>FHE的优点是支持的算子多并且运算次数没有限制，缺点是效率很低，目前还无法支撑大规模的计算。</strong></li>\n<li>基于格的同态加密算法（<strong>RLWE</strong>）支持有限次数的加法和乘法运算。<strong>RLWE的有点是密文结果较短，效率较与传统方法要好，缺点是该问题在密文中添加了噪声项，在加法特别是乘法期间，噪声项迅速增长，会导致最终无法再解密。</strong></li>\n</ol>\n<p>下面两篇工作是比较知名的用HE来做ML的安全论文。<br><a href=\"https://eprint.iacr.org/2018/073.pdf\">USENIX-18 GAZELLE: A Low Latency Framework for SecureNeural Network Inference</a><br><a href=\"https://eprint.iacr.org/2019/524\">Efficient Multi-Key Homomorphic Encryption with Packed Ciphertexts with Application to Oblivious Neural Network Inference</a></p>\n<h2 id=\"同态加密在机器学习中的应用\"><a href=\"#同态加密在机器学习中的应用\" class=\"headerlink\" title=\"同态加密在机器学习中的应用\"></a>同态加密在机器学习中的应用</h2><h3 id=\"1-联邦学习（PHE）\"><a href=\"#1-联邦学习（PHE）\" class=\"headerlink\" title=\"1.联邦学习（PHE）\"></a>1.联邦学习（PHE）</h3><p>在联邦学习中，多方联合训练模型一般需要交换中间结果，如果直接发送明文的结果可能会有隐私泄露风险。在这种场景下，同态加密就可以发挥很重要的作用。多方直接将中间结果用同态加密算法进行加密，然后发送给第三方进行聚合，再将聚合的结果返回给所有参与者，不仅保证了中间结果没有泄露，还完成了训练任务（第三方可以通过优化系统设计去除）。</p>\n<p>在联邦学习中，因为只需要对中间结果或模型进行聚合，一般使用的同态加密算法为PHE（多见为加法同态加密算法），例如在<a href=\"https://github.com/FederatedAI/FATE\">FATE</a>中使用的Paillier即为加法同态加密算法。为了更好地展示同态加密在联邦学习中的应用，我们在此展示一个同态加密在联邦学习推荐系统中的应用。<img src=\"https://img-blog.csdnimg.cn/20201118155633500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Bkc3d6ZA==,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"><br>在传统的推荐系统中，用户需要上传浏览记录、评价信息来实现个性化推荐，但是这些信息均属于个人的隐私数据，直接上传会带来很大的安全隐患。在联邦推荐系统中，每个用户将数据保存在本地，只上传特定的模型梯度。这样虽然避免了隐私数据的直接泄露，但是还是透露了梯度信息给云服务器。同时我们发现，从数学上可以证明，使用连续两次更新的梯度即可反推出用户的评分信息。这种情况下，就必须使用同态加密对用户上传的梯度进行保护，即用户在上传梯度前使用加法同态加密算法对梯度信息进行加密，然后云服务器将所有用户的密文梯度进行聚合（相加），再将更新后的模型返还给各个用户解密，完成训练更新。</p>\n<p>这个框架目前使用的公私钥加密方案是存在问题的，当server和其中一个client进行共谋时，私钥会泄露。因此后续可以采用私钥秘密分享的方法进行设计。</p>\n<h3 id=\"2-密态机器学习（LHE和FHE）\"><a href=\"#2-密态机器学习（LHE和FHE）\" class=\"headerlink\" title=\"2. 密态机器学习（LHE和FHE）\"></a>2. 密态机器学习（LHE和FHE）</h3><p>密态计算中使用的同态加密算法多为LHE和FHE。其实全同态加密研究的初衷，就是为了实现安全的云计算，即对云算力有需求的用户可以将本地的数据全部加密，然后上传到云端，然后云端的服务器即可按照用户指令完成计算，整个过程用户的数据不会泄露给云端，从而完成“绝对安全”的云计算服务。</p>\n<p>但是由于目前FHE效率比较低，所以使用全同态加密进行云计算远远没有达到应用的级别。机器学习在云计算中有着广阔的市场，而机器学习有训练和推理两种需求，训练过程一般数据较多、计算量很大，而推理则数据量相对较小、计算量也小，所以目前研究主要集中在密态下的机器学习推理，并且目前已经有速度比较快的方案 （USENIX-18 GAZELLE）；而密态下的机器学习训练研究稀少，是一个比较难解决的问题。</p>\n<h2 id=\"一些开源的密态机器学习方案实现\"><a href=\"#一些开源的密态机器学习方案实现\" class=\"headerlink\" title=\"一些开源的密态机器学习方案实现\"></a>一些开源的密态机器学习方案实现</h2><ol>\n<li><strong><a href=\"https://github.com/IntelAI/he-transformer\">IntelAI/he-transformer</a>：</strong><br>对加密数据进行本地机器学习，支持多种加密模式，如Microsoft的<a href=\"https://github.com/microsoft/SEAL\">SEAL-CKKS</a>的同态方案和<a href=\"https://github.com/encryptogroup/ABY\">ABY</a>的MPC方案。并有开源实现和论文支持。</li>\n<li><strong><a href=\"https://github.com/facebookresearch/CrypTen\">Facebook/CrypTen</a>：</strong><br>主要是采用了安全多方计算来实现数据隐私保护下的机器学习任务。目前还在原型阶段。</li>\n</ol>\n","categories":["secure ML"]},{"title":"DeepFaceLab使用简介","url":"/2021/01/25/DeepFaceLab/","content":"<h1 id=\"DeepFaceLab-运行步骤\"><a href=\"#DeepFaceLab-运行步骤\" class=\"headerlink\" title=\"DeepFaceLab 运行步骤\"></a>DeepFaceLab 运行步骤</h1><blockquote>\n<p>若要把A（某明星）的脸换在B（某普通人）头上，那么A是src，B是dst。 A–&gt;B</p>\n</blockquote>\n<h2 id=\"一、Video–-gt-Frame\"><a href=\"#一、Video–-gt-Frame\" class=\"headerlink\" title=\"一、Video–&gt;Frame\"></a>一、Video–&gt;Frame</h2><p>视频转换成帧：默认情况下一秒截30个frame，deepfake实质上是在每一帧上进行人脸的替换融合，因此第一步需要将视频转为帧图片。并保存在data_src和data_dst文件下。</p>\n<p><img src=\"dfl-2021-01-25-14-59-16.png\"></p>\n<h2 id=\"二、Extraction\"><a href=\"#二、Extraction\" class=\"headerlink\" title=\"二、Extraction\"></a>二、Extraction</h2><p>人脸提取，对视频帧来说，并不是每一帧图片中都含有人脸，因此需要在这一步将含有人脸的frame给挑选出来。</p>\n<p><img src=\"dfl-2021-01-25-15-32-58.png\"></p>\n<ul>\n<li><p>Face detection：第一步是人脸检测，对data_src和data_dst中的图片进行人脸检测，用到的模型有<strong>S3FD、RetinaFace、MTCNN</strong>，默认情况下使用的是S3FD。</p>\n</li>\n<li><p>Face Alignment：第二步是人脸对齐，需要用到landmarks对人脸关键点进行定位，提供了两种选择，1）是<strong>2DFAN</strong>；2）是<strong>PRNet</strong>。处理后的图片存到了aligned文件夹下。</p>\n</li>\n<li><p>Face Segmentation：第三步是可选，做人脸分割，将人脸不同部位做切分，为了使得模型在人脸戴有眼镜和手部遮挡等障碍物时更加robust，这里引入了<strong>XSeg</strong>，允许在小数据集上做切割。</p>\n</li>\n</ul>\n<p><img src=\"dfl-2021-01-25-15-33-26.png\"></p>\n<h2 id=\"三、Training\"><a href=\"#三、Training\" class=\"headerlink\" title=\"三、Training\"></a>三、Training</h2><p>训练这一步是最关键的，为了解决src和dst人脸不严格匹配的问题，采用共享<strong>Encoder</strong>权重的方法实现泛化，这里提供了两种训练架构，<strong>DF</strong>和<strong>LIAE</strong>。后面的GAN是可选的，源码默认的应该是不带GAN的DF框架。</p>\n<p><img src=\"dfl-2021-01-25-15-48-16.png\"></p>\n<p><img src=\"dfl-2021-01-25-17-17-36.png\"></p>\n<h2 id=\"四、Conversion\"><a href=\"#四、Conversion\" class=\"headerlink\" title=\"四、Conversion\"></a>四、Conversion</h2><ul>\n<li><p>转化这一步首先需要把src通过dst的decoder生成的face根据对齐步骤生成的mask放到dst对应的人脸部位；</p>\n</li>\n<li><p>接下来是<strong>blending</strong>步骤，达到肤色融合的目的，DFL提供了多种肤色融合算法，如RCT、IDT，定义了<strong>泊松融合</strong>优化问题；</p>\n</li>\n<li><p>接下来是<strong>sharpening</strong>锐化，让生成的图片的纹理更加明显一些。</p>\n</li>\n</ul>\n<p>到此为止，就得到了换脸后的图片。</p>\n<p><img src=\"dfl-2021-01-25-17-18-22.png\"></p>\n<p><img src=\"dfl-2021-01-25-16-04-09.png\"></p>\n<h2 id=\"五、Frame–-gt-Video\"><a href=\"#五、Frame–-gt-Video\" class=\"headerlink\" title=\"五、Frame–&gt;Video\"></a>五、Frame–&gt;Video</h2><p>最后一步，将换脸后的图片进行压缩形成视频。</p>\n<p><img src=\"dfl-2021-01-25-17-18-33.png\"></p>\n","categories":["DeepFace, AI Security"]},{"title":"FL工业框架落地调研","url":"/2021/01/24/FL%E8%90%BD%E5%9C%B0%E6%A1%86%E6%9E%B6%E8%B0%83%E7%A0%94/","content":"<h1 id=\"FL工业框架落地调研\"><a href=\"#FL工业框架落地调研\" class=\"headerlink\" title=\"FL工业框架落地调研\"></a>FL工业框架落地调研</h1><h2 id=\"一、FedML\"><a href=\"#一、FedML\" class=\"headerlink\" title=\"一、FedML\"></a>一、FedML</h2><blockquote>\n<p>简介视频：<a href=\"https://www.bilibili.com/video/BV1jK411N7gS/\">https://www.bilibili.com/video/BV1jK411N7gS/</a> \\<br>项目官网：<a href=\"https://fedml.ai/\">https://fedml.ai/</a> \\<br>论文地址：<a href=\"https://arxiv.org/pdf/2007.13518.pdf\">https://arxiv.org/pdf/2007.13518.pdf</a> \\<br>git地址：<a href=\"https://github.com/FedML-AI/FedML\">https://github.com/FedML-AI/FedML</a></p>\n</blockquote>\n<p><strong>FedML是一个提供科研和工业开发的框架，其中包含了目前FL应有尽有的一些研究方向，事科研人员更容易上手，并且内部支持多种训练场景，如IoT、移动端、星型式、去中心化式等等。</strong></p>\n<ol>\n<li>FL在现实中的一些挑战：比如每轮本地迭代的次数，和本机的计算能力有关，如何调节不同用户不同计算能力下的迭代轮数？还有比如多方如何训练一个很大的模型？服务器能否加入优化策略？如何筛选更有利于模型收敛的用户参与训练？实质上FL框架下，有很多挑战，只要把其中一组参数变的更加的贴近现实，就能够发现一些能够做的问题。</li>\n</ol>\n<p><img src=\"mk-2021-01-24-16-27-12.png\" alt=\"\"></p>\n<ol>\n<li>不同拓扑结构下：</li>\n</ol>\n<p><img src=\"mk-2021-01-24-16-39-01.png\" alt=\"\"></p>\n<ol>\n<li>FedML的设计框架：</li>\n</ol>\n<p><img src=\"mk-2021-01-24-16-45-42.png\" alt=\"\"></p>\n<blockquote>\n<p>支持多种场景；支持多种算法；如下所示</p>\n</blockquote>\n<p><img src=\"mk-2021-01-24-16-47-58.png\" alt=\"\"></p>\n","categories":["Federated Learning"]},{"title":"语音对抗攻击","url":"/2020/12/17/%E8%AF%AD%E9%9F%B3%E5%AF%B9%E6%8A%97%E8%B0%83%E7%A0%94/","content":"<h1 id=\"语音对抗调研\"><a href=\"#语音对抗调研\" class=\"headerlink\" title=\"语音对抗调研\"></a>语音对抗调研</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>相较于在图像上进行对抗攻击，对语音识别任务进行对抗攻击起步更晚，也更具有挑战性。原因在于语音识别极大依赖于语音信号数据中的频率和时序赖关系，在输入模型识别前需要对语音进行预处理和语音特征提取得到语音频谱图，图像上的对抗攻击可以利用给定可微损失函数的梯度来指导对抗搜索，但是语音上的损失标准一般不可分解，很难用梯度的方法去生成对抗样本。</p>\n<p><img src=\"2020-12-16-10-36-41.png\"></p>\n<p>下面说一下针对语音识别系统对抗攻击的几个标志性工作：</p>\n<p>2015年，Vaidya等人首次尝试针对语音识别系统的扰动攻击，在语音识别系统与处理环节中的特征提取步骤修改输入语音信号，生成人耳无法辨别但能被云隐识别系统正确识别的语音信号，但是实际扰动较大，效果并不好。</p>\n<p>2016年，Calini等人提出了“隐藏语音指令”攻击，设计了基于梯度优化的生成对抗扰动的方法，为了在音频信号特征提取后依然保持对抗性，采用了逆语音特征提取的方法反向得到音频信号。但同样容易被人耳感知。</p>\n<p>2017年，zhang等人提出了“海豚音攻击”。为了让攻击不被人耳察觉，将对抗扰动频率提高到了20Khz以上，虽然不容易被人耳察觉，但是很溶剂被过滤和检测，此外，还需要特定设备发射播放得到超声信号。</p>\n<p>2018年，Calini和Wagner在流行的语音识别系统DeepSpeech上生成了实际意义的对抗样本，借鉴了图像分类的C&amp;W攻击方法并在目标函数中使用了深度学习语音识别模型特有的CTC损失函数。使修改后的音频不易被人耳察觉，同时也能使基于深度学习的识别系统错误识别。但是在噪声环境下鲁棒性较差。</p>\n<p>ps：2018年之前的工作不能称为真正意义上的对抗性样本，因为生成的音频对原始语音的扰动太大，人很容易意识到语音被篡改，并且攻击的语音模型不是基于深度学习训练得到的。</p>\n<h2 id=\"深度学习语音识别框架\"><a href=\"#深度学习语音识别框架\" class=\"headerlink\" title=\"深度学习语音识别框架\"></a>深度学习语音识别框架</h2><h3 id=\"1-end-to-end语音识别模型\"><a href=\"#1-end-to-end语音识别模型\" class=\"headerlink\" title=\"1. end-to-end语音识别模型\"></a>1. end-to-end语音识别模型</h3><p>输入一段.wav格式的音频识别步骤：</p>\n<blockquote>\n<p>1）预处理：会进行解码、降噪等操作，把音频分成较短的帧；<br>2）特征提取：从短帧中提取声学特征，常用MFCC特征(梅尔倒谱系数);<br>3）基于模型的预测：将声学特征作为输入数据，生成预测结果，主流系统通常使用RNNs+CTC损失函数模型<br><img src=\"2020-12-15-11-00-49.png\"></p>\n</blockquote>\n<p>这里有两个地方需要进行了解:<br>一个是<strong>CTC</strong>函数，是一种声学模型，CTC-Loss函数，在设计语音对抗样本Loss函数时可能需要用到；一个是<strong>MFCC</strong>，了解语音信息的特征提取过程，便于设计对抗生成方法。</p>\n<h3 id=\"2-基于分类的语音系统\"><a href=\"#2-基于分类的语音系统\" class=\"headerlink\" title=\"2. 基于分类的语音系统\"></a>2. 基于分类的语音系统</h3><blockquote>\n<p>1）预处理：同上；<br>2）特征提取：一般用CNNs提取audio-level feature和frame-level feature<br>3）基于模型的预测：将声学特征作为输入数据，生成预测结果，模型通常用CNNs<br><img src=\"2020-12-15-17-05-48.png\"></p>\n</blockquote>\n<h3 id=\"3-目前各公司使用的一些语音识别系统：\"><a href=\"#3-目前各公司使用的一些语音识别系统：\" class=\"headerlink\" title=\"3.目前各公司使用的一些语音识别系统：\"></a>3.目前各公司使用的一些语音识别系统：</h3><p><img src=\"2020-12-15-11-39-50.png\"></p>\n<h2 id=\"背景知识\"><a href=\"#背景知识\" class=\"headerlink\" title=\"背景知识\"></a>背景知识</h2><h3 id=\"1-CTC\"><a href=\"#1-CTC\" class=\"headerlink\" title=\"1. CTC\"></a>1. CTC</h3><p>CTC是一种RNN的端到端训练方法，可以让RNN直接对序列数据进行学习，而无需事先标注好训练数据中输入序列和输出序列的映射关系。音频数据很难像文本那样进行分割，因此无法直接使用RNN进行训练。</p>\n<blockquote>\n<p>给定输入序列 X={x1,x2,……xT}以及对应的标签数据 Y={y1,y2,……yU}，目的是找到X到Y的一个映射，这种对时序数据进行分类的算法叫做Temporal Classification。</p>\n</blockquote>\n<p>CTC提供了解决方案，对于一个给定的输入序列 X ，CTC给出所有可能的 Y 的输出分布。根据这个分布，我们可以输出最可能的结果或者给出某个输出的概率。 $Y^*=argmax_yP(Y|X)$</p>\n<p><strong>1.1 对齐</strong></p>\n<p>在CTC中，多个输出路径会对应一个输出结果，输入X是”CAT”的语音，输出Y是文本[C,A,T],音频做切割后，每个时间片得到一个输出。因此X和Y之间的映射是多对一的，在对齐时要考虑<strong>去重</strong>和引入<strong>空白字符</strong>。</p>\n<ul>\n<li>HHHEE_LL_LLLOOO    去重</li>\n<li>HE_L_LO            去除空白字符</li>\n<li>HELLO 完成     </li>\n</ul>\n<p><strong>1.2 CTC loss数学推导</strong></p>\n<p>略，太长只过了一遍<br><a href=\"https://www.cnblogs.com/shiyublog/p/10493348.html\">https://www.cnblogs.com/shiyublog/p/10493348.html</a></p>\n<p><strong>1.3 结果搜索</strong></p>\n<p>贪心搜索是选取每一帧预测概率最大的那一项作为结果，此外还有束搜索，和基于动态规划的搜索。</p>\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><h3 id=\"场景\"><a href=\"#场景\" class=\"headerlink\" title=\"场景\"></a>场景</h3><p><strong>1. speech-to-text</strong></p>\n<table>\n<thead>\n<tr>\n<th>input</th>\n<th>output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>正常语音+扰动</td>\n<td>目标语义</td>\n</tr>\n<tr>\n<td>正常语音+扰动</td>\n<td>空语义</td>\n</tr>\n<tr>\n<td>随机噪音+扰动</td>\n<td>目标语义</td>\n</tr>\n</tbody></table>\n<p><strong>2. speech commands classification</strong></p>\n<table>\n<thead>\n<tr>\n<th>input</th>\n<th>output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>正常语音+扰动</td>\n<td>目标分类</td>\n</tr>\n<tr>\n<td>正常语音+扰动</td>\n<td>随机分类</td>\n</tr>\n<tr>\n<td>随机噪音+扰动</td>\n<td>目标分类</td>\n</tr>\n</tbody></table>\n<p>18年之前的工作基本都是基于传统的语音模型（非神经网络）来做的，或是需要生成全新的音频，例如一段音乐隐藏语音指令这种，不能实现类似于图像对抗中与源数据不可区分的效果；</p>\n<blockquote>\n<p><em>Hidden voice commands.<br>Dolphinattack: Inaudible voice commands.<br>Inaudible voice commands.</em> </p>\n</blockquote>\n<p>同时期另一条研究线路能够做到于源数据几乎不可区分的untarget攻击；</p>\n<blockquote>\n<p><em>Crafting adversarial examples for speech paralinguistics applications.<br>Houdini: Fooling deep structured prediction models.</em></p>\n</blockquote>\n<p>2018年Carlini &amp; Wagner实现了针对任意多词句子的语音识别系统构建了对抗样本，但不能在real world有效。同年CommanderSong开发出了在real world有效的对抗样本，但代价是给原始音频引入了明显的扰动。</p>\n<blockquote>\n<p><em>Audio adversarial examples: Targeted attacks on speech-to-text.<br>Commandersong: A systematic approach for practical adversarial voice recognition.</em></p>\n</blockquote>\n<p>此后，一些工作开发了对深度学习ASR系统的攻击，要么在real world中工作，要么不那么明显地被察觉</p>\n<blockquote>\n<p><em>Robust audio adversarial example for a physical attack.<br>Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding.</em></p>\n</blockquote>\n<p>在后面，就是一些将白盒设置转为黑盒设置的一些工作</p>\n<blockquote>\n<p><em>Adversarial blackbox attacks for automatic speech recognition systems using multi-objective genetic optimization.<br>Targeted adversarial examples for black box audio systems.</em></p>\n</blockquote>\n<h3 id=\"具体方法\"><a href=\"#具体方法\" class=\"headerlink\" title=\"具体方法\"></a>具体方法</h3><p>直觉是直接使用图像中生成对抗样本的算法，例如FGSM、DeepFool、PGD等等，但是这些方法在语音对抗样本生成过程中都不好用。因此需要新的算法设计。</p>\n<h4 id=\"1-真正意义上第一个对抗语音工作：Audio-Adversarial-Examples-Targeted-Attacks-on-Speech-to-Text-作者是C-amp-W\"><a href=\"#1-真正意义上第一个对抗语音工作：Audio-Adversarial-Examples-Targeted-Attacks-on-Speech-to-Text-作者是C-amp-W\" class=\"headerlink\" title=\"1. 真正意义上第一个对抗语音工作：Audio Adversarial Examples:Targeted Attacks on Speech-to-Text 作者是C&amp;W\"></a>1. 真正意义上第一个对抗语音工作：Audio Adversarial Examples:Targeted Attacks on Speech-to-Text 作者是C&amp;W</h4><p><a href=\"https://github.com/carlini/audio_adversarial_examples\">https://github.com/carlini/audio_adversarial_examples</a></p>\n<blockquote>\n<p>白盒场景下，针对百度DeepSpeech迭代优化攻击。给定任意音频波形，可以产生99.99%相似的另一个音频，且可以转录为所选择的任何短语。<br>算法：借鉴了图像对抗样本中的C&amp;W方法，将对抗语音生成问题首次转换为了优化问题，并首次在语音对抗样本生成目标函数中引入了深度学习语音识别模型特有的CTC损失函数。</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Initial</th>\n<th>Reformulation</th>\n<th>Solve $l_{\\infty}$  without converge</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><img src=\"2020-12-15-19-01-45.png\"></td>\n<td><img src=\"2020-12-15-19-06-05.png\"></td>\n<td><img src=\"2020-12-15-19-21-33.png\"></td>\n</tr>\n</tbody></table>\n<p>在这里设$l(x’,t)=CTC-Loss(x’,t)$，</p>\n<p><strong>Improved loss function：</strong></p>\n<table>\n<thead>\n<tr>\n<th><img src=\"2020-12-15-19-46-04.png\"></th>\n<th><img src=\"2020-12-15-19-46-40.png\"></th>\n<th><img src=\"2020-12-15-19-47-44.png\"></th>\n</tr>\n</thead>\n</table>\n<p>本文在improved loss中说明攻击方法只能在 DeepSpeech 使用 Greedy-Search 的情况下有效。</p>\n<ul>\n<li><strong>Demo</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Status</th>\n<th>Audio</th>\n<th>Transcription</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>original</td>\n<td><audio id=\"audio\" controls=\"\" preload=\"none\"><source id=\"mp3\" src=\"https://nicholas.carlini.com/code/audio_adversarial_examples/adversarial0.wav\"></audio></td>\n<td>without the dataset the article is useless</td>\n</tr>\n<tr>\n<td>adversarial</td>\n<td><audio id=\"audio\" controls=\"\" preload=\"none\"><source id=\"mp3\" src=\"https://nicholas.carlini.com/code/audio_adversarial_examples/adversarial0.wav\"></audio></td>\n<td>okay google browse to evil dot com</td>\n</tr>\n</tbody></table>\n<h4 id=\"2-两阶段优化求解噪声：SirenAttack-Generating-Adversarial-Audio-for-End-to-End-Acoustic-Systems\"><a href=\"#2-两阶段优化求解噪声：SirenAttack-Generating-Adversarial-Audio-for-End-to-End-Acoustic-Systems\" class=\"headerlink\" title=\"2. 两阶段优化求解噪声：SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Systems\"></a>2. 两阶段优化求解噪声：SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Systems</h4><blockquote>\n<p>白盒：第一阶段先利用粒子群算法(PSO)求出粗粒度的噪声$\\delta$,第二阶段再利用CTC-loss反馈的梯度信息使用Fooling Gradient方法对其矫正求出更精确的$\\delta$。<br>   黑盒：由于没有loss信息，所以在第一阶段引入模型置信度量，引导粒子群向更优方向迭代，效果有限。</p>\n</blockquote>\n<p>亮点：涉及到了黑盒、计算效率高生成时间段、攻击测试了多个主流的语音识别模型</p>\n<h4 id=\"3-设计新的代理损失函数：Houdini-Fooling-Deep-Structured-Visual-and-Speech-Recognition-Models-with-Adversarial-Examples\"><a href=\"#3-设计新的代理损失函数：Houdini-Fooling-Deep-Structured-Visual-and-Speech-Recognition-Models-with-Adversarial-Examples\" class=\"headerlink\" title=\"3. 设计新的代理损失函数：Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples\"></a>3. 设计新的代理损失函数：Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples</h4><p>解决了对抗样本生成中评价指标存在组合及不可谓的问题，如语音识别中常用word error rate来评价，其就是不可微的。文章中提到，虽然CTC针对语音识别任务是一种可微的代理损失函数，但是针对其他结构性任务（如语义分割、姿态估计等）并没有一致性保证。本文实际是提出了一种针对类似不可微问题下生成对抗样本的框架。</p>\n<p>文章提出了一种Houdini代理损失函数，可以用来针对结构性任务生成对抗样本：<br><img src=\"2020-12-16-11-16-23.png\"></p>\n<p>文章中还对基于Houdini和CTC两种损失函数生成的对抗样本进行了比较，结果显示Houdini要优于CTC，但是target攻击效果并不好。 ps:由于遇到MFC层反向传播的困难，因此攻击只能生成音谱对抗数据，不能直接生成audio，工作1解决了这个问题。</p>\n<h4 id=\"4-不可察觉-鲁棒：Imperceptible-Robust-and-Targeted-Adversarial-Examples-for-Automatic-Speech-Recognition\"><a href=\"#4-不可察觉-鲁棒：Imperceptible-Robust-and-Targeted-Adversarial-Examples-for-Automatic-Speech-Recognition\" class=\"headerlink\" title=\"4. 不可察觉+鲁棒：Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition\"></a>4. 不可察觉+鲁棒：Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition</h4><ul>\n<li>白盒、有目标的、针对端到端LAS模型的对抗攻击算法；</li>\n<li>心里掩蔽效应</li>\n<li>模拟房间声学响应</li>\n</ul>\n<p>亮点在于优化语音对抗输入使其对人耳不可分辨，且更加Robust，首次在real world中产生有效攻击。提出了非基于$l_p$的对抗样本构造方法。</p>\n<p>不可察觉：文中未使用$l_p$失真度量，而是依赖于构建<strong>心理学模型</strong>使得对抗语音难以察觉：<strong>频率掩蔽</strong>通过使用另外一种充当“掩蔽器”的信号对对抗性样本进行掩护，实质是在人类听不到音频区域添加对抗扰动。优化分为两阶段，首先寻找一个相对较小的的扰动欺骗网络（和C&amp;W工作相同），再去优化对抗样本使其不易察觉。</p>\n<p>物理世界可用：考虑在训练时引入随机房间环境模拟器，考虑现实环境中的影响，实验只在模拟环境下有效。带混响的优化。</p>\n<p>最后算法将两个目标结合，设计了优化损失函数。</p>\n<h4 id=\"5-白盒鲁棒：Robust-Audio-Adversarial-Example-for-a-Physical-Attack\"><a href=\"#5-白盒鲁棒：Robust-Audio-Adversarial-Example-for-a-Physical-Attack\" class=\"headerlink\" title=\"5. 白盒鲁棒：Robust Audio Adversarial Example for a Physical Attack\"></a>5. 白盒鲁棒：Robust Audio Adversarial Example for a Physical Attack</h4><p><a href=\"https://github.com/hiromu/robust_audio_ae\">https://github.com/hiromu/robust_audio_ae</a></p>\n<p>亮点：引入脉冲响应，首次实现了pysical下的对抗样本攻击，直接思路就是讲环境带来的扰动提前考虑进来，用到了三个技术：</p>\n<ul>\n<li>Band-pass Filter：麦克风会去除环境中的一些杂音，若对抗扰动超出范围，则会被剪切，因此在损失函数中讲对抗扰动固定在不会被剪切的一个范围。</li>\n<li>Impulse Response：将环境中的脉冲响应考虑进损失函数，增强对抗语音在使用中对混响的鲁棒性。</li>\n<li>White Gaussian Noise：模拟环境中许多自然产生的背景噪声</li>\n</ul>\n<h4 id=\"6-鲁棒：Towards-Resistant-Audio-Adversarial-Examples\"><a href=\"#6-鲁棒：Towards-Resistant-Audio-Adversarial-Examples\" class=\"headerlink\" title=\"6. 鲁棒：Towards Resistant Audio Adversarial Examples\"></a>6. 鲁棒：Towards Resistant Audio Adversarial Examples</h4><p>考虑实际场景下播放器和麦克风之间距离引起的位移偏差，在优化对抗数据时加入偏差因素，使得最终的对抗语音具有较好的鲁棒性。</p>\n<h4 id=\"7-黑盒：Targeted-Adversarial-Examples-for-Black-Box-Audio-Systems\"><a href=\"#7-黑盒：Targeted-Adversarial-Examples-for-Black-Box-Audio-Systems\" class=\"headerlink\" title=\"7. 黑盒：Targeted Adversarial Examples for Black Box Audio Systems\"></a>7. 黑盒：Targeted Adversarial Examples for Black Box Audio Systems</h4><p>code：<a href=\"https://github.com/rtaori/Black-Box-Audio\">https://github.com/rtaori/Black-Box-Audio</a></p>\n<ul>\n<li>黑盒、有目标的攻击DeepSpeech的对抗攻击算法</li>\n<li>两阶段方法：遗传（改进引入Momentum mutation）+梯度估计</li>\n<li>问题：出现了query次数巨大的问题，适用场景有限</li>\n</ul>\n<h4 id=\"8-Cocaine-Noodles-Exploiting-the-Gap-between-Human-and-Machine-Speech-Recognition\"><a href=\"#8-Cocaine-Noodles-Exploiting-the-Gap-between-Human-and-Machine-Speech-Recognition\" class=\"headerlink\" title=\"8. Cocaine Noodles: Exploiting the Gap between Human and Machine Speech Recognition\"></a>8. Cocaine Noodles: Exploiting the Gap between Human and Machine Speech Recognition</h4><ul>\n<li>攻击特征模块</li>\n<li>思路简单，提取MFCC特征（这个过程会丢失一些语音信息），然后把MFCC特征逆转为语音信号</li>\n<li>算法描述不清晰，如何对MFCC逆转没过多提及。</li>\n</ul>\n<p><img src=\"2020-12-17-17-32-37.png\"></p>\n<h4 id=\"9-黑盒-amp-白盒：Hidden-Voice-Commands\"><a href=\"#9-黑盒-amp-白盒：Hidden-Voice-Commands\" class=\"headerlink\" title=\"9. 黑盒&amp;白盒：Hidden Voice Commands\"></a>9. 黑盒&amp;白盒：Hidden Voice Commands</h4><p>黑盒下生成一段无关的音频，其中隐藏有语音指令，本质上还不能称之为对抗语音，因为生成的音频不是不可察觉的。</p>\n<ul>\n<li>是对Cocaine Noodles的完善</li>\n<li>攻击特征提取模块</li>\n<li>使用梯度下降算法攻击GMM-HMM模型</li>\n<li>从这篇开始语音对抗有了比较清晰的思路，calini还是很有经验的。</li>\n</ul>\n<p><img src=\"2020-12-16-20-14-15.png\"></p>\n<p>比较关键的是MFCC parameter和inverse MFCC这两步。计算MFCC然后逆转为时域信号的过程，能够从理论上保留<strong>语音识别算法关注的特征，而抹除不想管的语音特征</strong>，而抹除的这部分特征又很可能是对人的听觉影响是很大的，导致人无法听清对抗样本。</p>\n<h4 id=\"10-黑盒：“Did-you-hear-that-Adversarial-Examples-Against-Automatic-Speech-Recognition-”\"><a href=\"#10-黑盒：“Did-you-hear-that-Adversarial-Examples-Against-Automatic-Speech-Recognition-”\" class=\"headerlink\" title=\"10. 黑盒：“Did you hear that? Adversarial Examples Against Automatic Speech Recognition,”\"></a>10. 黑盒：“Did you hear that? Adversarial Examples Against Automatic Speech Recognition,”</h4><p><a href=\"https://github.com/nesl/adversarial_audio\">https://github.com/nesl/adversarial_audio</a></p>\n<p>场景是语音分类场景下的对抗样本攻击，攻击目标是ASR，指出语音对抗样本无法适用基于梯度的优化迭代方法，使用了<strong>遗传算法</strong>。</p>\n<img src=\"2020-12-17-13-29-34.png\" width=\"60%\" height=\"60%\">\n\n<h4 id=\"11-不可听超声波蕴含语义信息：DolphinAttack-Inaudible-Voice-Commands\"><a href=\"#11-不可听超声波蕴含语义信息：DolphinAttack-Inaudible-Voice-Commands\" class=\"headerlink\" title=\"11. 不可听超声波蕴含语义信息：DolphinAttack: Inaudible Voice Commands\"></a>11. 不可听超声波蕴含语义信息：DolphinAttack: Inaudible Voice Commands</h4><p>海豚音攻击：利用麦克风的非线性来用高于20 kHz的超声波调节基带音频信号，可以隐藏转录。让正常的语音隐藏到高频波段中，从而让其无法被听到。</p>\n<p><strong>实质上是利用了麦克风的非线性漏洞，使得高频信号采样后出现额外的低频分量</strong></p>\n<h4 id=\"12-心理声学模型：Adversarial-Attacks-Against-Automatic-Speech-Recognition-Systems-via-Psychoacoustic-Hiding\"><a href=\"#12-心理声学模型：Adversarial-Attacks-Against-Automatic-Speech-Recognition-Systems-via-Psychoacoustic-Hiding\" class=\"headerlink\" title=\"12. 心理声学模型：Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding\"></a>12. 心理声学模型：Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding</h4><p>介绍了一种基于心理声学隐藏的攻击ASR系统的新型对抗样本。利用如MP3编码中的心理声学建模，以减少可察觉的噪音。</p>\n<ul>\n<li>白盒、有目标的、针对DNN-HMM模型的对抗攻击算法</li>\n<li>首次提出使用升学掩蔽效应</li>\n</ul>\n<p><img src=\"2020-12-17-17-03-20.png\"></p>\n<p>example：黑色为1Khz处60db的能量掩蔽曲线</p>\n<h4 id=\"13-音乐-指令隐藏：CommanderSong-A-Systematic-Approach-for-Practical-Adversarial-Voice-Recognition\"><a href=\"#13-音乐-指令隐藏：CommanderSong-A-Systematic-Approach-for-Practical-Adversarial-Voice-Recognition\" class=\"headerlink\" title=\"13 音乐+指令隐藏：CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition\"></a>13 音乐+指令隐藏：CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition</h4><p>在歌曲中加入语音命令，通过播放歌曲实施攻击</p>\n<h4 id=\"14-Practical-Hidden-Voice-Attacks-against-Speech-and-Speaker-Recognition-Systems\"><a href=\"#14-Practical-Hidden-Voice-Attacks-against-Speech-and-Speaker-Recognition-Systems\" class=\"headerlink\" title=\"14. Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems\"></a>14. Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems</h4><ul>\n<li>Hidden voice的黑盒扩展，将正常语音模糊化，攻击特征提取模块</li>\n<li>体现出语音攻击在物理、黑盒环境下的挑战是很大的</li>\n</ul>\n<h4 id=\"15-Hear-“No-Evil”-See-“Kenansville”-Efficient-and-Transferable-Black-Box-Attacks-on-Speech-Recognition-and-Voice-Identification-Systems\"><a href=\"#15-Hear-“No-Evil”-See-“Kenansville”-Efficient-and-Transferable-Black-Box-Attacks-on-Speech-Recognition-and-Voice-Identification-Systems\" class=\"headerlink\" title=\"15. Hear “No Evil”, See “Kenansville”: Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems.\"></a>15. Hear “No Evil”, See “Kenansville”: Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems.</h4><p><a href=\"https://github.com/kwarren9413/kenansville_attack\">https://github.com/kwarren9413/kenansville_attack</a></p>\n<ul>\n<li>声称能做到近实时、黑盒、可迁移欺骗ASR，对抗语音几乎不会误导人耳，但是会产生非目标识别错误</li>\n<li>可以大大减少黑盒攻击下query的次数</li>\n<li>不是基于梯度来做的</li>\n</ul>\n<p><img src=\"2020-12-17-19-48-34.png\"></p>\n<p>signal decomposition用的是DFT or SSA，没用MFCC，步骤也比较简单，就是设置阈值对信号进行截断，在不影响人耳识别的情况下，尽可能欺骗ASR。由于不涉及梯度，所以可以看作是黑盒的。</p>\n<h4 id=\"16-The-Faults-in-our-ASRs-An-Overview-of-Attacks-against-Automatic-Speech-Recognition-and-Speaker-Identification-Systems\"><a href=\"#16-The-Faults-in-our-ASRs-An-Overview-of-Attacks-against-Automatic-Speech-Recognition-and-Speaker-Identification-Systems\" class=\"headerlink\" title=\"16. The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems\"></a>16. The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems</h4><blockquote>\n<p>2021年 S&amp;P的一篇综述：（Hadi Abdullah在audio攻击有若干工作）下图是总结了已有攻击的各个维度的对比。</p>\n</blockquote>\n<p><img src=\"2020-12-17-17-45-53.png\"></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>对语音识别系统做对抗样本攻击，首先要了解语音模型的识别流程，重点是特征提取的CTC步骤，然后在黑盒下要注意使用遗传算法，同时，要做到现实可用且不可察觉两点要求。基于上述的攻击目标，目前的工作基本只能完成其中几项，且实验条件苛刻。   </p>\n<p><img src=\"%E8%AF%AD%E9%9F%B3%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB.png\"></p>\n<h2 id=\"一些开源的实现\"><a href=\"#一些开源的实现\" class=\"headerlink\" title=\"一些开源的实现\"></a>一些开源的实现</h2><h3 id=\"Audio-Adversarial-Examples-Paper-List\"><a href=\"#Audio-Adversarial-Examples-Paper-List\" class=\"headerlink\" title=\"Audio Adversarial Examples Paper List\"></a>Audio Adversarial Examples Paper List</h3><table>\n<thead>\n<tr>\n<th>Title</th>\n<th>Authors</th>\n<th>Publication</th>\n<th>Year</th>\n<th>Code</th>\n<th>Demo</th>\n<th>Presentation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><a href=\"http://arxiv.org/abs/1708.07238\">Inaudible Voice Commands</a></td>\n<td>L. Song et al.</td>\n<td>CCS</td>\n<td>2017</td>\n<td><a href=\"https://github.com/lwsong/inaudible-voice-commands\">☑️</a></td>\n<td><a href=\"https://www.youtube.com/watch?v=wF-DuVkQNQQ\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1801.01944\">Audio Adversarial Examples: Targeted Attacks on Speech-to-Text</a></td>\n<td>N. Carlini et al.</td>\n<td>DLS</td>\n<td>2018</td>\n<td><a href=\"https://github.com/carlini/audio_adversarial_examples\">☑️</a></td>\n<td><a href=\"https://nicholas.carlini.com/code/audio_adversarial_examples/\">☑️</a></td>\n<td><a href=\"https://www.youtube.com/watch?v=Ho5jLKfoKSA\">☑️</a></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1805.11852\">ADAGIO: Interactive Experimentation with Adversarial Attack and Defense for Audio</a></td>\n<td>N. Das et al.</td>\n<td>ECML-PKDD</td>\n<td>2018</td>\n<td></td>\n<td><a href=\"https://youtu.be/0W2BKMwSfVQ\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"https://www.usenix.org/conference/usenixsecurity18/presentation/yuan-xuejing\">CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition</a></td>\n<td>X. Yuan et al.</td>\n<td>USENIX Security</td>\n<td>2018</td>\n<td></td>\n<td></td>\n<td><a href=\"https://www.usenix.org/conference/usenixsecurity18/presentation/yuan-xuejing\">☑️</a></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1811.01312\">Adversarial Black-Box Attacks for Automatic Speech Recognition Systems Using Multi-Objective Genetic Optimization</a></td>\n<td>S. Khare et al.</td>\n<td>Interspeech</td>\n<td>2019</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1808.05665\">Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding</a></td>\n<td>L. Schönherr et al.</td>\n<td>NDSS</td>\n<td>2019</td>\n<td><a href=\"https://github.com/rub-ksv/adversarialattacks\">☑️</a></td>\n<td><a href=\"https://adversarial-attacks.net/\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1810.11793\">Robust Audio Adversarial Example for a Physical Attack</a></td>\n<td>H. Yakura et al.</td>\n<td>IJCAI</td>\n<td>2019</td>\n<td><a href=\"https://github.com/hiromu/robust_audio_ae\">☑️</a></td>\n<td><a href=\"https://yumetaro.info/projects/audio-ae/\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1809.10875\">Characterizing Audio Adversarial Examples Using Temporal Dependency</a></td>\n<td>Z. Yang et al.</td>\n<td>ICLR</td>\n<td>2019</td>\n<td><a href=\"https://github.com/AI-secure/Characterizing-Audio-Adversarial-Examples-using-Temporal-Dependency\">☑️</a></td>\n<td><a href=\"https://www.ibm.com/blogs/research/2019/05/audio-adversarial-attacks/\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1904.05734\">Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems</a></td>\n<td>H. Abdullah et al.</td>\n<td>NDSS</td>\n<td>2019</td>\n<td></td>\n<td><a href=\"https://sites.google.com/view/transcript-evasion\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1910.05262\">Hear ‘No Evil’, See ‘Kenansville’: Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems</a></td>\n<td>H. Abdullah et al.</td>\n<td>arXiv</td>\n<td>2019</td>\n<td><a href=\"https://github.com/hamzayacoob/VPSesAttacks\">☑️</a></td>\n<td><a href=\"https://sites.google.com/view/transcript-evasion\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1901.07846\">SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Systems</a></td>\n<td>T. Du et al.</td>\n<td>NDSS</td>\n<td>2019</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1901.10300\">Towards Weighted-Sampling Audio Adversarial Example Attack</a></td>\n<td>X. Liu et al.</td>\n<td>AAAI</td>\n<td>2019</td>\n<td></td>\n<td><a href=\"https://sites.google.com/view/audio-adversarial-examples\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1905.03828\">Universal Adversarial Perturbations for Speech Recognition Systems</a></td>\n<td>P. Neekharaet al.</td>\n<td>Interspeech</td>\n<td>2019</td>\n<td></td>\n<td><a href=\"https://universal-audio-perturbation.herokuapp.com/index.html\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1903.10346\">Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition</a></td>\n<td>Y. Qin et al.</td>\n<td>ICML</td>\n<td>2019</td>\n<td><a href=\"https://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_asr\">☑️</a></td>\n<td><a href=\"http://cseweb.ucsd.edu/~yaq007/imperceptible-robust-adv.html\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"http://arxiv.org/abs/1908.01551\">Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech Recognition Systems</a></td>\n<td>L. Schönherr et al.</td>\n<td>arXiv</td>\n<td>2019</td>\n<td></td>\n<td><a href=\"http://ota-adversarial-examples.selfip.org/\">☑️</a></td>\n<td></td>\n</tr>\n<tr>\n<td><a href=\"https://www.usenix.org/system/files/sec20summer_chen-yuxuan_prepub.pdf\">Devil’s Whisper: A General Approach for Physical Adversarial Attacks against Commercial Black-box Speech Recognition Devices</a></td>\n<td>Y. Chen et al.</td>\n<td>USENIX Security</td>\n<td>2020</td>\n<td><a href=\"https://github.com/RiskySignal/Devil-Whisper-Attack\">☑️</a></td>\n<td><a href=\"https://sites.google.com/view/devil-whisper\">☑️</a></td>\n<td></td>\n</tr>\n</tbody></table>\n","categories":["secure ML, Audio"]},{"title":"对抗训练paper by wys","url":"/2021/01/25/wangyisen-%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/","content":"<h1 id=\"On-the-Convergence-and-Robustness-of-Adversarial-Training\"><a href=\"#On-the-Convergence-and-Robustness-of-Adversarial-Training\" class=\"headerlink\" title=\"On the Convergence and Robustness of Adversarial Training\"></a>On the Convergence and Robustness of Adversarial Training</h1><p><strong>摘要：</strong> 对抗学习实质上是在解决一个min-max问题，max问题解的怎么样直接决定了模型的鲁棒性，本文提出了一个衡量max解的方法FOSC，可以对生成的对抗样本的收敛性进行定量评测，并发现在模型训练后期使用收敛性好的对抗样本可以提高模型的鲁棒性，在前期使用是不必要的甚至有害于模型精度，最后提出了一种动态鲁棒训练的算法使得模型更加鲁棒。</p>\n<p><strong>Tips：</strong> </p>\n<ul>\n<li><p>对抗训练实质上是在解决min-max优化问题，max问题实质就是在寻找对抗样本，max问题解的怎么样直接决定了模型的鲁棒性。<br><img src=\"2021-01-22-20-36-19.png\"></p>\n</li>\n<li><p>两种对抗方法，FGSM和PGD，FGSM是单步，PGD是多步。</p>\n</li>\n</ul>\n<p><img src=\"2021-01-22-20-48-36.png\"></p>\n<p><img src=\"2021-01-22-20-48-55.png\"></p>\n<ul>\n<li>提出了一个衡量max问题解的收敛性的标准FOSC，随后实验中测了一些FOSC收敛性和对抗效果（accuracy，loss）的影响。</li>\n</ul>\n<p><img src=\"2021-01-22-20-50-32.png\"></p>\n<ul>\n<li>提出了对抗学习在不同阶段应使用不同的对抗学习方法，在模型训练前期使用较强的对抗方法几乎不会提高鲁棒性甚至对模型准确率有害，进而提出了一种动态对抗学习的方法。</li>\n</ul>\n","categories":["Adversarial Learning"]},{"title":"语音对抗黑盒方法","url":"/2021/01/25/%E8%AF%AD%E9%9F%B3%E5%AF%B9%E6%8A%97%E9%BB%91%E7%9B%92%E6%80%BB%E7%BB%93/","content":"<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>在tx实习一个月，这一个月主要在研究针对语音的对抗攻击，前期阅读了目前为止一些流行的工作，包括了白盒和黑盒，在后期进行实验复现的时候，mentor主要还是想聚焦在黑盒场景下，不过确实感到黑盒才是在现实世界中更有意义的攻击。目前主要跑了以下几种黑盒的攻击：</p>\n<h3 id=\"1-Hear-“No-Evil”-See-“Kenansville”-Efficient-and-Transferable-Black-Box-Attacks-on-Speech-Recognition-and-Voice-Identification-Systems\"><a href=\"#1-Hear-“No-Evil”-See-“Kenansville”-Efficient-and-Transferable-Black-Box-Attacks-on-Speech-Recognition-and-Voice-Identification-Systems\" class=\"headerlink\" title=\"1. Hear “No Evil”, See “Kenansville”: Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems\"></a>1. Hear “No Evil”, See “Kenansville”: Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems</h3><p>本文的攻击使用了信号处理转换的手法来改变音频样本的表现形式，分为两类，data-independent和data-dependent。这两种方法的思路都是做<strong>原始样本—&gt;提取特征—&gt;特征剪枝—&gt;求逆返回样本。</strong> 假设是ASR识别所需要的关键成分对人耳的听感并不重要。将关键成分抹除达到untarget攻击。是一种<strong>查询式</strong>的黑盒攻击，需要query的次数较少。因此比较实用。<br><strong>1）data-independent：</strong> 不同的信号特征提取方法会提取出不同的音频特征，这种方法对DFT（离散傅里叶变换）特征变换过程进行干扰，因为DFT是一种通用的语音处理工具，并且也是MFCC其中的一步。对DFT过程进行扰动，会对后续的一系列处理都产生影响。<br><strong>攻击过程</strong>：实质上就是对某一阈值以下的频率进行置零处理，直至找到最优的那个阈值，最大程度的去除频率信息并且保障语义最小出错。阈值的优化过程采用了二分法。文章中说，这样做能够把模型决策的特征给扰动，并且不太影响听觉的感知。实质上，攻击复现后听感还是有一定的影响。若当前阈值下translate出错，则降低阈值，反之则提升阈值。<br><img src=\"2021-01-13-13-15-26.png\"><br><strong>2）data-dependent:</strong>  SSA方法：data—-&gt;特征向量，特征值小的特征向量蕴含的信息较少。实际实验中，这个攻击效果不如上面，但是对音频的听感扰动较小。<br><img src=\"2021-01-13-13-27-26.png\"><br><strong>3）overview</strong><br><img src=\"2021-01-13-13-59-43.png\"><br><strong>4）攻击效果</strong><br>在腾讯云、Google、IBM、百度云等多个speech API进行测试，都能产生误转录。不同厂商的鲁棒性不同。同时能产生攻击效果的对抗样本也存在一定程度的质量下降。文章中说可以只针对部分区间内的因素进行扰动，可以提升扰动后音频的质量。在实验中，只是跑了一下对整体音频区间的扰动。<a href=\"https://github.com/kwarren9413/kenansville_attack\">https://github.com/kwarren9413/kenansville_attack</a><br><strong>5）一些思考</strong><br>可以只针对某部分因素或者单词进行扰动，不需要全部进行扰动，但是存在的问题是这部分扰动的效果相比于clean部分很明显，虽然只扰动部分能够对后续的预测结果产生影响，但是很容易引起人的怀疑。</p>\n<h3 id=\"2-Practical-Hidden-Voice-Attacks-against-Speech-and-Speaker-Recognition-Systems\"><a href=\"#2-Practical-Hidden-Voice-Attacks-against-Speech-and-Speaker-Recognition-Systems\" class=\"headerlink\" title=\"2. Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems\"></a>2. Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems</h3><p><a href=\"https://github.com/hamzayacoob/VPSesAttacks\">https://github.com/hamzayacoob/VPSesAttacks</a></p>\n<p><img src=\"2021-01-15-11-52-34.png\"><br><strong>宗旨：</strong> 在尽量不影响音频特征提取结果的情况下，最大程度的降低音频的听感，攻击发生在黑盒场景下，可以实现语义信息的隐藏。 翻转能够使得帧内的频谱经过FFT变换后尽可能不变，使ASR翻译正确，同时翻转能够使得音频变得不平滑，从心理声学的角度使听者很难分辨具体发音。<br><strong>方法：</strong> </p>\n<ul>\n<li><p><strong>TDI（Time domain inversion）:</strong> 时域反转，选取一定大小的时间窗口，对每个窗口内的音频信号进行翻转。</p>\n<blockquote>\n<p>语音信号是一个非稳态的、时变的信号。但在 短时间 范围内可以认为语音信号是稳态的、时不变的。这个短时间一般取 10-30ms，因此在进行语音信号处理时，为减少语音信号整体的非稳态、时变的影响，从而对语音信号进行分段处理，其中每一段称为一帧，帧长一般取 25ms。</p>\n</blockquote>\n</li>\n<li><p><strong>RPG（Random Phase Generation）：</strong> 随机相位生成。经过FFT求出的频率包含了复数相位信息，幅值可以通过相位信息ai和bi求得，这是一个many-to-one的性质，因此可以修改ai和bi并保证Y不变。这样得到的音频也会使得音频不连续，影响听感。<br><img src=\"2021-01-15-11-48-17.png\"></p>\n</li>\n<li><p><strong>HFA（High Frequency Addition）：</strong> 添加高频信息，在信号处理阶段，语音识别系统会使用低通滤波器，把超过某一频率阈值的部分给cut掉，阈值的下限一般是8000HZ，因为人的声音一般在这个频率之下。因此可以增加一定密度的高频信号，当作mask，来掩盖掉原有音频。</p>\n</li>\n<li><p><strong>TS（Time Scaling）：</strong> 音频压缩加速，达到人通常无法到达的语速，这种方法一般需要搭配上面的方法才能够起到较好的隐蔽效果。</p>\n</li>\n</ul>\n<h3 id=\"3-Targeted-Adversarial-Examples-for-Black-Box-Audio-Systems-amp-Did-you-hear-that-Adversarial-Examples-Against-Automatic-Speech-Recognition\"><a href=\"#3-Targeted-Adversarial-Examples-for-Black-Box-Audio-Systems-amp-Did-you-hear-that-Adversarial-Examples-Against-Automatic-Speech-Recognition\" class=\"headerlink\" title=\"3. Targeted Adversarial Examples for Black Box Audio Systems &amp; Did you hear that? Adversarial Examples Against Automatic Speech Recognition\"></a>3. Targeted Adversarial Examples for Black Box Audio Systems &amp; Did you hear that? Adversarial Examples Against Automatic Speech Recognition</h3><ul>\n<li><strong>宗旨：</strong> 这两篇文章属于一系列的黑盒攻击方法，都是需要获得输出层的logits或者probability，实质上还是属于<strong>半白盒</strong>，因为现有的大多数ASR服务只会返回转译结果string，并不会返回其他预测概率的信息，因此这两种攻击的实用性并不是很大。</li>\n<li><strong>攻击目标：</strong> Target攻击，能够将某一音频进行扰动使其转译为目标phrase。</li>\n<li><strong>攻击方法：</strong> 都使用到了遗传算法，属于经典的黑盒优化方法。</li>\n<li><strong>实验代码：</strong> <a href=\"https://github.com/nesl/adversarial_audio\">https://github.com/nesl/adversarial_audio</a> | <a href=\"https://github.com/daniter-cu/AdversarialSpeech\">https://github.com/daniter-cu/AdversarialSpeech</a></li>\n<li><strong>攻击流程：</strong> 可以看出，targeted是对did you hear that的一个改进版本，将一阶段的遗传算法扩展了第二阶段的梯度估计微调。</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Targeted Adversarial Examples</th>\n<th>Did you hear that?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><img src=\"2021-01-15-14-41-02.png\"></td>\n<td><img src=\"2021-01-15-14-41-28.png\"></td>\n</tr>\n</tbody></table>\n<ul>\n<li><strong>实验效果：</strong> Did you hear只在较小的command classification模型上进行了实验，Targeted在大的DeepSpeech上进行的实验，产生的对抗样本效果确实还可以。但这种攻击有几个问题，需要的query需要上千次，每次需要能够获得model的scores，这在现有的ASR服务中是获取不到的。不知道如果只有转译结果String怎么设计遗传算法的fitness函数，may be impossible。</li>\n<li><strong>安全风险：</strong> 说明ASR泄露概率等分数数据，会让攻击者获取到对攻击有利的信息，从而发起更加精准的攻击。因此，ASR服务商应关闭这一返回结果。</li>\n</ul>\n","categories":["secure ML, Audio"]},{"title":"DeepFaceLab","url":"/2021/01/25/DFL%E7%AE%80%E4%BB%8B/","content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><h2 id=\"一、什么是-DeepFaceLab-2-0\"><a href=\"#一、什么是-DeepFaceLab-2-0\" class=\"headerlink\" title=\"一、什么是 DeepFaceLab 2.0\"></a>一、什么是 DeepFaceLab 2.0</h2><p>DeepFaceLab 是目前使用人数最多，效果最好的 AI 换脸软件</p>\n<p>1.0 和 2.0 有什么区别？ DFL 2.0 有什么新功能？</p>\n<ul>\n<li>DFL 2.0 的核心与 1.0 非常相似，但是对它进行了重写和优化，以使其运行速度更快并提供更好的质量。</li>\n<li>不再支持 AMD 卡，并且新模型（SAEHD 系列和 Quick96）与以前的版本不兼容。</li>\n<li>但是，在高版本的 DFL 1.0 中提取的数据集仍可以在 2.0 中使用。</li>\n</ul>\n<h2 id=\"二、软件下载\"><a href=\"#二、软件下载\" class=\"headerlink\" title=\"二、软件下载\"></a>二、软件下载</h2><p>官方代码仓库（适合源码大佬研究）：<a href=\"https://github.com/iperov/DeepFaceLab\">https://github.com/iperov/DeepFaceLab</a></p>\n<p>提供几个坛友分享的国内下载链接：</p>\n<ul>\n<li><a href=\"http://dfldata.xyz/forum.php?mod=viewthread&tid=42&extra=page=1\">《DeepFaceLab2020-8-2 中文版分享》</a></li>\n<li><a href=\"http://dfldata.xyz/forum.php?mod=viewthread&tid=46&extra=page=1\">汉化包 + 交互界面 免费搬运 回帖有奖，注册会员即可观看</a></li>\n</ul>\n<h2 id=\"三、新手教程\"><a href=\"#三、新手教程\" class=\"headerlink\" title=\"三、新手教程\"></a>三、新手教程</h2><p><a href=\"http://dfldata.xyz/forum.php?mod=viewthread&tid=34&extra=page=1\">【新手教程点我】</a></p>\n<h2 id=\"四、名词解释\"><a href=\"#四、名词解释\" class=\"headerlink\" title=\"四、名词解释\"></a>四、名词解释</h2><p>逛论坛时，大家经常说着一些炼丹、神丹、预训练等词，听得萌新一愣一愣，在此解释下。</p>\n<p><strong>丹</strong>：AI 换脸模型</p>\n<p><strong>炼丹</strong>：训练模型的过程</p>\n<p><strong>筑基丹、神丹</strong>：别人训练好的模型，拿过来用自己素材重新训练下很快就能出效果</p>\n<p><strong>预训练</strong>：在训练自己特定素材前，用其他角度丰富的人脸数据给模型预热的过程</p>\n<p><strong>炉子</strong>：显卡</p>\n<h1 id=\"DFL-2-0-官方使用手册翻译\"><a href=\"#DFL-2-0-官方使用手册翻译\" class=\"headerlink\" title=\"DFL 2.0 官方使用手册翻译\"></a>DFL 2.0 官方使用手册翻译</h1><h2 id=\"2-0-主要新增功能\"><a href=\"#2-0-主要新增功能\" class=\"headerlink\" title=\"2.0 主要新增功能\"></a>2.0 主要新增功能</h2><ul>\n<li>2 个模型大类：SAEHD 和 Quick 96。</li>\n<li>支持多 GPU。</li>\n<li>比 DFL 1.0 更快的提脸、训练、合成速度。</li>\n<li>提供人脸素材加强脚本。</li>\n<li>训练支持 GAN（一种 AI 技术），以还原更多细节。</li>\n<li>新增 TrueFace 参数 - (只支持 DF 模型架构) - 让 SRC 和 DST 表情更相似，减少死鱼脸。</li>\n<li>合成阶段附带输出遮罩图片， 方便后期导入其他软件编辑。</li>\n<li>带交互界面的合成脚本（1.0 的合成需要手动填参数，非常反人类，这个 2.0 做的很人性化）。</li>\n<li>提脸使用 s3fd 算法，并支持手动模式。</li>\n<li>模型分辨率可选择为任意 16 或 32 倍数。</li>\n<li>多种模型架构 (两种核心框架 DF, LIAE, 以及 -U, -D ,-UD 三种附加后缀，这个下文详细展开)</li>\n<li>使用 Xseg 遮罩模型，提供自带画遮罩的工具。（滚石注：对追求细节的高玩适合用 Xseg，普通小白还是别用了，很繁琐。个人觉得还是 1.0 版本的 FANseg 省心）。</li>\n</ul>\n<h2 id=\"2-0-硬件要求\"><a href=\"#2-0-硬件要求\" class=\"headerlink\" title=\"2.0 硬件要求\"></a>2.0 硬件要求</h2><p>DFL 2.0 只支持英伟达显卡，不支持 AMD 显卡。</p>\n<p>需要显卡 CUDA Compute Capability 3.0 以上。</p>\n<p>DeepFaceLab 2.0 由几个.bat 文件组成，这些文件用于执行创建 Deepfake 的各种任务 / 步骤，它们与两个子文件夹一起位于主文件夹中：</p>\n<ul>\n<li>_internal - 相关源代码。</li>\n<li>workspace - 放置你的模型、视频、人脸数据的地方。</li>\n</ul>\n<p>下面是一些术语解释</p>\n<p>Dataset (faceset) - 是一组从图片帧（从视频中提取）或照片已提取的图像。</p>\n<p>DFL 2.0 中使用了两个数据集，分别是 <strong>data_dst</strong> 和 <strong>data_src</strong>：</p>\n<ul>\n<li>“<strong>data_dst</strong>“ 是一个文件夹，其中包含从 data_dst.mp4 文件提取的帧 - 这是我们要换脸的目标视频。 它还包含 2 个文件夹，这些文件夹是在从提取的帧中提取人脸后创建的：“aligned” 包含人脸图像（内嵌了人脸特征点数据，用于生成原始遮罩） “aligned_debug” 包含原始帧，这些帧画出了人脸特征点，用于标识检验人脸提取是否正确（并且不参与训练或合成过程）。清理完数据集后，可以将其删除以节省空间。</li>\n<li>“<strong>data_src</strong>“ 是一个文件夹，用于保存从 data_src.mp4 文件提取的帧（可以是采访，电影，预告片等），也可以在其中放散装图片 - 就是您要在换到视频上的人。 “aligned” 包含人脸图像（内嵌了人脸特征点数据，用于生成原始遮罩） “aligned_debug” 包含原始帧，这些帧画出了人脸特征点，用于标识检验人脸提取是否正确（并且不参与训练或合成过程）。清理完数据集后，可以将其删除以节省空间。</li>\n</ul>\n<p>为方便国内朋友理解，我举个例子：我要做一个马保国替换叶问中甄子丹的换脸视频。 那么马保国就是 <strong>data_src</strong>，叶问就是 <strong>data_dst</strong>。</p>\n<p>但是，在提取脸部之前，必须先从以下对象中提取脸部：</p>\n<ul>\n<li>对于 data_dst，您应该准备目标（目标）视频并将其命名为 data_dst.mp4</li>\n<li>对于 data_src，您应该准备源视频（如上例所示）并将其命名为 data_src.mp4，或者准备 jpg 或 png 格式的图像。</li>\n</ul>\n<p>从视频中提取帧的过程也称为提取，因此在本指南的其余部分中，我将这两个过程都称为 “面部提取” 和 “帧提取”。</p>\n<p>如开头所述，所有这些数据都存储在 “workspace” 文件夹中。data_dst.mp4 和 data_src.mp4 放在 workspace 文件夹目录；data_src 文件夹和 data_dst 文件夹用于放置分解视频得到的原始画面帧或散装图片。运行面部提取后，会在其中自动生成存储人脸的 “ aligned” 文件夹。</p>\n<h2 id=\"1-工作区清理\"><a href=\"#1-工作区清理\" class=\"headerlink\" title=\"1. 工作区清理\"></a>1. 工作区清理</h2><p><strong>1) Clear Workspace -</strong> 删除 workspace 下所有内容。别手贱点他。</p>\n<h2 id=\"2-从-src-视频中提取画面-data-src-mp4\"><a href=\"#2-从-src-视频中提取画面-data-src-mp4\" class=\"headerlink\" title=\"2. 从 src 视频中提取画面 (data_src.mp4)\"></a>2. 从 src 视频中提取画面 (data_src.mp4)</h2><p><strong>2) Extract images from video data_src -</strong> 从 data_src.mp4 视频中提取帧并将其放入自动创建的 “data_src” 文件夹中，可用选项：-FPS - 跳过视频的默认帧速率，输入其他帧速率的数值（例如，输入 5 将仅以每秒 5 帧的速度呈现视频，这意味着将提取较少的帧） -JPG / PNG - 选择提取帧的格式，jpg 较小，通常质量足够好，因此建议使用，png 较大，不能提供明显更高的质量，但是可以选择。</p>\n<h2 id=\"3-视频切割-（可选环节）\"><a href=\"#3-视频切割-（可选环节）\" class=\"headerlink\" title=\"3. 视频切割 （可选环节）\"></a>3. 视频切割 （可选环节）</h2><p><strong>3) cut video (drop video on me)</strong> - 通过将视频拖放到该.bat 文件中，可以快速将视频剪切为所需的长度。 如果您没有视频编辑软件并且想快速剪切视频，则很有用，可以选择以下选项： 从时间开始 - 视频开始 结束时间 - 视频结束 音轨 - 保留默认设置 比特率 - 让我们更改视频的比特率（质量）- 最好保留默认设置</p>\n<h2 id=\"3-从目标视频中提取画面-data-dst-mp4\"><a href=\"#3-从目标视频中提取画面-data-dst-mp4\" class=\"headerlink\" title=\"3. 从目标视频中提取画面 (data_dst.mp4)\"></a>3. 从目标视频中提取画面 (data_dst.mp4)</h2><p><strong>3) extract images from video data_dst FULL FPS -</strong> 从 data_dst.mp4 视频文件中提取帧并将其放入自动创建的 “data_dst” 文件夹中，可用选项： - JPG/PNG - 同 2)</p>\n<h2 id=\"4-提取-Data-src-中的人脸\"><a href=\"#4-提取-Data-src-中的人脸\" class=\"headerlink\" title=\"4. 提取 Data_src 中的人脸\"></a>4. 提取 Data_src 中的人脸</h2><p>准备源数据集的第一步是对齐人脸（把人脸都摆正了），并从位于 “data_src” 文件夹中的提取帧中生成 512x512 面部图像。</p>\n<p>有 2 个选项：</p>\n<p><strong>4) data_src faceset extract MANUAL</strong> - 手动提取器，用法请参见 5.1。</p>\n<p><strong>4) data_src faceset extract</strong> - 使用 S3FD 算法的自动提取</p>\n<p>S3FD 和 MANUAL 提取器的可用选项包括： - 根据要训练的模型的面部类型选择提取的覆盖区域：</p>\n<p><strong>a) full face</strong> (简称 F 脸，额头部分有些许被裁到)</p>\n<p><strong>b) whole face</strong> (简称 WF 脸，范围更大，整个额头都取了，兼容 WF 和 F 脸模型)</p>\n<p><strong>c) head</strong> (不常用，给高玩做 avatar 用，萌新用不到) - 选择用于面部提取 / 对齐过程的 GPU（或 cpu） - 选择是否生成 “ aligned_debug” 文件夹</p>\n<h2 id=\"4-Data-src-整理\"><a href=\"#4-Data-src-整理\" class=\"headerlink\" title=\"4. Data_src 整理\"></a>4. Data_src 整理</h2><p>完成此操作后，下一步是清理错误 faceset / 数据集 / 不正确对齐的 faces，有关详细信息，请检查以下帖子：<a href=\"https://mrdeepfakes.com/forums/thread-guide-celebrity-faceset-dataset-creation-how-to-create-celebrity-facesets\">https://mrdeepfakes.com/forums/thread-gu…y-facesets</a></p>\n<p><strong>4.1) data_src view aligned result</strong> - 不常用</p>\n<p><strong>4.2) data_src sort</strong> - 给图片排序，方便你筛选错误图片</p>\n<ul>\n<li>blur 模糊程度</li>\n<li>face yaw direction 俯仰角度</li>\n<li>face pitch direction 左右角度</li>\n<li>face rect size in source image 人脸在原图中的大小</li>\n<li>histogram similarity 颜色直方图相似度</li>\n<li>histogram dissimilarity 颜色直方图不相似度</li>\n<li>brightness 亮度</li>\n<li>hue 颜色色相</li>\n<li>amount of black pixels 黑色像素的数量（常用于筛选异常人脸提取结果）</li>\n<li>original filename 源文件名字</li>\n<li>one face in image 是否是画面中唯一人脸</li>\n<li>absolute pixel difference 绝对的像素差异</li>\n<li>best faces 筛选最佳的人脸</li>\n<li>best faces faster 更快的筛选最佳的人脸</li>\n</ul>\n<p><strong>4.2) data_src util add landmarks debug images</strong> - 重新生成 debug 文件夹</p>\n<p><strong>4.2) data_src util faceset enhance</strong> - 用 AI 算法提升素材清晰度</p>\n<p>另一个可选的提升 SRC 的素材的方法，可用 DFDNET ，谷歌 colab 连接如下： <a href=\"https://colab.research.google.com/github/rocketsvm/DFDNet/blob/master/DFDNet_DFL_Colab.ipynb#scrollTo=OZ7-E-vRSYar\">https://colab.research.google.com/github…7-E-vRSYar</a></p>\n<p><strong>4.2) data_src util faceset metadata restore and 4.2) data_src util faceset metadata save</strong> - 让我们从源面集 / 数据集中保存和还原嵌入的对齐数据，以便在提取某些面部图像（例如将它们锐化，编辑眼镜，皮肤瑕疵，颜色校正）后可以对其进行编辑，而不会丢失对齐数据。如果不按此步骤编辑 “已对齐” 文件夹中的任何图像，则将不会再使用对齐数据和这些图片进行训练，因此，在保持名称相同的情况下，不允许翻转 / 旋转，仅是简单的编辑，例如彩色 。</p>\n<p><strong>4.2) data_src util faceset pack and 4.2) data_src util faceset unpack -</strong> 将 “aligned” 文件夹中的所有面孔打包 / 解压缩到一个文件中。 主要用于准备自定义的预训练数据集或更易于共享为一个文件。</p>\n<p><strong>4.2.other) data_src util recover original filename -</strong> 将面部图像的名称恢复为原始顺序 / 文件名（排序后）。 可选，无论 SRC face 文件名如何，训练和合成都能正确运行。</p>\n<h2 id=\"5-Data-dst-数据准备\"><a href=\"#5-Data-dst-数据准备\" class=\"headerlink\" title=\"5. Data_dst 数据准备\"></a>5. Data_dst 数据准备</h2><p>这里的步骤与源数据集几乎相同，除了少数例外，让我们从面部提取 / 对齐过程开始。 我们仍然有 Manual 和 S3FD 提取方法，但是还有一种结合了这两种方法和一种特殊的手动提取模式，始终会生成 “aligned_debug” 文件夹。</p>\n<p><strong>5) data_dst faceset extract MANUAL RE-EXTRACT DELETED ALIGNED_DEBUG</strong> - 从 “aligned_debug” 文件夹中删除的帧进行手动重新提取。 有关更多信息，请参见 5. Data_dst 清理。 以下步骤 5.1 中的用法。</p>\n<p><strong>5) data_dst faceset extract MANUAL</strong> - 纯手动模式</p>\n<p><strong>5) data_dst faceset extract + manual fix</strong> - 半自动，机器识别不了的会切手动</p>\n<p><strong>5) data_dst faceset extract</strong> - 纯自动提取</p>\n<p>选项和 src 的一样，不重复说了</p>\n<h2 id=\"5-1-手动人脸提取的操作说明\"><a href=\"#5-1-手动人脸提取的操作说明\" class=\"headerlink\" title=\"5.1 手动人脸提取的操作说明\"></a>5.1 手动人脸提取的操作说明</h2><p>启动手动提取器或重新提取器后，将打开一个窗口，您可以在其中手动找到要提取 / 重新提取的脸部：</p>\n<ul>\n<li>使用鼠标定位脸部</li>\n<li>使用鼠标滚轮更改搜索区域的大小</li>\n<li>确保所有或至少是大多数地标（在某些情况下，取决于角度，照明或当前障碍物，可能无法精确对齐所有地标，因此，请尝试找到一个最能覆盖所有可见位并且是 “t 太不对准）落在重要的部位，例如眼睛，嘴巴，鼻子，眉毛上，并正确遵循面部形状，向上箭头指示您面部的 “向上” 或 “顶部” 在哪里</li>\n<li>使用键 A 更改精度模式，现在地标不会对检测到的面部 “粘” 太多，但您可能能够更正确地定位地标</li>\n<li>用户 &lt;和&gt; 键（或，和。）来回移动，以确认检测到鼠标左键单击并移至下一个或按 Enter</li>\n<li>鼠标右键，用于检测无法检测到的正面或非人脸（需要应用 xseg 进行正确的遮罩）</li>\n<li>q 跳过剩余的面孔并退出提取器（到达最后一张面孔并确认时也会关闭）</li>\n</ul>\n<h2 id=\"5-2-Data-dst-数据整理\"><a href=\"#5-2-Data-dst-数据整理\" class=\"headerlink\" title=\"5.2 Data_dst 数据整理\"></a>5.2 Data_dst 数据整理</h2><p>对齐 data_dst 面后，我们必须清理它们，类似于我们使用源 faceset /dataset 进行处理时，我们将选择一些排序方法，由于它们的工作方式与 src 完全相同，因此我将不作解释。 但是清理目标数据集与源数据集有所不同，因为我们要使所有存在的帧的所有面对齐（包括可以在 XSeg 编辑器中标记的受遮挡的面），然后训练 XSeg 模型以将其遮盖 - 有效地使障碍物在学到的面孔上清晰可见，更多的是在下面的 XSeg 阶段。</p>\n<p>这块做法和 data_src 类似，区别在于，最后合成时是根据 dst 中 aligned 文件数量来合成。删掉的 dst 人脸数据对应的画面就不会换脸</p>\n<h2 id=\"5-3-XSeg-model-的训练和使用（画遮罩）\"><a href=\"#5-3-XSeg-model-的训练和使用（画遮罩）\" class=\"headerlink\" title=\"5.3 XSeg model 的训练和使用（画遮罩）\"></a>5.3 XSeg model 的训练和使用（画遮罩）</h2><p>这章比较复杂，晚点翻译。萌新先不要使用遮罩。不用遮罩正常也能训练</p>\n<h2 id=\"6-训练\"><a href=\"#6-训练\" class=\"headerlink\" title=\"6. 训练\"></a>6. 训练</h2><p>有两种模式可以选择：</p>\n<p><strong>SAEHD (6GB+)：高质量模型，至少 6GB 显存</strong></p>\n<p>特点 / 设置</p>\n<p>最高 640x640 分辨率，</p>\n<p>可支持 half face, mid-half face, full face, whole face and head face 5 中人脸尺寸类型</p>\n<p>8 种模型结构：DF, LIAE, 每种 4 个变种 - regular, -U, -D and -UD</p>\n<p>可调节的批大小（batchsize）</p>\n<p>可调节的模型各层维度大小</p>\n<p>Auto Backup feature 自动备份</p>\n<p>Preview History 预览图存档</p>\n<p>Adjustable Target Iteration 目标迭代次数</p>\n<p>Random Flip (yaw) 随机水平翻转</p>\n<p>Uniform Yaw 按角度顺序来训练</p>\n<p>Eye Priority 眼神训练优先</p>\n<p>Masked Training 带遮罩训练</p>\n<p>GPU Optimizer 优化器放 GPU 上</p>\n<p>Learning Dropout 学习率自动下降</p>\n<p>Random Warp 随机扭曲</p>\n<p>GAN Training Power 使用 GAN</p>\n<p>True Face Training Power 提高人脸相似度</p>\n<p>Face and Background Style Power 提高颜色相似度</p>\n<p>Color Transfer modes 变对素材变色</p>\n<p>Gradient Clipping 梯度裁剪 - Pretrain Mode 使用预训练模式</p>\n<p><strong>Quick96 (2-4GB)：低配电脑可用</strong></p>\n<p>特点：</p>\n<ul>\n<li>96x96 分辨率</li>\n<li>只支持 Full Face</li>\n<li>Batch size 4</li>\n<li>默认 DF-UD 结构</li>\n</ul>\n<p><strong>6) train SAEHD</strong></p>\n<p><strong>6) train Quick96</strong></p>\n<p>由于 Quick96 不可调节，因此您将看到命令窗口弹出并仅询问一个问题 - CPU 或 GPU（如果您有更多问题，它将选择其中之一或同时进行训练）。 但是，SAEHD 将为您提供更多调整选项。</p>\n<p>在这两种情况下，首先都会出现一个命令行窗口，您可以在其中输入模型设置。 初次使用时，您将可以访问以下说明的所有设置，在使用已经受过训练的模型进行训练并在 “模型” 文件夹中显示该模型时，您还将收到提示，您可以在其中选择要训练的模型（ （如果您的 “模型” 文件夹中存在多个模型文件）。 您还将始终提示您选择要在其上运行培训器的 GPU 或 CPU。</p>\n<p>启动后将看到的第二件事是预览窗口，如下所示：</p>\n<p><a href=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E9%A2%84%E8%A7%88%E7%AA%97%E5%8F%A3.jpg\"><img src=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E9%A2%84%E8%A7%88%E7%AA%97%E5%8F%A3.jpg\" alt=\"预览窗口.jpg\">预览窗口.jpg</a></p>\n<p>这是所有功能的更详细说明，以便在开始训练新模型时将其呈现给用户：</p>\n<p>请注意，由于这些模型的工作方式，其中一些已锁定，一旦开始训练就无法更改，例如，稍后无法更改的示例如下：</p>\n<ul>\n<li>model resolution 模型分辨率</li>\n<li>model architecture 模型结构</li>\n<li>models dimensions (dims settings) 模型维度参数</li>\n<li>face type 人脸类型</li>\n</ul>\n<p><strong>Autobackup every N hour ( 0..24 ?:help )</strong> ：自动备份频率，0 不备份</p>\n<p><strong>Target iteration :</strong> 将在达到一定的迭代次数后停止训练，例如，如果要将模型训练为仅进行 100.000 次迭代，则应输入值 100000。将其保留为 0 将使其运行，直到您手动将其停止为止。 默认值为 0（禁用）。</p>\n<p><strong>Flip faces randomly ( y/n ?:help ) :</strong> 基本不开。在您没有要交换到目标的人脸（源数据集）的所有必要角度的情况下的有用选项。 例如，如果您有一个目标 / 目标视频，人物直视向右，而您的源只具有直视向左的脸，则应启用此功能，但请记住，由于没有人脸对称，结果看起来可能不太像 src 以及来源面部的特征（例如美容标记，疤痕，痣等）都会被镜像。 默认值为 n（禁用）。</p>\n<p><strong>Batch_size ( ?:help ) :</strong> 批处理大小设置会影响每次迭代中相互比较的面孔数。 最低值是 2，您可以提高到 GPU 允许的最大值，受 GPU 影响。 模型分辨率，尺寸越高，启用的功能越多，将需要更多的显存，因此可能需要较小的批处理大小。 建议不要使用低于 4 的值。批量越大，质量越好，但训练时间越长（迭代时间越长）。 对于初始阶段，可以将其设置为较低的值以加快初始训练的速度，然后将其升高。 最佳值为 6-12。 如何猜测要使用的批量大小？ 您可以使用试错法，也可以通过查看 DFL 2.0 电子表格来了解其他人在他们的 GPU 上可以实现什么，以帮助自己。<a href=\"https://mrdeepfakes.com/forums/thread-dfl-2-0-user-model-settings-spreadsheet\">https://mrdeepfakes.com/forums/thread-dfl-2-0-user-model-settings-spreadsheet</a></p>\n<p><strong>Resolution ( 64-640 ?:help ) :</strong> 在这里，您可以设置模型的分辨率，请记住，在训练过程中不能更改此选项。 它会影响交换的面部的分辨率，模型的分辨率越高 - 学习的面部越详细，但训练的负担也将越来越长。 分辨率可以从 64x64 增至 640x640，其增量为： 16（对于常规和 - U 体系结构变体） 32（用于 - D 和 - UD 体系结构变体） 更高的分辨率可能需要增加模型尺寸（尺寸）。</p>\n<p><strong>Face type ( h/mf/f/wf/head ?:help ) :</strong> 此选项使您可以设置要训练的脸部区域，共有 5 个选项 - 半脸，半脸，全脸，全脸和头部： a）H 半脸 - 仅从嘴巴到眉毛训练，但在某些情况下可以割破脸部的顶部或底部（眉毛，下巴，嘴巴）。 b）MF 中半脸 - 旨在解决此问题，方法是遮盖脸部比半脸大 30％，这应该可以防止大多数不希望的割伤的发生，但仍然可以发生。 c）F 全脸 - 覆盖除额头以外的大部分脸部区域，有时会割掉一点下巴，但是这种情况很少发生 - 当 SRC 和 / 或 DST 的额头覆盖头发时，最推荐使用此方法。 d）WF 整脸 - 扩大该区域以覆盖几乎整个脸部，包括额头，甚至一点点头发，但是当我们要交换整个脸部（不包括头发）时，应使用此模式。该脸部类型的另一个选项是 masked_training，它使您可以优先确定学习脸部的整个脸部的优先级，然后（禁用之后）让模型学习像额头一样的脸部其余部分。 e）头 - 用于交换整个头，不适合长发的对象，如果源面组 / 数据集来自单个源并且 SRC 和 DST 都短发或不变，则效果最好形状取决于角度。此脸型的最低建议分辨率为 224。<a href=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E8%84%B8%E5%9E%8B.jpg\"><img src=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E8%84%B8%E5%9E%8B.jpg\" alt=\"脸型.jpg\"></a></p>\n<p><a href=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E8%84%B8%E5%9E%8B.jpg\">脸型.jpg</a></p>\n<p><strong>whole face 案例</strong></p>\n<p><a href=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E6%95%B4%E8%84%B8%E6%A1%88%E4%BE%8B.png\"><img src=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E6%95%B4%E8%84%B8%E6%A1%88%E4%BE%8B.png\" alt=\"整脸案例.png\">整脸案例.png</a></p>\n<p>整脸案例.png</p>\n<p><strong>AE architecture (df/liae/df-u/liae-u/df-d/liae-d/df-ud/liae-ud ?:help ) :</strong> 此选项使您可以在 2 种主要的体系结构 DF 和 LIAE 及其 - U，-D 和 - UD 变体之间进行选择。</p>\n<p>DF 和 LIAE 体系结构是基础体系结构，两者均提供高质量和良好的性能。 DF-U，DF-UD，LIAE-U 和 LIAE-UD 是其他体系结构变体。</p>\n<p><strong>DF:</strong> 此模型体系结构提供了更直接的人脸交换，不使人脸变形，但要求源和目标 / 目标人脸 / 头部具有相似的人脸形状。 该模型在正面拍摄时效果最好，并且要求您的源数据集具有所有所需的角度，在侧面轮廓上可能会产生较差的结果。</p>\n<p><strong>LIAE:</strong> 当涉及源和目标 / 目标之间的面部 / 头部形状相似性时，此模型体系结构没有那么严格，但是该模型确实使面部变形，因此建议使实际面部特征（眼睛，鼻子，嘴巴，整体面部结构）相似 在源和目标 / 目标之间。 该模型与源头镜头的源相似性较差，但可以更好地处理侧面轮廓，并且在源源面集 / 数据集方面更宽容，通常可以产生更精致的人脸替换，并具有更好的颜色 / 照明匹配度。 <strong>-U:</strong> 此变体旨在提高训练结果面与 SRC 数据集的相似性 / 相似性。 <strong>-D:</strong> 此变体旨在提高性能，让您以两倍的分辨率训练模型，而无需额外的计算成本（VRAM 使用）和类似的性能，例如以与 128 分辨率相同的 VRAM 使用和速度（迭代时间）训练 256 分辨率模型</p>\n<p><strong>-UD:</strong> 结合 U 和 D</p>\n<p>接下来的 4 个选项控制模型神经网络的尺寸，这些尺寸会影响模型的学习能力，对其进行修改可能会对所学面孔的性能和质量产生重大影响，因此应将其保留为默认值。</p>\n<p><strong>AutoEncoder dimensions ( 32-1024 ?:help ) :</strong> 自动编码器中间层维度大小</p>\n<p><strong>Encoder dimensions ( 16-256 ?:help ) :</strong> 编码器尺寸设置会影响模型学习面孔总体结构的能力。</p>\n<p><strong>Decoder dimensions ( 16-256 ?:help ) :</strong> 解码器尺寸设置会影响模型学习细节的能力。</p>\n<p><strong>Decoder mask dimensions ( 16-256 ?:help ) :</strong> 遮罩解码器的尺寸设置会影响学习到的遮罩的质量。</p>\n<p>更改每个设置时的性能变化可能会对性能产生不同的影响，如果没有大量的培训，就无法衡量每个参数对性能和质量的影响。 每个设置为某个默认值，该默认值应提供最佳结果，并在训练速度和质量之间取得良好的折衷。</p>\n<p>同样，在更改一个参数时，也应更改其他参数，以保持它们之间的关系相似（例如，如果将 “编码器” 和 “解码器” 的尺寸从 64 降低到 48，则还可以将 “自动编码器” 的尺寸从 256 降低到 192-240）。 随意尝试各种设置。 如果要获得最佳结果，请将其保留为默认值，或者对于高分辨率型号，将其略微提高。</p>\n<p><strong>Eyes priority ( y/n ?:help ) :</strong> 试图通过强制神经网络训练优先级更高的眼睛来解决眼睛训练问题。 请记住，它不能保证正确的眼睛方向，它只会影响眼睛的细节和周围区域。 示例（之前和之后）：</p>\n<p><a href=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%BC%E7%9D%9B%E4%BC%98%E5%85%88.jpg\"><img src=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%BC%E7%9D%9B%E4%BC%98%E5%85%88.jpg\" alt=\"眼睛优先.jpg\">眼睛优先.jpg</a></p>\n<p>眼睛优先.jpg</p>\n<p><strong>Place models and optimizer on GPU ( y/n ?:help ) :</strong> 启用 GPU 优化器会将所有负载都放在 GPU 上，这将大大提高性能（迭代时间），但会导致更高的 VRAM 使用率，禁用此功能会将优化器的某些工作分担给 CPU，从而减少了 GPU 和 VRAM 使用率的负载，从而使您可以实现 批处理量更大或以更高的迭代时间为代价运行更苛刻的模型。 如果您收到 OOM（内存不足）错误，并且不想减小批量大小或禁用某些功能，则应禁用此功能，这样一来，一些工作将被卸载到 CPU 上，而某些数据将从 GPU VRAM 转移到系统 RAM 中 - 您将能够以较低速度为代价运行模型而不会出现 OOM 错误。 默认值为 y（启用）。</p>\n<p><strong>Use learning rate dropout ( y/n/cpu ?:help ) :</strong> LRD 被用于加速面部的培训，并减少相比，如果没有启用它，使用它较少的迭代子像素抖动：</p>\n<ul>\n<li>在禁用 RW 之前和运行其他选项之后一次。</li>\n<li>禁用 RW 之后（也是 LRD）第二次使用其他选项（均匀偏航，样式效果，真面，眼睛优先），但在启用 GAN 之后。</li>\n</ul>\n<p>此选项会影响 VRAM 的使用，因此，如果遇到 OOM 错误，则可以在 CPU 上运行它，但需要花费 20％的迭代时间或降低批处理大小。</p>\n<p>有关 LRD 的详细说明以及在培训期间启用主要功能的顺序，请参阅 FAQ 问题 8（此主题的第 3 个帖子）：”When should I enable or disable random warp, GAN, True Face, Style Power, Color Transfer and Learning Rate Dropout?”</p>\n<p><strong>Enable random warp of samples ( y/n ?:help ) :</strong> 随机扭曲用于概括模型，以便它可以正确学习所有基本形状，面部特征，面部结构，表情等，但是只要启用该模型，学习精细细节就可能会遇到麻烦 - 因为它 建议您只要您的脸部仍在改善中（通过查看减少的损耗值和预览窗口）就启用此功能，一旦对脸部进行了全面训练并想要获得更多详细信息，则应禁用它并进行数千次迭代 应该会开始看到更多详细信息，并且禁用此功能后，您将继续进行培训。 默认值为 y（启用）。 <strong>Uniform_yaw ( y/n ?:help ) :</strong> 有助于训练轮廓脸部，迫使模型根据其偏航角在所有面孔上均匀地训练，并优先考虑轮廓脸部，可能会导致正面脸部的训练速度变慢，这在预训练期间默认启用，可与随机变形类似地使用（在开始时 （训练过程）或在禁用或禁用 RW 后启用（当您对面部进行或多或少的训练，并且您希望轮廓脸部看起来更好且更少模糊时）。 当您的源数据集没有很多轮廓照片时很有用。 可以帮助降低损失值。 默认值为 n（禁用）。</p>\n<p><strong>GAN power ( 0.0 .. 10.0 ?:help ) :</strong> GAN 代表 Generative Adversarial Network，在 DFL 2.0 的情况下，它是作为获得更详细 / 更清晰面孔的一种额外培训方式而实施的。 此选项的调整范围是 0.0 到 10.0，只有在模型或多或少地完成训练后（禁用样本随机扭曲并启用 LRD 之后），才应启用该选项。 建议从低值 0.1 开始，该值在大多数情况下也是建议值，一旦启用，就不应禁用它，请确保对模型进行备份，以防不满意结果。 默认值为 0.0（禁用）。</p>\n<p>用 GAN 训练 0.1 的面部进行 40k 迭代之前 / 之后的示例：</p>\n<p><a href=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_GAN.png\"><img src=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_GAN.png\" alt=\"GAN.png\">GAN.png</a></p>\n<p>GAN.png</p>\n<p><strong>‘True face’ power. ( 0.0000 .. 1.0 ?:help ) :</strong> 使用可变功率设置的真实面部训练，让您将模型鉴别器设置为更高或更低的值，这样做是为了使最终面孔看起来更像 src，而对于 GAN，只有在禁用了随机扭曲后，才应启用此功能 并且模型训练有素。 在启用此功能之前，请考虑进行备份。 切勿使用较高的值，典型值为 0.01，但可以使用较低的值，例如 0.001。 设置越高，结果面将越像源数据集中的面，这可能导致颜色匹配问题，并导致出现伪影，因此重要的是不要使用高值。 它对性能的影响很小，可能会导致 OOM 错误发生。 默认值为 0.0（禁用）。</p>\n<p><a href=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%9F%E5%AE%9E%E9%9D%A2%E9%83%A8%E6%9D%83%E9%87%8D.png\"><img src=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%9F%E5%AE%9E%E9%9D%A2%E9%83%A8%E6%9D%83%E9%87%8D.png\" alt=\"真实面部权重.png\">真实面部权重.png</a></p>\n<p>真实面部权重.png</p>\n<p><strong>Face style power ( 0.0..100.0 ?:help ) and Background style power ( 0.0..100.0 ?:help ) :</strong> 此设置控制图像的面部或背景部分的样式转移，用于将目标 / 目标面孔（data_dst）的样式转移到最终学习的面孔，这样可以提高质量和合并后最终结果的外观，但是 高值可能导致学习的人脸看起来更像 data_dst，而不是 data_src。 它将从 DST 传输一些颜色 / 照明信息到结果脸部。 建议不要使用大于 10 的值。从 0.001-0.01 之类的小值开始。 此功能对性能有很大影响，使用它会增加迭代时间，并且可能需要您减小批处理大小，禁用 gpu 优化器或在 CPU 上运行 LRD。 在启用此功能之前，请考虑进行备份。 默认值为 0.0（禁用）。</p>\n<p><strong>Color transfer for src faceset ( none/rct/lct/mkl/idt/sot ?:help ) :</strong> 此功能用于将 data_src 的颜色与 data_dst 进行匹配，以使最终结果具有与 data_dst 相似的肤色 / 色调，并且训练后的最终结果不会在人脸移动时改变颜色（如果脸部不同，可能会发生这种情况 角度是从包含不同光照条件或颜色分级不同的各种光源获取的。 有以下几种选择：</p>\n<p><strong>rct</strong> (reinhard color transfer)（我常用，滚石注）：基于 <a href=\"https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf\">https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf</a></p>\n<p><strong>lct</strong> (linear color transfer)：使用线性变换将目标图像的颜色分布与源图像的颜色分布匹配。</p>\n<p><strong>mkl</strong> (Monge-Kantorovitch linear)：基于 <a href=\"http://www.mee.tcd.ie/~sigmedia/pmwiki/uploads/Main.Publications/fpitie07b.pdf\">http://www.mee.tcd.ie/~sigmedia/pmwiki/uploads/Main.Publications/fpitie07b.pdf</a></p>\n<p><strong>idt</strong> (Iterative Distribution Transfer)：基于 <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.1052&amp;rep=rep1&amp;type=pdf\">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.1052&amp;rep=rep1&amp;type=pdf</a></p>\n<p><strong>sot</strong> (sliced optimal transfer)：基于 <a href=\"https://dcoeurjo.github.io/OTColorTransfer/\">https://dcoeurjo.github.io/OTColorTransfer/</a></p>\n<p><strong>Enable gradient clipping ( y/n ?:help ) :</strong> 梯度裁剪。实现此功能是为了防止在使用 DFL 2.0 的各种功能时可能发生的所谓的模型崩溃 / 损坏。 它对性能的影响很小，因此，如果您真的不想使用它，则必须启用自动备份，因为崩溃后的模型无法恢复，必须将其废弃，并且必须从头开始进行培训。 默认值为 n（禁用），但是由于对性能的影响非常低，并且如果保持启用状态，可以防止模型崩溃而节省大量时间。 使用 Style Powers 时最容易发生模型崩溃，因此强烈建议您启用渐变裁剪或备份（也可以手动进行）。</p>\n<p><strong>Enable pretraining mode ( y/n ?:help ) :</strong> 启用预训练过程，该过程使用随机人脸数据集对模型进行初始训练，将其训练约 200k-400k 次迭代后，可以在开始使用要训练的实际 data_src 和 data_dst 进行训练时使用此类模型，因为您可以节省时间不必每次都从 0 开始全面训练（模型将 “知道” 面孔的外观，从而加快初始训练阶段）。可以随时启用 pretrain 选项，但建议在开始时仅对模型进行一次预训练。您还可以使用自己的自定义面集进行预训练，您要做的就是创建一个（可以是 data_src 或 data_dst），然后使用 4.2）data_src（或 dst）util faceset pack .bat 文件打包成一个文件，然后将其重命名为 faceset.pak 并替换（备份旧的）“ …  _ internal  pretrain_CelebA” 文件夹中的文件。默认值为 n（禁用）。但是，如果要节省一些时间，可以去论坛找别人训练好的模型。</p>\n<p>共享模型：<a href=\"https://mrdeepfakes.com/forums/thread-dfl-2-0-pretrained-models-general-thread-for-user-made-models-and-requests\">https://mrdeepfakes.com/forums/thread-dfl-2-0-pretrained-models-general-thread-for-user-made-models-and-requests</a></p>\n<p>要使用共享的预训练模型，只需下载它，将所有文件直接放入模型文件夹中，开始训练，在选择要训练的模型（如果在模型文件夹中有更多内容）和用于训练的设备后 2 秒钟内按任意键 （GPU / CPU）来覆盖模型设置，并确保禁用预训练选项，以便您开始正确的训练；如果您启用了预训练选项，则模型将继续进行预训练。 请注意，该模型会将迭代计数恢复为 0，这是预训练模型的正常行为。</p>\n<h2 id=\"7-Merging-合成\"><a href=\"#7-Merging-合成\" class=\"headerlink\" title=\"7. Merging 合成:\"></a>7. Merging 合成:</h2><p>训练完模型后，该将学习的人脸合并到原始帧上以形成最终视频了（转换）。</p>\n<p>为此，我们有 2 个对应于 2 种可用型号的转换脚本：</p>\n<p><strong>7) merge SAEHD</strong></p>\n<p><strong>7) merge Quick96</strong></p>\n<p>选择其中任何一个后，命令行窗口将出现，并带有多个提示。 第一个将询问您是否要使用带交互界面的转化器，默认值为 y（启用），除非你受虐狂，不然就好好开着吧，边调参数边预览</p>\n<p><strong>Use interactive merger? ( y/n ) :</strong></p>\n<p>第二个将询问您要使用哪种模型：</p>\n<p><strong>Choose one of saved models, or enter a name to create a new model.</strong></p>\n<p>[r] ：rename</p>\n<p>[d] ：delete</p>\n<p>[0] ：df192 - latest</p>\n<p>:</p>\n<p>第 3 个会问您要在合并（转换）过程中使用哪个 GPU / GPU：</p>\n<p><strong>Choose one or several GPU idxs (separated by comma).</strong></p>\n<p>[CPU] ：CPU</p>\n<p>[0] ：GeForce GTX 1070 8GB</p>\n<p>[0] Which GPU indexes to choose?</p>\n<p>:</p>\n<p>按 Enter 将使用默认值 [0]。</p>\n<p>完成之后，您将看到一个带有当前设置的命令行窗口以及一个预览窗口，其中显示了操作交互式转换器 / 合并程序所需的所有控件。</p>\n<p>这是命令行窗口和转换器预览窗口的快速浏览：</p>\n<p><a href=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E5%BF%AB%E9%80%9F%E6%B5%8F%E8%A7%88.png\"><img src=\"https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E5%BF%AB%E9%80%9F%E6%B5%8F%E8%A7%88.png\" alt=\"快速浏览.png\">快速浏览.png</a></p>\n<p>快速浏览.png</p>\n<p>Converter 具有许多选项，可用于更改遮罩类型，大小，羽化 / 模糊，还可以添加其他颜色转移并进一步锐化 / 增强最终训练的脸部。</p>\n<p>这是解释的所有合并 / 转换器功能的列表：</p>\n<p><strong>1. Main overlay modes:</strong></p>\n<ul>\n<li><strong>original:</strong> 显示原始画面而没有换脸</li>\n<li><strong>overlay:</strong> 简单地将学习到的脸覆盖在框架上 （推荐用这个，滚石注）</li>\n<li><strong>hist-match:</strong> 根据直方图叠加学习的面部和试图以使其匹配（具有 2 种模式：正常模式和可通过 Z 切换的蒙版）</li>\n<li><strong>seamless:</strong> 使用 opencv 泊松无缝克隆功能在原始帧的头部上方融合新学习的面部</li>\n<li><strong>seamless hist match:</strong> 结合了直方图匹配和无缝匹配。</li>\n<li><strong>raw-rgb:</strong> 覆盖原始学习过的脸部而没有任何遮罩</li>\n</ul>\n<p>注意：无缝模式可能导致闪烁。</p>\n<p><strong>2. Hist match threshold:</strong> 在直方图匹配和无缝直方图覆盖模式下控制直方图匹配的强度。</p>\n<p>Q - 增加值</p>\n<p>A - 减小值</p>\n<p><strong>3. Erode mask:</strong> 控制遮罩的大小。</p>\n<p>W - 增加遮罩腐蚀（较小的遮罩）</p>\n<p>S - 减少遮罩腐蚀 (较大的遮罩)</p>\n<p><strong>4. Blur mask</strong>：使遮罩边缘模糊 / 羽化，以实现更平滑的过渡</p>\n<p>E - 增加值</p>\n<p>D - 减小值</p>\n<p><strong>5. Motion blur:</strong> 动态模糊。输入初始参数（转换器模式，模型，GPU / CPU）后，合并将加载所有帧和 data_dst 对齐的数据，同时，它会计算用于创建此设置控制的运动模糊效果的运动矢量，让您 将其添加到人脸移动的地方，但是即使移动很小，高值也可能使人脸模糊。 该选项仅在 “data_dst /aligned” 文件夹中存在一组面孔时才有效 - 如果在清理过程中某些面孔带有_1 前缀（即使只有一个人的面孔），效果将不起作用，同样 如果有一个可以反射目标人员面部的镜子，在这种情况下，您将无法使用运动模糊，并且添加该模糊的唯一方法是分别训练每组面部。</p>\n<p>R - 增加 motion blur</p>\n<p>F - 减少 motion blur</p>\n<p><strong>6. Super resolution:</strong> 超分辨率使用与 data_src 数据集 / 面部设置增强器类似的算法，它可以为牙齿，眼睛等区域添加更多定义，并增强所学面部的细节 / 纹理。</p>\n<p>T - 增加细节 the enhancement effect</p>\n<p>G - 减少细节</p>\n<p><strong>7. Blur/sharpen:</strong> 使用方块或高斯方法模糊或锐化所学的面部。</p>\n<p>Y - sharpens the face</p>\n<p>H - blurs the face</p>\n<p>N - box/gaussian mode switch</p>\n<p><strong>8. Face scale:</strong> 缩放人脸</p>\n<p>U - scales learned face down</p>\n<p>J - scales learned face up</p>\n<p><strong>9. Mask modes:</strong> 6 种遮罩计算方式，效果自己试一遍就知道了</p>\n<p>dst: uses masks derived from the shape of the landmarks generated during data_dst faceset/dataset extraction.</p>\n<p>learned-prd: uses masks learned during training. Keep shape of SRC faces.</p>\n<p>learned-dst: uses masks learned during training. Keep shape of DST faces.</p>\n<p>learned-prd*dst: combines both masks, smaller size of both.</p>\n<p>learned-prd+dst: combines both masks, bigger size of both.</p>\n<p>XSeg-prd: uses XSeg model to mask using data from source faces.</p>\n<p>XSeg-dst: uses XSeg model to mask using data from destination faces.</p>\n<p>XSeg-prd*dst: combines both masks, smaller size of both.</p>\n<p>learned-prd<em>dst</em>XSeg-dst*prd: combines all 4 mask modes, smaller size of all.</p>\n<p><strong>10. Color transfer modes:</strong> 与训练过程中的颜色转移类似，您可以使用此功能将学习到的脸部的肤色与原始帧更好地匹配，以实现更加无缝和逼真的脸部交换。 有 8 种不同的模式：</p>\n<p>RCT</p>\n<p>LCT</p>\n<p>MKL</p>\n<p>MKL-M</p>\n<p>IDT</p>\n<p>IDT-M</p>\n<p>SOT - M</p>\n<p>MIX-M</p>\n<p><strong>11. Image degrade modes:</strong> 您可以使用 3 种设置来影响原始帧的外观（不影响换面）：</p>\n<p>Denoise - denoises image making it slightly blurry (I - increases effect, K - decrease effect)</p>\n<p>Bicubic - blurs the image using bicubic method (O - increases effect, L - decrease effect)</p>\n<p>Color - decreases color bit depth (P - increases effect, ; - decrease effect)</p>\n<p>附加控件：</p>\n<p><strong>TAB button</strong> - 在主预览窗口和帮助屏幕之间切换。</p>\n<p>请记住，您只能在主预览窗口中更改参数，按帮助屏幕上的任何其他按钮都不会更改它们。</p>\n<p><strong>-/_ and =/+</strong> buttons are used to scale the preview window.</p>\n<p>Use caps lock to change the increment from 1 to 10 (affects all numerical values).</p>\n<p>要保存 / 覆盖当前一帧中所有下一帧的设置 <strong>shift + /</strong></p>\n<p>要保存 / 覆盖当前一帧中所有先前帧的设置 <strong>shift + M</strong></p>\n<p>要开始合并所有帧，请按 <strong>shift + &gt;</strong></p>\n<p>要返回第一帧，请按 <strong>shift + &lt;</strong></p>\n<p>要仅转换下一帧，请按 <strong>&gt;</strong></p>\n<p>要返回上一帧，请按 <strong>&lt;</strong></p>\n<h2 id=\"8-把转化好的帧合成为视频\"><a href=\"#8-把转化好的帧合成为视频\" class=\"headerlink\" title=\"8. 把转化好的帧合成为视频\"></a>8. 把转化好的帧合成为视频</h2><p>合并 / 转换所有面部之后，“data_dst” 文件夹中将有一个名为 “ merged” 的文件夹，其中包含构成视频的所有帧。 最后一步是将它们转换回视频，并与 data_dst.mp4 文件中的原始音轨合并。</p>\n<p>为此，您将使用提供的 4 个.bat 文件之一，这些文件将使用 FFMPEG 将所有帧组合成以下格式之一的视频 - avi，mp4，lessless mp4 或 lossless mov：</p>\n<p><strong>8) merged to avi</strong></p>\n<p><strong>8) merged to mov lossless 无损 mov</strong></p>\n<p><strong>8) merged to mp4 lossless 无损 MP4</strong></p>\n<p><strong>8) merged to mp4</strong></p>\n<h1 id=\"FAQ\"><a href=\"#FAQ\" class=\"headerlink\" title=\"FAQ\"></a>FAQ</h1><p><strong>1. Q：1.0 和 2.0 有什么区别？</strong></p>\n<p>答：2.0 是经过改进和优化的版本，由于进行了优化，它提供了更好的性能，这意味着您可以训练更高分辨率的模型或更快地训练现有模型。 合并和提取也明显更快。 问题在于 DFL 2.0 不再支持 AMD GPU / OpenCL，唯一的使用方法是与 Nvidia GPU（需要最少 3.0 CUDA 计算级别的 GPU 支持）或 CPU 一起使用。 请记住，在 CPU 上进行训练的速度要慢得多，其他所有步骤（例如提取和合并（以前称为转换））也要慢得多。 此外，新版本仅提供 2 种型号 - SAEHD 和 Quick 96，没有 H128 / H64 / DF / LIAEF / SAE 型号。 同样，从 1.0 开始的所有训练 / 预训练模型（SAE / SAEHD）与 2.0 不兼容，因此您需要训练新模型。 在上面的主要指南中，您还可以了解其他一些更改。</p>\n<p><strong>2. Q：做一次换脸需要多长时间？</strong></p>\n<p>A：根据目标（DST）视频的时长，SRC 数据集 / 面集的大小以及用于训练的硬件（GPU）的模型的类型。 如果您是从头开始训练模型或制作更长的 deepfake，则在经过预训练的模型上进行简单，短暂的可能需要半天到 5 到 7 天的时间，特别是如果高分辨率全脸型需要额外的时间 培训 XSeg 模型，甚至在合并视频编辑软件后进行一些工作。 这也取决于您拥有的硬件，如果您的 GPU 的 VRAM 较少（4-6 GB），则需要花费更长的时间来训练具有更强大 GPU（8-24GB）的模型。 它还取决于您的技能，查找 SRC 数据集 / 面容的原始资料的速度，可以找到合适的目标（DST）视频的速度以及为训练准备两个数据集的速度有多快。</p>\n<p><strong>3. Q：你能用几张照片制作一个 Deepfake 视频吗？</strong></p>\n<p>A：通常，答案是 “否”。 推荐使用面部表情制作像样的 Deepfake 的方法是使用视频。角度和面部表情越多越好。 当然，您可以尝试只用几百张照片制作一个 Deepfake 视频，它可以正常工作，但结果却不那么令人信服。</p>\n<p><strong>4. Q：理想的人脸数据数量是多少？</strong></p>\n<p>A：对于 data_src（名人）脸部，建议至少拥有 4000-6000 张不同的图像。当然，您可以拥有更多但通常 10.000-15.000 张图像就足够了，只要数据集中有多种（不同的面部角度和表情）。 最好将它们用于尽可能少的源，使用的源越多（尤其是当不同源之间相似的表达式 / 角度 “重叠” 时），将模型变形为 DST 的可能性越高，看起来更像是 SRC，可能需要运行 TF 或保持启用 RW 的模型训练时间更长。 有关制作源面集 / 数据集的更多详细指南，请查看以下指南： <a href=\"https://mrdeepfakes.com/forums/thread-gu...y-facesets\">https://mrdeepfakes.com/forums/thread-gu...y-facesets</a></p>\n<p><strong>5. Q：为什么我的 Deepfake 变得模糊？</strong></p>\n<p>A：面孔模糊的原因很多。最常见的原因包括 - 训练时间不够长，源数据集中缺少必要的角度，提取的源或目标面集 / 数据集的对齐方式不正确，训练或合并过程中的设置不正确，源面或目标面集 / 数据集中的人脸模糊。 如果您想知道做些什么以及在做一次深度仿真时应避免什么，请阅读本主题（指南部分）的第一篇文章。</p>\n<p><strong>6. Q：为什么我的结果脸没有眨眼 / 眼睛看起来不对 / 眼睛交叉？?</strong></p>\n<p>A：这很可能是由于 data_src 中缺少图像而导致的，这些图像包含闭着眼睛或以特定角度或某些角度朝特定方向注视的面孔。确保在所有可能的角度上都有相当数量的不同面部表情，以匹配目标 / 目标视频中的面部表情和角度 - 其中包括睁着眼睛并朝不同方向看的面部，而模型不知道这些 脸部的眼睛看起来应该怎样，导致眼睛无法睁开或看上去全都错了。 造成此问题的另一个原因可能是使用错误的设置或调暗设置减少了跑步训练。</p>\n<p><strong>7. Q：我什么时候应该停止训练？</strong></p>\n<p>A：没有正确的答案，但是普遍的共识是使用预览窗口来判断何时停止训练和转换。没有确切的迭代次数或损失值，您应在此停止训练。 如果您正在运行预训练的模型，则建议至少进行 100.000 次迭代；如果从 0 开始运行新模型，则建议至少进行 200.000 次迭代，但是该数目甚至可能高达 300.000 次迭代（取决于模型必须学习的面孔的数量和数量） ）。</p>\n<p><strong>8. Q：我什么时候应该启用或禁用随机扭曲，GAN，真面，样式效果，色彩转移和学习率下降？When should I enable or disable random warp, GAN, True Face, Style Power, Color Transfer and Learning Rate Dropout?</strong></p>\n<p>A： 1. 从 “随机扭曲” 开始，只要使模型能够泛化并达到 0.4-0.6 左右的损耗值，就应启用它 - 预测面（预览的第二和第四列）和最终结果面（第五列）应看起来正确，但可能仍然模糊。</p>\n<p>\\2. 接下来，您可以启用诸如 Uniform Yaw 之类的选项，以帮助概括轮廓面或帮助培训它们，以防您在源数据集中没有很多东西的情况下使用，但是不必总是使用 RW 来运行此选项，以后可以在启用该选项时启用禁用 RW。</p>\n<p>\\3. 在禁用 RW 之前，您可以启用 LRD 并对其进行更多地训练，以使其在此阶段中学习更多，接下来，您将禁用 RW 和 LRD，以便模型可以在下一阶段中正确进行训练。稍后，在禁用 RW 的情况下进行训练并有选择地为 4-9 运行一些其他选项之后，您可以第二次启用 LRD，在 LRD 之后，您不应启用 GAN 以外的任何其他选项。</p>\n<p>\\4. 可以在禁用 “随机扭曲” 之前或之后使用 “ True Face and Style Power”，但是建议在禁用 RW 之后以及启用 LRD 之前使用它们。</p>\n<p>5.GAN 应该最后启用，并且在同时启用 LRD 时也必须启用。</p>\n<p>\\6. 可以从培训开始或结束时就启用颜色转移，这完全取决于您的 SRC 面部表情 / 数据集在色彩方面与 DST / 目标视频的匹配程度。</p>\n<p>\\7. 仅当源面集 / 数据集缺少某些角度时才应启用随机翻转 - 注意使用随机翻转可能会导致某些问题，并且如果面部具有一些不对称的特征 - 它们将被镜像，建议您在启动随机翻转时 RW 仍处于启用状态。</p>\n<p>\\8. 对于大多数训练，应该启用蒙版训练（仅全脸），因为它有助于训练模型需要学习的内容，即所应用的 XSeg 遮罩（如果 XSeg 遮罩为未应用），但是如果您打算在合并过程中腐蚀蒙版（如果源面比 DST 大 / 宽，并且您不希望它的形状被蒙版剪切），则可以禁用它（如果这样做）确保在启用 GAN 之前先执行此操作。</p>\n<p>\\9. 禁用 RW 之后但在 LRD 和 GAN 之前，应启用眼睛优先级，因为其他选项可能会引起问题（真面目，样式效果）。在某些情况下，当仍在使用 RW 时，用户在运行它时报告了良好的效果，但请记住，启用它会优先考虑眼睛区域和面部其余部分，其学习速度不会像平常那样快。</p>\n<p><strong>9. Q：DFL 根本不起作用（提取，培训，合并）和 / 或我收到错误消息。</strong></p>\n<p>您的 GPU 可能不受支持，您正在尝试在较新版本的 DFL 上运行旧模型（或以其他方式），或者软件或 PC 出现问题。</p>\n<p>首先检查您的 GPU 是否受支持，DFL 需要 CUDA 计算能力为 3.0： <a href=\"https://zh.wikipedia.org/wiki/CUDA#GPUs_supported\">https://zh.wikipedia.org/wiki/CUDA#GPUs_supported</a> 然后查看是否拥有 DFL 的最新版本：GOOGLE DRIVE TORRENT 然后检查您要运行的模型是否仍然兼容，最简单的方法是尝试运行具有相同参数的新模型（将批次大小调整为较低的值，例如 2-4，以进行测试） 如果仍然有问题，请检查您的 PC，GPU 驱动程序过时和 Windows 更新等待中之类的事情可能会引起一些问题。 如果仍然无法运行，则可以在问题部分中创建一个新线程，但是在执行此操作之前，请检查 github 上的问题选项卡，以查看其他用户是否没有相同的问题 / 错误：DFL 2.0 GITHUB 如果找不到任何东西，并且您已在论坛上搜索类似的问题，请在此处创建新主题：<a href=\"https://mrdeepfakes.com/forums/forum-questions\">https://mrdeepfakes.com/forums/forum-questions</a> 或在此处发布有关信息。</p>\n<p><strong>10. 训练 SAEHD 时出现 OOM / 内存不足错误。</strong> 如果遇到 OOM 错误，则意味着您的 VRAM 即将用尽，可以更改各种设置来解决此问题：</p>\n<p>a）减少批次大小 - 较低的批次大小意味着模型在较少的图像上进行训练，因此使用较少的 VRAM，但这意味着与达到较高的批次大小相比，您将需要更长的训练时间才能达到相同的结果，例如 2-4 的极低批次大小也会导致结果准确性降低。</p>\n<p>b）更改优化器设置（models_opt_on_gpu）- 设置为 True 时，优化器和模型均由 GPU 处理，这意味着更快的迭代时间 / 性能 / 更快的训练，但 VRAM 使用率更高；如果设置为 False，则将处理运行网络优化器的职责通过 CPU 进行操作，这意味着较少的 VRAM 使用量，可能没有 OOM，甚至可能的批处理量更大，但是由于迭代时间较长，因此训练速度会变慢。</p>\n<p>c）关闭其他功能，例如面部和 bg 样式转换，TrueFace 训练，GAN 训练或性能沉重的 CT 方法（如 SOT-M）- 使它们增加迭代 / 训练时间并使用更多的 VRAM。</p>\n<p><strong>11. Q：I’ve turned off all additional features and the training is still giving me OOM errors even at low batch size.</strong></p>\n<p>A：在这种情况下，您甚至可以更改更多设置，但由于您只能设置一次，因此将需要您开始新的模型训练：</p>\n<p>a）以降低的分辨率运行模型 - 即使可以进行所有优化并禁用各种功能，仍然可能无法运行所需的分辨率，只需降低分辨率直到可以运行（分辨率为 16 倍）</p>\n<p>b）减小自动编码器尺寸，编码器尺寸，解码器尺寸，解码器蒙版尺寸。 这些设置控制着模型的尺寸，因此更改它们可以对模型学习面部和表情特征的能力产生巨大影响。 将该值设置为低值可能意味着该模型不会闭上眼睛或无法学习某些面部特征。 如果您无能为力，请仅更改它们。 有关他们做什么的更多信息，请查看指南。</p>\n<p>c）购买具有更多 VRAM 的 GPU。</p>\n<p><strong>12. Q：我的源数据集中有太多相似的面孔，有没有可以用来删除它们的工具？</strong></p>\n<p>A：是的，您可以使用内置的 DFL 排序方法，也可以使用 VisiPics 之类的应用来检测源数据集中的相似外观并将其删除。</p>\n<p><strong>13. Q：我正在训练已经进行了数千次迭代的模型，但是预览窗口中的面孔突然变成黑 / 白 / 看起来很怪异，我的损失值上升 / 为零。</strong></p>\n<p>A：您的模型已经崩溃，这意味着您不能再使用它，必须重新开始，如果有备份，请使用它们。为防止模型崩溃，请使用渐变裁剪或仅启用备份，通常除非使用样式功能（否则强烈建议启用渐变裁剪），否则模型不会崩溃（但强烈建议您启用渐变裁剪），即使您担心即使没有启用任何其他功能也要崩溃 您可以始终在不影响性能的情况下始终启用它（可能很难注意到，根据我的测试，最多最多可以增加 50-100 毫秒）。</p>\n<p><strong>14. Q：如果我训练 Celeb A 的模型（data_src）并使用 Celeb B（data_dst）作为目的地，是否可以使用相同的 Celeb A 模型替换新的 Celeb C？ 我可以重用模型吗？</strong></p>\n<p>A：是的，实际上，如果您打算对同一来源进行更多的伪造，甚至在使用同一数据集时，也建议重用您的模型。 当使用完全不同的源和目标 / 目标时，您也可以重新使用模型。</p>\n<p><strong>15. Q：我应该预训练我的模型吗？</strong></p>\n<p>A：与重用一样，是的，您应该进行预训练。使用 DFL 内置的预训练功能，您可以在启动模型时选择该功能。 这是预训练模型，在 200k 至 400k 迭代中的任何位置运行此功能并在要完成预训练后将其关闭的正确方法。</p>\n<p><strong>16. Q：我遇到错误：不是在 DeepFaceLab 中训练所需的 dfl 图像文件 error: is not a dfl image file required for training in DeepFaceLab</strong></p>\n<p>A：这意味着 data_src /aligned 和 / 或 data_dst 内部的图片对于 DFL 训练无效。 这可能是由以下几方面引起的：</p>\n<p>\\1. 您正在使用名人的共享数据集之一，尽管它们看起来像对齐的面孔（256x256 图像）可能只是从中提取的图片，但它们是由不同于 DFL 的软件或较旧版本制作的。以不同方式存储界标 / 路线数据的不同应用。要修复它们，您只需要对它们运行对齐过程，只需将它们放入 “data_src” 文件夹（而不是其中的 “ aligned” 文件夹），然后使用 4 重新对齐它们即可。data_src 提取面 S3FD</p>\n<p>\\2. 对齐后，您在 gimp /photoshop 中的 data_src 或 data_dst 的对齐文件夹中编辑了脸部 / 图像。 编辑这些图像时，将覆盖存储在其中的地标 / 路线数据。 如果要编辑这些图像，请首先运行 4.2）data_src util faceset 元数据保存以将对齐信息保存在单独的文件中，然后编辑图像并运行 4.2）data_src util faceset 元数据恢复以还原该数据。 仅允许编辑 AI 放大 / 增强功能（您现在也可以使用 4.2 进行操作）data_src 实用面部设置增强功能，而不是使用外部应用程序（例如 Gigapixel），颜色校正或对不改变其形状的面部进行编辑（例如删除或添加内容），则不允许翻转 / 镜像或旋转。</p>\n<p>\\3. 您的 “data_src /dst” 或 “ aligned” 文件夹中有未提取的未对齐的常规图像。</p>\n<p>\\4. 您的 “data_src /aligned” 文件夹中有_debug 面。删除它们。</p>\n<p><strong>17. Q：转换时出现错误：找不到 XYZ.jpg/png 的人脸，没有人脸地复制。I’m getting errors during conversion: no faces found for XYZ.jpg/png, copying without faces.</strong></p>\n<p>A：这意味着对于 “data_dst” 文件夹中的 XYZ 帧，没有脸部被提取到 “ aligned” 文件夹中。 这可能是因为在该帧中实际上没有可见的脸部（正常），或者它们是可见的，但是由于它们所处的角度或障碍物而无法检测到。 要解决此问题，您需要手动提取这些面孔。 查看主要指南，尤其是有关清理 data_dst 数据集的部分。</p>\n<p>总体而言，在开始训练之前，应确保已对齐并正确提取了多少张面孔。 并记住，在训练之前应该清理两个数据集，以了解更多信息，请查阅第一篇文章（指南），并阅读以下有关准备源数据集以供训练使用并在我们的论坛上共享的主题</p>\n<p><strong>18. Q：转换期间 警告：检测到多张脸。 强烈建议分开对待它们，并警告：检测到多张面孔。 将不使用定向模糊。 I’m getting errors: Warning: several faces detected. Highly recommended to treat them separately and Warning: several faces detected. Directional Blur will not be used. during conversion</strong></p>\n<p>A：这是由 data_dst /aligned 文件夹中的多个面孔引起的。 提取过程试图不惜一切代价检测每帧中的面部。 如果它确实检测到多个面部或一个真实的面部并错误地将其他事物检测为面部，则会为每个帧创建多个文件，如下所示：0001_0.jpg 0001_1.jpg 0001_2.jpg（如果检测到 3 张面部）。</p>\n<p><strong>19. Q：合并后，我在某些或所有合并帧中看到原始 / DST 面。</strong></p>\n<p>A：确保将转换器模式设置为叠加模式或 “原始” 模式以外的任何其他模式，并确保已对齐 data_dst.mp4 文件所有帧中的面。 如果您仅在某些帧上看到原始人脸，那是因为未从相应的帧中检测到它们 / 将它们对齐，则可能由于各种原因而发生这种情况：难以看到人脸的极端角度，模糊 / 运动模糊，障碍物等。 总体而言，您希望始终将 data_dst.mp4 中的所有面孔对齐。</p>\n<p><strong>20. Q：训练时那些 0.2513 0.5612 代表什么意思？</strong></p>\n<p>A：这些是损失值。 它们表明模型的训练程度。但是，除非它们已经稳定在某个值附近（假设您没有更改任何模型参数），否则您不应该关注它们，除非看到它们的值突然上升（上下波动），而应关注预览窗口并查找诸如 牙齿分离，美容痕迹，鼻子和眼睛，如果它们锋利且看起来不错，那么您就不必担心任何事情。 如果您发现损耗值由于某些原因尽管没有改变但仍因某些原因而上升，请考虑停止训练并通过梯度裁剪恢复它，或者禁用一些其他选项，这些选项可能是您在错误的设置下启用的，这现在会引起问题。</p>\n<p><strong>21. Q：理想损耗值是多少，低 / 高损耗值应为多少？</strong></p>\n<p>A：这完全取决于设置，数据集和各种不同的因素。通常，您希望在禁用所有功能的情况下开始训练，但样本的随机扭曲（以及可选的梯度裁剪，以防止模型崩溃和万一您的源数据集缺少某些面部 / 头部角度的情况下进行随机翻转）损失到 0.4-0.5 以下（取决于 模型架构，以及是启用了屏蔽训练的全脸模型还是启用了屏蔽训练的全脸 / 头部模型，以及模型分辨率或模型尺寸。 禁用随机扭曲模型后，应该能够达到 0.15 到 0.25 之间的损失值。 在某些情况下，您的模型可能会陷入某些损失值，或者永远无法达到较低的损失值。</p>\n<p><strong>22. Q：我的模型崩溃了，我能以某种方式恢复它吗？</strong></p>\n<p>A：不，您需要重新开始，或者如果使用了备份，请使用备份。</p>\n<p><strong>23. Q：如果您使用名人面部表情训练并且想要向其中添加更多面部 / 图像 / 框架怎么办？ 如何为现有 src /source/celebrity 数据集增加更多种类？</strong></p>\n<p>A：最安全的方法是将整个 “data_src” 文件夹的名称更改为其他名称，或将其临时移动到其他位置，然后从新的 data_src.mp4 文件中提取帧，或者如果您已经提取了帧并准备好一些图片，则创建一个 新文件夹 “ data_src”，将其复制到其中并运行 data_src 提取 / 对齐过程，然后将对齐的图像从旧的 data_src /aligned 文件夹复制到新文件夹中，并在 Windows 要求替换或跳过时，选择重命名选项 文件，因此您可以保留所有文件，而不会最终用新文件替换旧文件。</p>\n<p><strong>24. Q：dst faceset /data_dst.mp4 是否也需要清晰且高质量？ dst faceset /dataset/data_dst 中的某些面孔会有点模糊 / 有阴影等吗？ 我的 data_dst /aligned 文件夹中的面孔模糊怎么办</strong></p>\n<p>A：您希望您的 data_dst 尽可能清晰，并且没有任何运动模糊。 data_dst 中的面孔模糊会导致几个问题： - 首先是某些框架中的某些面孔将不会被检测到 - 转换 / 合并时，原始面孔将显示在这些框架上，因为在提取过程中无法正确对齐它们，因此您必须手动提取。 - 其次是其他人可能未正确对齐 - 这将导致该帧上的最终脸部旋转 / 模糊，并且看起来很不正确，并且与其他模糊脸部相似，必须手动对齐才能用于训练和转换。 - 第三 - 即使在某些情况下使用手动对齐，也可能无法正确检测 / 对齐面部，这又将导致原始面部在相应的帧上可见。 - 包含运动模糊或正确对齐的模糊（不清晰）的面部可能仍会产生不良结果，因为用于训练的模型无法理解运动模糊，模糊时面部的某些部分（如嘴巴）可能显得更大 / 更宽或只是不同而已，模型会将其解释为该部分的形状 / 外观发生了变化，因此，预测的假面和最终的假面都将看起来不自然。 您应该从训练数据集（data_dst /aligned 文件夹）中删除那些模糊的面部，然后将它们放在其他地方，然后再将它们复制回 data_dst /aligned 文件夹，然后再进行转换，这样我们可以将交换的面部显示在与那些模糊的面部相对应的帧上。 要消除运动中的怪异表情，您可以在合并中使用运动模糊（但不适用于 “data_dst /aligned” 文件夹中只有一组面孔且所有文件都以_0 前缀结尾的情况）。</p>\n<p>您希望 SRC 数据集和 DST 数据集都尽可能清晰和高质量。 某些帧上的少量模糊不应该引起很多问题。至于阴影，这取决于我们在谈论多少阴影，可能看不见小的浅色阴影，您可以在脸上带有阴影的情况下获得良好的效果，但在很大程度上也会看起来很糟糕，您希望将脸部照亮均匀分布，并尽可能减少刺眼 / 尖锐和深色阴影。</p>\n<p><strong>25. Q：当我尝试将 Deepfake 转换回 8）转换为 mp4 的 mp4 时，找不到错误 reference_file。</strong></p>\n<p>A：您在 “工作区” 文件夹中缺少 data_dst.mp4 文件，请检查是否未删除该文件： 之所以需要它，是因为即使使用 3）从视频 data_dst FULL FPS 提取图像将其分成单独的帧，“ data_dst” 文件夹中的所有内容也只是视频的帧，您还需要声音，该声音取自 原始的 data_dst.mp4 文件。</p>\n<p><strong>26. Q：我不小心删除了 data_dst.mp4 文件，无法恢复，仍然可以将合并 / 转换后的帧转换为 mp4 视频吗？</strong></p>\n<p>A：是的，如果您已经永久删除了 data_dst.mp4，并且无法恢复它或呈现相同的文件，您仍然可以使用 ffmpeg 和适当的命令将其手动转换回 mp4（尽管没有声音）：</p>\n<p>- start by going into folder …:_internal\ffmpeg and copy ffmpeg.exe - paste it into the merged folder - open up command line by pressing windows key + r (run) and typing cmd or searching it up after pressing windows key and typing cmd/cmd.exe - copy address of your merged folder (example: DFLworkspacedata_dstmerged) - in the command line type the letter of your drive, as in example above that would be “d:” (without quotation marks) and press enter - line D:&gt; should appear, next type “cd: FULL_ADDRESS”, example: “cd: D:workspacedata_dstmerged” - you should now see your entire address like this: DFLworkspacedata_dstmerged&gt; - enter this command:</p>\n<p>ffmpeg -r xx -i %d.jpg -vcodec libx264 -crf 20 -pix_fmt yuv420p result.mp4</p>\n<p>- xx is framerate - d is a number representing amount of numbers in the file name so if your merged frames have names like 15024.jpg that would be 5, if it’s 5235.jpg it is 4, etc. If your images are pngs, change .jpg to .png - crf is quality setting, best to be left at 20. If your merged file names have some letters in front like out12345.jpg add “out” before the % sign.</p>\n<p>Example command for converting frames named “out_2315.png” into an 30 fps .mp4 file named “deepfake”.</p>\n<p>ffmpeg -r 30 -i out%4.png -vcodec libx264 -crf 20 -pix_fmt yuv420p deepfake.mp4</p>\n<p>If you want to use x265 encoding change libx264 to libx265.</p>\n<p><strong>27. Q：您可以暂停合并，然后再恢复吗？ 您可以保存合并设置吗？ 我的合并失败 / 合并时出现错误，并且卡在％处，我可以再次启动它，然后从上一个成功合并的帧开始合并吗？</strong></p>\n<p>A：是的，默认情况下，交互式转换器 / 合并在 “模型” 文件夹中创建会话文件，该文件同时保存进度和设置。</p>\n<p>如果您只想暂停训练，则可以单击 &gt;，它将暂停。 但是，如果您需要将其完全关闭 / 重新启动 PC 等，则退出了与 esc 的合并，并等待它保存进度，下次选择合并 / 转换器（Y / N）后，下次启动合并时 - 是，您会 将会提示您是否要使用保存 / 会话文件并恢复进度，合并将在正确的框架处以正确的设置加载。</p>\n<p>如果合并失败并且没有保存进度，则必须手动恢复它，方法是先备份 “data_dst” 文件夹，然后删除 data_dst 中所有提取的帧以及 “ aligned” 文件夹中的所有图像 在 “ data_dst” 内部，对应于已在文件夹 “ merged” 中转换 / 合并的帧。 然后，只需启动合并 / 转换器，输入之前使用的设置，然后转换其余帧，然后从备份的 “ data_dst” 文件夹中将新合并的帧与旧的合并，并照常转换为.mp4 即可。</p>\n<p><strong>28. Q：训练期间预览中的面孔看起来不错，但转换后看起来很糟。 我看到了原始脸的一部分（下巴，眉毛，双脸轮廓）。</strong></p>\n<p>A：预览中的面孔是 AI 的原始输出，然后需要在原始素材上进行合成。因此，当人脸形状不同或稍小 / 较大时，您可能会在 DFL 合并创建的蒙版周围 / 外部看到原始人脸的一部分。 要解决此问题，您需要更改转换设置，方法是：</p>\n<p>- 调整遮罩类型</p>\n<p>- 调整遮罩腐蚀（大小）和模糊（羽化，使边缘平滑）</p>\n<p>- 调整脸部大小（比例）</p>\n<p>注意：负腐蚀会增加面罩的尺寸（覆盖更多），正腐蚀会减小面罩的尺寸。</p>\n<p><strong>29. Q：使用 “无缝” 模式时，最终结果 / 深层伪影具有怪异的伪影，面部变化的颜色，背景的颜色渗出，并使其在角落 / 边缘的颜色闪烁 / 变暗 / 更改颜色。</strong></p>\n<p>A：您正在使用无缝 / 历史 / 无缝 + 历史叠加模式，或者使用具有变化光照条件的源数据集 / 面集训练了模型，并且在训练过程中未使用任何颜色转移。 - 使用覆盖或除无缝 / 历史 / 无缝 + 历史之外的任何其他模式 - 如果要使用无缝： - 减小遮罩 / 脸部的大小，以使其不会 “触摸” 外部区域，因此不会通过增加 “侵蚀遮罩” 值来获得脸部 / 头部外部的背景 / 区域的颜色。 - 或通过增加 “模糊蒙版”（Blur Mask）值来平滑蒙版 / 脸部的边缘，这可能会掩盖某些颜色变化，还有助于使脸部看起来更…“无缝”（当您减小蒙版大小时）。 如果仍然持续使用上述简单叠加模式，则这两种方法都可能会或可能不会解决问题。</p>\n<p>如果您的源数据集包含具有不同光照条件的面部图像并且未使用颜色转移，则可能需要返回并继续启用颜色转移进行更多训练。 万一打开它会严重洗掉颜色或以不好的方式影响训练数据 / 面部的颜色（褪色，错误的颜色，过度饱和的颜色，噪点）或使学习的面部模糊（由于模型的变化太大）必须全面学习，好像源数据集和目标数据集中有新面孔一样），您可能需要保存界标数据并编辑源数据集颜色以更好地匹配目标数据集，并且变化较少。</p>\n<p>我建议除非绝对必要，否则不要使用无缝模式，即使如此，我还是建议在每个主要角度和相机移动 / 光线变化时停下来看看它是否不会引起那些伪影。</p>\n<p><strong>30. Q：半脸，半脸，全脸和全脸 face_type 模式有什么区别？</strong></p>\n<p>A：全脸是覆盖整个脸部 / 头部的新模式，这意味着它也覆盖了整个额头，甚至覆盖了某些头发和其他特征，这些特征可能会被全脸模式剪切掉，并且在使用一半或一半时绝对不会出现脸部模式。它还在训练过程中带有新选项，让您训练称为 masked_training 的额头。首先，先启用它，然后将训练蒙版剪切到整个脸部区域，一旦对脸部进行了足够的训练，就禁用它并训练整个脸部 / 头部。此模式需要在后期手动屏蔽或训练自己的 XSeg 模型：<a href=\"https://mrdeepfakes.com/forums/thread-gu...g-tutorial\">https://mrdeepfakes.com/forums/thread-gu...g-tutorial</a></p>\n<p>建议使用全脸 face_type 模式，以尽可能多地遮盖脸部，而无需多余的东西（发际线，额头和头部其他部位） 半脸模式是 H64 和 H128 模型中的默认 face_type 模式。它只覆盖一半的脸（从嘴到眉毛以下） 半脸是覆盖半脸约 30％区域的一种模式。</p>\n<p><strong>31. Q：什么是最适合深度伪造的 GPU？ 我想升级我的 GPU，我应该得到哪一个？</strong></p>\n<p>A：20 系最好，30 系等更新</p>\n<p><strong>32. Q：AutoEncoder，Encoder，Decoder 和 D_Mask_Decoder 维度设置有什么作用？ 更改它们有什么作用？</strong></p>\n<p>A：可以更改它们以提高性能或质量，将它们设置为高将使模型真的很难训练（缓慢，高使用 vram），但会提供更准确的结果和更多的 src，如外观，将其设置为低将提高性能但是结果将不太准确，并且模型可能无法学习人脸的某些特征，从而导致通用输出看起来更像 dst 或什么都不像 dst 或 src。 自动编码器尺寸（32-1024？：help）：这是学习的整体模型能力。 价值太低，将无法学习所有内容 - 更高的价值将使模型能够学习更多表达式，并且以性能为代价更加准确。</p>\n<p>编码器尺寸（16-256？：help）：这会影响模型学习不同表情，面部状态，角度，照明条件的能力。 值太低，模型可能无法学习某些表达式，模型可能无法闭上眼睛，嘴巴，某些角度的细节可能不够准确，较高的值将导致模型更加准确和富有表现力，前提是 AE 昏暗程度会相应提高。性能成本。</p>\n<p>解码器尺寸（16-256？：help）：这会影响模型学习精细细节，纹理，牙齿，眼睛的能力，这些微小的事物会使人的面部变得细腻且可识别。 值太低将导致无法学习某些细节（例如牙齿和眼睛看起来模糊，缺少纹理），也可能无法正确学习一些微妙的表情和面部特征 / 纹理，从而导致像面部表情一样的 src 更少，价值更高将使面部更加细化，模型将能够以性能为代价选择更多这些细微的细节。</p>\n<p>解码器蒙版尺寸（16-256？：help）：在启用学习蒙版的情况下进行训练时，会影响学习的蒙版的质量。不影响培训质量。</p>\n<p><strong>33. Q：推荐的批量大小是多少？ 我应该设置多大的批量大小？ 批量大小可以设置多低？</strong></p>\n<p>A：没有建议的批量大小，但是合理的值在 8-12 之间，其中 16-22 以上的值非常好，最小 4-6 的值。 批次大小 2 不足以正确训练模型，因此建议的最小值为 4，值越大越好，但是在某些时候批次大小可能不利，尤其是在迭代时间开始增加或您有禁用 models_opt_on_gpu - 从而在 CPU 上强制优化器，这会减慢训练速度 / 增加迭代时间。 您可以通过将迭代时间除以批次大小来计算何时增加批次大小变得效率较低。选择可以在给定的迭代时间内为每个批次降低 ms 值的批次大小，例如：</p>\n<p>批次 8-1000/8 = 1000 时 1000 ms 批处理 1500 毫秒 10-1500/10 = 150</p>\n<p>在这种情况下，与批次 10 相比，在批次 8 中运行将在给定时间内提供更多的数据模型，但是差异很小。如果说我们要使用批处理 12，但得到一个 OOM - 因此我们禁用 models_opt_on_gpu，它现在看起来可能像这样： 批次 12 时为 2300 毫秒（CPU 上的 Optimizer）-2300/12 = 191 毫秒，这比批次 8 和迭代时间为 1000 毫秒的 128 毫秒长得多。</p>\n<p>启动模型时，最好使用较小的批处理大小 - 较长的迭代时间，然后在禁用随机扭曲时增加它。</p>\n<p><strong>34. Q：如何使用预训练模型？</strong></p>\n<p>A：只需下载它，然后将所有文件直接放入模型文件夹即可。 开始训练，在选择要训练的模型（如果文件夹中还有更多）和要训练的设备（GPU / CPU）之后的 2 秒钟内按任意键，以覆盖模型设置并确保禁用预训练选项，以便正确启动 训练中，如果您启用了预训练选项，则模型将继续进行预训练。 请注意，模型会将迭代计数还原为 0，这是预训练模型的正常行为，这与不使用预训练功能而只是在随机面孔上训练的模型不同。</p>\n<p><strong>35. Q：我的 GPU 使用率非常低，尽管选择了 GPU 进行训练 / 合并，也没有使用 GPU。</strong></p>\n<p>A：它可能正在使用中，但是 Windows 不仅报告 CUDA 使用情况（这是您应该查看的），而且 GPU 的总使用情况可能会更低（大约 5-10％）。 要在培训期间（在 Windows 10 中）查看 CUDA / GPU 的真实使用情况，请进入任务管理器 -&gt; 性能 -&gt; 选择 GPU-&gt; 将 4 个较小的图形之一更改为 CUD。</p>\n<p>如果您使用的是其他版本的 Windows，请下载外部监视软件（例如 HWmonitor 或 GPU-Z），或者查看 VRAM 的使用情况，该使用率应接近培训期间的最大值。</p>\n<h1 id=\"转载来源\"><a href=\"#转载来源\" class=\"headerlink\" title=\"转载来源\"></a>转载来源</h1><p><a href=\"http://dfldata.xyz/forum.php?mod=viewthread&tid=116&page=1&extra=#pid1515\">【必看】DFL 官方使用说明【已汉化】 - 软件下载与教程 - Discuz! Board - Powered by Discuz!</a></p>\n","categories":["DeepFace, AI Security"]},{"title":"DeepFake检测","url":"/2021/02/07/deep_detection/","content":"<h1 id=\"DeepFake-Dectection\"><a href=\"#DeepFake-Dectection\" class=\"headerlink\" title=\"DeepFake Dectection\"></a>DeepFake Dectection</h1><p>读完后的一些思考：</p>\n<ul>\n<li><p>定义什么叫做fake视频，视频中只要出现fake换脸就定义为fake，不论换了多少个人或者换了多少帧。</p>\n</li>\n<li><p>检测大多是都是基于帧间的时序特征来建模。提取帧后当作伪造image检测来做。但是存在的问题是，视频中会存在多个人，一部分换脸一部分不换脸，对每一帧的分数做平均后会影响最终对视频的整体评判打分。</p>\n</li>\n<li><p>检测方法比较费算力和存储资源，模型很大并且很难泛化到未知换脸算法。</p>\n</li>\n<li><p>牙齿通常根本没有建模。这一点在很多视频中都很明显，在这些视频中，牙齿看起来是一个白色的小点，而不是单个的牙齿。能否根据这个特征建模？或者将人脸进行切割，不同部分进行不同建模？</p>\n</li>\n<li><p>为了避免针对每一帧都进行检测，能否将一段时间的视频帧融合求一个特征？来判断这一段时间内是否是fake。</p>\n</li>\n<li><p>git上的一些检测论文note：<a href=\"https://github.com/592McAvoy/fake-face-detection/blob/master/detectmd#arxiv-2018-forensictransfer-weakly-supervised-domain-adaptation-for-forgery-detection\">https://github.com/592McAvoy/fake-face-detection/blob/master/detectmd#arxiv-2018-forensictransfer-weakly-supervised-domain-adaptation-for-forgery-detection</a></p>\n</li>\n</ul>\n<h2 id=\"基于video的检测\"><a href=\"#基于video的检测\" class=\"headerlink\" title=\"基于video的检测\"></a>基于video的检测</h2><blockquote>\n<p>总体流程大致相似，先将视频转为帧，然后用CNN特征提取，再用RNN做时序建模。缺点是帧级标注成本较高，过多关注时序建模。都是在堆模型。</p>\n</blockquote>\n<h3 id=\"Deepfake-Video-Detection-Using-Recurrent-Neural-Networks-（2018）\"><a href=\"#Deepfake-Video-Detection-Using-Recurrent-Neural-Networks-（2018）\" class=\"headerlink\" title=\"Deepfake Video Detection Using Recurrent Neural Networks （2018）\"></a><em>Deepfake Video Detection Using Recurrent Neural Networks （2018）</em></h3><p><strong>摘要：</strong> 使用CNN去提取帧（frame）级别的特征，然后训练一个RNN去做判别器。</p>\n<p><strong>检测思想：</strong> 由于deepfake是一帧一帧的进行替换，因此当前帧的替换并不会考虑上一帧的结果，这种时序上关联的缺失会造成一些异常现象，比如脸部替换区域的闪动，尽管这些异常现象对人眼来说不那么明显，但是会被pixel-level的CNN捕捉到，生成的视频会有不正确的<strong>颜色恒常性</strong>。</p>\n<p>fake视频训练方法：</p>\n<p><img src=\"dfl-2021-01-27-14-42-05.png\"></p>\n<p><strong>检测模型结构：</strong> CNN(InceptionV3) + LSTM</p>\n<p>CNN（Inception V3）做帧特征的提取输出2048维特征向量，接一个LSTM做时序序列的分析输出real和fake的概率。</p>\n<p><img src=\"dfl-2021-01-27-14-59-08.png\"></p>\n<h3 id=\"CVPR-2019-Recurrent-Convolutional-Strategies-for-Face-Manipulation-Detection-in-Videos\"><a href=\"#CVPR-2019-Recurrent-Convolutional-Strategies-for-Face-Manipulation-Detection-in-Videos\" class=\"headerlink\" title=\"[CVPR 2019] Recurrent Convolutional Strategies for Face Manipulation Detection in Videos\"></a><em>[CVPR 2019] Recurrent Convolutional Strategies for Face Manipulation Detection in Videos</em></h3><p><strong>摘要：</strong> 和上文相似，也是CNN+RNN的方法，也是利用的时序上的特点进行检测，无非就是具体细节有一些不同，利用到了人脸对齐的操作去除不相干的因素，二是使用了双向RNN网络进行建模。</p>\n<p><img src=\"dfl-2021-01-27-15-27-06.png\"></p>\n<p><strong>检测模型结构：</strong> CNN (DenseNet) + bidirectional RNN</p>\n<h3 id=\"（CVPR-Workshops-2020）Deepfakes-Detection-with-Automatic-Face-Weighting\"><a href=\"#（CVPR-Workshops-2020）Deepfakes-Detection-with-Automatic-Face-Weighting\" class=\"headerlink\" title=\"（CVPR Workshops 2020）Deepfakes Detection with Automatic Face Weighting\"></a><em>（CVPR Workshops 2020）Deepfakes Detection with Automatic Face Weighting</em></h3><p><strong>摘要：</strong> 用到了DFDC作为测试集，同样也是CNN+RNN的检测模型结构。DFDC比赛top 6%</p>\n<p><strong>技术细节：</strong> 引入了Automatic Face Weighting ，对人脸的重要区域做一个加权，传统做法是对每一个frame做一个预测打分然后平均，但是每一个经过人脸检测得到的frame的质量不相同（如同时出现多人只有一人被换脸、模糊人脸等），因此加入了加权步骤，对efficientNet输出的每一个logit进行加权，得到的features、logit、weight、输入到GRU的RNN中进行最终预测。</p>\n<p><strong>检测模型结构：</strong> CNN(EfficientNet) + RNN（GRU）</p>\n<p><img src=\"dfl-2021-01-27-15-58-12.png\"></p>\n<h3 id=\"（MultiMedia-2020）-Sharp-Multiple-Instance-Learning-for-DeepFake-Video-Detection-阿里方法\"><a href=\"#（MultiMedia-2020）-Sharp-Multiple-Instance-Learning-for-DeepFake-Video-Detection-阿里方法\" class=\"headerlink\" title=\"（MultiMedia 2020） Sharp Multiple Instance Learning for DeepFake Video Detection 阿里方法\"></a><em>（MultiMedia 2020） Sharp Multiple Instance Learning for DeepFake Video Detection 阿里方法</em></h3><p><strong>摘要：</strong> 介绍了一种视频级别的检测方法，将检测问题转换为多实例学习，一个视频中，只要有一帧是fake的，这个视频就应该被定义为fake，此外，一个视频中出现多个人，有一个人被换脸，也应被定义为fake。这种检测场景和多实例学习是相似的。在多实例学习中，一个包由多个实例组成，只要其中有一个实例是正类，那么该包就是正类的，否则就是负类。每一个视频只需要打一个标签，而不是在帧级别上打标签。</p>\n<h3 id=\"arXiv-2018-ForensicTransfer-Weakly-supervised-Domain-Adaptation-for-Forgery-Detection\"><a href=\"#arXiv-2018-ForensicTransfer-Weakly-supervised-Domain-Adaptation-for-Forgery-Detection\" class=\"headerlink\" title=\"[arXiv 2018] ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery Detection\"></a>[arXiv 2018] ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery Detection</h3><p><strong>摘要：</strong> 传统的方法容易在特定的扰动方法上过拟合，本文提出了一种可以迁移的fake detector训练方法。</p>\n<p><strong>模型架构：</strong> autoencoder<br>利用数据在encoder之后的中间潜在向量信息</p>\n<p><img src=\"dfl-2021-01-27-21-03-46.png\"></p>\n<p><strong>检测流程：</strong> </p>\n<p><img src=\"dfl-2021-01-27-21-03-38.png\"></p>\n"},{"title":"DeepFake检测","url":"/2021/02/07/deepdetection/deep_detection/","content":"<h1 id=\"DeepFake-Dectection\"><a href=\"#DeepFake-Dectection\" class=\"headerlink\" title=\"DeepFake Dectection\"></a>DeepFake Dectection</h1><p>读完后的一些思考：</p>\n<ul>\n<li>定义什么叫做fake视频，视频中只要出现fake换脸就定义为fake，不论换了多少个人或者换了多少帧。</li>\n<li>检测大多是都是基于帧间的时序特征来建模。提取帧后当作伪造image检测来做。但是存在的问题是，视频中会存在多个人，一部分换脸一部分不换脸，对每一帧的分数做平均后会影响最终对视频的整体评判打分。</li>\n<li>检测方法比较费算力和存储资源，模型很大并且很难泛化到未知换脸算法。</li>\n<li>牙齿通常根本没有建模。这一点在很多视频中都很明显，在这些视频中，牙齿看起来是一个白色的小点，而不是单个的牙齿。能否根据这个特征建模？或者将人脸进行切割，不同部分进行不同建模？</li>\n<li><p>为了避免针对每一帧都进行检测，能否将一段时间的视频帧融合求一个特征？来判断这一段时间内是否是fake。</p>\n</li>\n<li><p>git上的一些检测论文note：<a href=\"https://github.com/592McAvoy/fake-face-detection/blob/master/detect.md#cvpr-2019-recurrent-convolutional-strategies-for-face-manipulation-detection-in-videos\">https://github.com/592McAvoy/fake-face-detection/blob/master/detect.md#cvpr-2019-recurrent-convolutional-strategies-for-face-manipulation-detection-in-videos</a></p>\n</li>\n</ul>\n<h2 id=\"基于video的检测\"><a href=\"#基于video的检测\" class=\"headerlink\" title=\"基于video的检测\"></a>基于video的检测</h2><blockquote>\n<p>总体流程大致相似，先将视频转为帧，然后用CNN特征提取，再用RNN做时序建模。缺点是帧级标注成本较高，过多关注时序建模。都是在堆模型。</p>\n</blockquote>\n<h3 id=\"Deepfake-Video-Detection-Using-Recurrent-Neural-Networks-（2018）\"><a href=\"#Deepfake-Video-Detection-Using-Recurrent-Neural-Networks-（2018）\" class=\"headerlink\" title=\"Deepfake Video Detection Using Recurrent Neural Networks （2018）\"></a><em>Deepfake Video Detection Using Recurrent Neural Networks （2018）</em></h3><p><strong>摘要：</strong> 使用CNN去提取帧（frame）级别的特征，然后训练一个RNN去做判别器。</p>\n<p><strong>检测思想：</strong> 由于deepfake是一帧一帧的进行替换，因此当前帧的替换并不会考虑上一帧的结果，这种时序上关联的缺失会造成一些异常现象，比如脸部替换区域的闪动，尽管这些异常现象对人眼来说不那么明显，但是会被pixel-level的CNN捕捉到，生成的视频会有不正确的<strong>颜色恒常性</strong>。</p>\n<p>fake视频训练方法：</p>\n<p><img src=\"dfl-2021-01-27-14-42-05.png\" alt=\"\"></p>\n<p><strong>检测模型结构：</strong> CNN(InceptionV3) + LSTM</p>\n<p>CNN（Inception V3）做帧特征的提取输出2048维特征向量，接一个LSTM做时序序列的分析输出real和fake的概率。</p>\n<p><img src=\"dfl-2021-01-27-14-59-08.png\" alt=\"\"></p>\n<h3 id=\"CVPR-2019-Recurrent-Convolutional-Strategies-for-Face-Manipulation-Detection-in-Videos\"><a href=\"#CVPR-2019-Recurrent-Convolutional-Strategies-for-Face-Manipulation-Detection-in-Videos\" class=\"headerlink\" title=\"[CVPR 2019] Recurrent Convolutional Strategies for Face Manipulation Detection in Videos\"></a><em>[CVPR 2019] Recurrent Convolutional Strategies for Face Manipulation Detection in Videos</em></h3><p><strong>摘要：</strong> 和上文相似，也是CNN+RNN的方法，也是利用的时序上的特点进行检测，无非就是具体细节有一些不同，利用到了人脸对齐的操作去除不相干的因素，二是使用了双向RNN网络进行建模。</p>\n<p><img src=\"dfl-2021-01-27-15-27-06.png\" alt=\"\"></p>\n<p><strong>检测模型结构：</strong> CNN (DenseNet) + bidirectional RNN</p>\n<h3 id=\"（CVPR-Workshops-2020）Deepfakes-Detection-with-Automatic-Face-Weighting\"><a href=\"#（CVPR-Workshops-2020）Deepfakes-Detection-with-Automatic-Face-Weighting\" class=\"headerlink\" title=\"（CVPR Workshops 2020）Deepfakes Detection with Automatic Face Weighting\"></a><em>（CVPR Workshops 2020）Deepfakes Detection with Automatic Face Weighting</em></h3><p><strong>摘要：</strong> 用到了DFDC作为测试集，同样也是CNN+RNN的检测模型结构。DFDC比赛top 6%</p>\n<p><strong>技术细节：</strong> 引入了Automatic Face Weighting ，对人脸的重要区域做一个加权，传统做法是对每一个frame做一个预测打分然后平均，但是每一个经过人脸检测得到的frame的质量不相同（如同时出现多人只有一人被换脸、模糊人脸等），因此加入了加权步骤，对efficientNet输出的每一个logit进行加权，得到的features、logit、weight、输入到GRU的RNN中进行最终预测。</p>\n<p><strong>检测模型结构：</strong> CNN(EfficientNet) + RNN（GRU）</p>\n<p><img src=\"dfl-2021-01-27-15-58-12.png\" alt=\"\"></p>\n<h3 id=\"（MultiMedia-2020）-Sharp-Multiple-Instance-Learning-for-DeepFake-Video-Detection-阿里方法\"><a href=\"#（MultiMedia-2020）-Sharp-Multiple-Instance-Learning-for-DeepFake-Video-Detection-阿里方法\" class=\"headerlink\" title=\"（MultiMedia 2020） Sharp Multiple Instance Learning for DeepFake Video Detection 阿里方法\"></a><em>（MultiMedia 2020） Sharp Multiple Instance Learning for DeepFake Video Detection 阿里方法</em></h3><p><strong>摘要：</strong> 介绍了一种视频级别的检测方法，将检测问题转换为多实例学习，一个视频中，只要有一帧是fake的，这个视频就应该被定义为fake，此外，一个视频中出现多个人，有一个人被换脸，也应被定义为fake。这种检测场景和多实例学习是相似的。在多实例学习中，一个包由多个实例组成，只要其中有一个实例是正类，那么该包就是正类的，否则就是负类。每一个视频只需要打一个标签，而不是在帧级别上打标签。</p>\n<h3 id=\"arXiv-2018-ForensicTransfer-Weakly-supervised-Domain-Adaptation-for-Forgery-Detection\"><a href=\"#arXiv-2018-ForensicTransfer-Weakly-supervised-Domain-Adaptation-for-Forgery-Detection\" class=\"headerlink\" title=\"[arXiv 2018] ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery Detection\"></a>[arXiv 2018] ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery Detection</h3><p><strong>摘要：</strong> 传统的方法容易在特定的扰动方法上过拟合，本文提出了一种可以迁移的fake detector训练方法。</p>\n<p><strong>模型架构：</strong> autoencoder<br>利用数据在encoder之后的中间潜在向量信息</p>\n<p><img src=\"dfl-2021-01-27-21-03-46.png\" alt=\"\"></p>\n<p><strong>检测流程：</strong> </p>\n<p><img src=\"dfl-2021-01-27-21-03-38.png\" alt=\"\"></p>\n","categories":["DeepFace, AI Security"]},{"title":"天池对抗鲁棒","url":"/2021/02/07/adversarial%20competition/adversarial%20competition/adversarial-competition/","content":"<h2 id=\"FAST-IS-BETTER-THAN-FREE-REVISITING-ADVERSARIAL-TRAINING\"><a href=\"#FAST-IS-BETTER-THAN-FREE-REVISITING-ADVERSARIAL-TRAINING\" class=\"headerlink\" title=\"FAST IS BETTER THAN FREE: REVISITING ADVERSARIAL TRAINING\"></a>FAST IS BETTER THAN FREE: REVISITING ADVERSARIAL TRAINING</h2><p><strong>摘要：</strong> 这篇文章提出了一种比基于PGD进行对抗训练更高效的方法，传统的基于PGD的训练方法时间开销很大，本文发现，可以利用FGSM和随机初始化结合达到与PGD相同的模型鲁棒性，并且时间开销更小。并且结合了一些传统的提高机器学习训练效率的方法进行性能提升。</p>\n<p><strong>Tips：</strong></p>\n<ul>\n<li>基于PGD的对抗训练方法<br><img src=\"dfl-2021-02-01-15-11-46.png\" alt=\"\"></li>\n<li>在本文之前，已经有基于FGSM的对抗训练方法提出，叫做“FREE” ADVERSARIAL TRAINING，但是虽然free方法要比基于PGD的更快，但是，还是还是没有达到本文的性能。perturbation没有两个minibatch之间进行重置。<br><img src=\"mk-![](mk-2021-02-07-22-21-58.png\" alt=\"\">.png)</li>\n<li>本文的贡献：将FGSM和随机初始化结合，达到和PGD对抗训练相同的效果，并用了一些公开的加快训练速度的方法提升性能。<br>该工作可以看做是对free的一个改进，增加了随机初始化的步骤，因为没有理论能够证明在FGSM方法下，上一个minibatch得到的perturbation对于下一个minibatch来说是合理的。可以从伪代码看出，Uniform了一个随机扰动<br><img src=\"mk-2021-02-07-22-43-18.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"Adversarial-Training-for-Free\"><a href=\"#Adversarial-Training-for-Free\" class=\"headerlink\" title=\"Adversarial Training for Free!\"></a>Adversarial Training for Free!</h2><p><strong>摘要：</strong> 减少对抗训练所需要的时间开销并达到与PGD相同的鲁棒性。算法核心思想是使用一个同时backward pass的步骤，老更新模型参数和perturbation，而不是对每个更新步骤使用单独的梯度计算。</p>\n<p><img src=\"mk-2021-02-08-00-05-03.png\" alt=\"\"></p>\n","categories":["Adversarial training, AI Security"]}]