<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"gaojiqiang.github.io","root":"/","scheme":"Pisces","version":"8.0.2","exturl":false,"sidebar":{"position":"left","width":250,"display":"always","padding":10,"offset":12,"scrollpercent":true},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="简介一、什么是 DeepFaceLab 2.0DeepFaceLab 是目前使用人数最多，效果最好的 AI 换脸软件 1.0 和 2.0 有什么区别？ DFL 2.0 有什么新功能？  DFL 2.0 的核心与 1.0 非常相似，但是对它进行了重写和优化，以使其运行速度更快并提供更好的质量。 不再支持 AMD 卡，并且新模型（SAEHD 系列和 Quick96）与以前的版本不兼容。 但是，在高版本">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepFaceLab">
<meta property="og:url" content="https://gaojiqiang.github.io/2021/01/25/DFL%E7%AE%80%E4%BB%8B/index.html">
<meta property="og:site_name" content="G Blog">
<meta property="og:description" content="简介一、什么是 DeepFaceLab 2.0DeepFaceLab 是目前使用人数最多，效果最好的 AI 换脸软件 1.0 和 2.0 有什么区别？ DFL 2.0 有什么新功能？  DFL 2.0 的核心与 1.0 非常相似，但是对它进行了重写和优化，以使其运行速度更快并提供更好的质量。 不再支持 AMD 卡，并且新模型（SAEHD 系列和 Quick96）与以前的版本不兼容。 但是，在高版本">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E9%A2%84%E8%A7%88%E7%AA%97%E5%8F%A3.jpg">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E8%84%B8%E5%9E%8B.jpg">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E6%95%B4%E8%84%B8%E6%A1%88%E4%BE%8B.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%BC%E7%9D%9B%E4%BC%98%E5%85%88.jpg">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_GAN.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%9F%E5%AE%9E%E9%9D%A2%E9%83%A8%E6%9D%83%E9%87%8D.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E5%BF%AB%E9%80%9F%E6%B5%8F%E8%A7%88.png">
<meta property="article:published_time" content="2021-01-25T12:27:00.187Z">
<meta property="article:modified_time" content="2021-01-25T12:27:07.696Z">
<meta property="article:author" content="Jiqiang Gao">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E9%A2%84%E8%A7%88%E7%AA%97%E5%8F%A3.jpg">


<link rel="canonical" href="https://gaojiqiang.github.io/2021/01/25/DFL%E7%AE%80%E4%BB%8B/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>DeepFaceLab | G Blog</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">G Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Stay Hungry, Stay Foolish</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF-DeepFaceLab-2-0"><span class="nav-number">1.1.</span> <span class="nav-text">一、什么是 DeepFaceLab 2.0</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E8%BD%AF%E4%BB%B6%E4%B8%8B%E8%BD%BD"><span class="nav-number">1.2.</span> <span class="nav-text">二、软件下载</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E6%96%B0%E6%89%8B%E6%95%99%E7%A8%8B"><span class="nav-number">1.3.</span> <span class="nav-text">三、新手教程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A"><span class="nav-number">1.4.</span> <span class="nav-text">四、名词解释</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DFL-2-0-%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C%E7%BF%BB%E8%AF%91"><span class="nav-number">2.</span> <span class="nav-text">DFL 2.0 官方使用手册翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-0-%E4%B8%BB%E8%A6%81%E6%96%B0%E5%A2%9E%E5%8A%9F%E8%83%BD"><span class="nav-number">2.1.</span> <span class="nav-text">2.0 主要新增功能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-0-%E7%A1%AC%E4%BB%B6%E8%A6%81%E6%B1%82"><span class="nav-number">2.2.</span> <span class="nav-text">2.0 硬件要求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%B7%A5%E4%BD%9C%E5%8C%BA%E6%B8%85%E7%90%86"><span class="nav-number">2.3.</span> <span class="nav-text">1. 工作区清理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E4%BB%8E-src-%E8%A7%86%E9%A2%91%E4%B8%AD%E6%8F%90%E5%8F%96%E7%94%BB%E9%9D%A2-data-src-mp4"><span class="nav-number">2.4.</span> <span class="nav-text">2. 从 src 视频中提取画面 (data_src.mp4)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E8%A7%86%E9%A2%91%E5%88%87%E5%89%B2-%EF%BC%88%E5%8F%AF%E9%80%89%E7%8E%AF%E8%8A%82%EF%BC%89"><span class="nav-number">2.5.</span> <span class="nav-text">3. 视频切割 （可选环节）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E4%BB%8E%E7%9B%AE%E6%A0%87%E8%A7%86%E9%A2%91%E4%B8%AD%E6%8F%90%E5%8F%96%E7%94%BB%E9%9D%A2-data-dst-mp4"><span class="nav-number">2.6.</span> <span class="nav-text">3. 从目标视频中提取画面 (data_dst.mp4)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E6%8F%90%E5%8F%96-Data-src-%E4%B8%AD%E7%9A%84%E4%BA%BA%E8%84%B8"><span class="nav-number">2.7.</span> <span class="nav-text">4. 提取 Data_src 中的人脸</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Data-src-%E6%95%B4%E7%90%86"><span class="nav-number">2.8.</span> <span class="nav-text">4. Data_src 整理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Data-dst-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">2.9.</span> <span class="nav-text">5. Data_dst 数据准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E6%89%8B%E5%8A%A8%E4%BA%BA%E8%84%B8%E6%8F%90%E5%8F%96%E7%9A%84%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E"><span class="nav-number">2.10.</span> <span class="nav-text">5.1 手动人脸提取的操作说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Data-dst-%E6%95%B0%E6%8D%AE%E6%95%B4%E7%90%86"><span class="nav-number">2.11.</span> <span class="nav-text">5.2 Data_dst 数据整理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-XSeg-model-%E7%9A%84%E8%AE%AD%E7%BB%83%E5%92%8C%E4%BD%BF%E7%94%A8%EF%BC%88%E7%94%BB%E9%81%AE%E7%BD%A9%EF%BC%89"><span class="nav-number">2.12.</span> <span class="nav-text">5.3 XSeg model 的训练和使用（画遮罩）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E8%AE%AD%E7%BB%83"><span class="nav-number">2.13.</span> <span class="nav-text">6. 训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Merging-%E5%90%88%E6%88%90"><span class="nav-number">2.14.</span> <span class="nav-text">7. Merging 合成:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E6%8A%8A%E8%BD%AC%E5%8C%96%E5%A5%BD%E7%9A%84%E5%B8%A7%E5%90%88%E6%88%90%E4%B8%BA%E8%A7%86%E9%A2%91"><span class="nav-number">2.15.</span> <span class="nav-text">8. 把转化好的帧合成为视频</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#FAQ"><span class="nav-number">3.</span> <span class="nav-text">FAQ</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BD%AC%E8%BD%BD%E6%9D%A5%E6%BA%90"><span class="nav-number">4.</span> <span class="nav-text">转载来源</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiqiang Gao"
      src="/images/icon.jpg">
  <p class="site-author-name" itemprop="name">Jiqiang Gao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/GaoJiqiang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;GaoJiqiang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:617624576@qq.com" title="E-Mail → mailto:617624576@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://gaojiqiang.github.io/2021/01/25/DFL%E7%AE%80%E4%BB%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/icon.jpg">
      <meta itemprop="name" content="Jiqiang Gao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="G Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DeepFaceLab
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-01-25 20:27:00 / Modified: 20:27:07" itemprop="dateCreated datePublished" datetime="2021-01-25T20:27:00+08:00">2021-01-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/DeepFace-AI-Security/" itemprop="url" rel="index"><span itemprop="name">DeepFace, AI Security</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="一、什么是-DeepFaceLab-2-0"><a href="#一、什么是-DeepFaceLab-2-0" class="headerlink" title="一、什么是 DeepFaceLab 2.0"></a>一、什么是 DeepFaceLab 2.0</h2><p>DeepFaceLab 是目前使用人数最多，效果最好的 AI 换脸软件</p>
<p>1.0 和 2.0 有什么区别？ DFL 2.0 有什么新功能？</p>
<ul>
<li>DFL 2.0 的核心与 1.0 非常相似，但是对它进行了重写和优化，以使其运行速度更快并提供更好的质量。</li>
<li>不再支持 AMD 卡，并且新模型（SAEHD 系列和 Quick96）与以前的版本不兼容。</li>
<li>但是，在高版本的 DFL 1.0 中提取的数据集仍可以在 2.0 中使用。</li>
</ul>
<h2 id="二、软件下载"><a href="#二、软件下载" class="headerlink" title="二、软件下载"></a>二、软件下载</h2><p>官方代码仓库（适合源码大佬研究）：<a target="_blank" rel="noopener" href="https://github.com/iperov/DeepFaceLab">https://github.com/iperov/DeepFaceLab</a></p>
<p>提供几个坛友分享的国内下载链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://dfldata.xyz/forum.php?mod=viewthread&tid=42&extra=page=1">《DeepFaceLab2020-8-2 中文版分享》</a></li>
<li><a target="_blank" rel="noopener" href="http://dfldata.xyz/forum.php?mod=viewthread&tid=46&extra=page=1">汉化包 + 交互界面 免费搬运 回帖有奖，注册会员即可观看</a></li>
</ul>
<h2 id="三、新手教程"><a href="#三、新手教程" class="headerlink" title="三、新手教程"></a>三、新手教程</h2><p><a target="_blank" rel="noopener" href="http://dfldata.xyz/forum.php?mod=viewthread&tid=34&extra=page=1">【新手教程点我】</a></p>
<h2 id="四、名词解释"><a href="#四、名词解释" class="headerlink" title="四、名词解释"></a>四、名词解释</h2><p>逛论坛时，大家经常说着一些炼丹、神丹、预训练等词，听得萌新一愣一愣，在此解释下。</p>
<p><strong>丹</strong>：AI 换脸模型</p>
<p><strong>炼丹</strong>：训练模型的过程</p>
<p><strong>筑基丹、神丹</strong>：别人训练好的模型，拿过来用自己素材重新训练下很快就能出效果</p>
<p><strong>预训练</strong>：在训练自己特定素材前，用其他角度丰富的人脸数据给模型预热的过程</p>
<p><strong>炉子</strong>：显卡</p>
<h1 id="DFL-2-0-官方使用手册翻译"><a href="#DFL-2-0-官方使用手册翻译" class="headerlink" title="DFL 2.0 官方使用手册翻译"></a>DFL 2.0 官方使用手册翻译</h1><h2 id="2-0-主要新增功能"><a href="#2-0-主要新增功能" class="headerlink" title="2.0 主要新增功能"></a>2.0 主要新增功能</h2><ul>
<li>2 个模型大类：SAEHD 和 Quick 96。</li>
<li>支持多 GPU。</li>
<li>比 DFL 1.0 更快的提脸、训练、合成速度。</li>
<li>提供人脸素材加强脚本。</li>
<li>训练支持 GAN（一种 AI 技术），以还原更多细节。</li>
<li>新增 TrueFace 参数 - (只支持 DF 模型架构) - 让 SRC 和 DST 表情更相似，减少死鱼脸。</li>
<li>合成阶段附带输出遮罩图片， 方便后期导入其他软件编辑。</li>
<li>带交互界面的合成脚本（1.0 的合成需要手动填参数，非常反人类，这个 2.0 做的很人性化）。</li>
<li>提脸使用 s3fd 算法，并支持手动模式。</li>
<li>模型分辨率可选择为任意 16 或 32 倍数。</li>
<li>多种模型架构 (两种核心框架 DF, LIAE, 以及 -U, -D ,-UD 三种附加后缀，这个下文详细展开)</li>
<li>使用 Xseg 遮罩模型，提供自带画遮罩的工具。（滚石注：对追求细节的高玩适合用 Xseg，普通小白还是别用了，很繁琐。个人觉得还是 1.0 版本的 FANseg 省心）。</li>
</ul>
<h2 id="2-0-硬件要求"><a href="#2-0-硬件要求" class="headerlink" title="2.0 硬件要求"></a>2.0 硬件要求</h2><p>DFL 2.0 只支持英伟达显卡，不支持 AMD 显卡。</p>
<p>需要显卡 CUDA Compute Capability 3.0 以上。</p>
<p>DeepFaceLab 2.0 由几个.bat 文件组成，这些文件用于执行创建 Deepfake 的各种任务 / 步骤，它们与两个子文件夹一起位于主文件夹中：</p>
<ul>
<li>_internal - 相关源代码。</li>
<li>workspace - 放置你的模型、视频、人脸数据的地方。</li>
</ul>
<p>下面是一些术语解释</p>
<p>Dataset (faceset) - 是一组从图片帧（从视频中提取）或照片已提取的图像。</p>
<p>DFL 2.0 中使用了两个数据集，分别是 <strong>data_dst</strong> 和 <strong>data_src</strong>：</p>
<ul>
<li>“<strong>data_dst</strong>“ 是一个文件夹，其中包含从 data_dst.mp4 文件提取的帧 - 这是我们要换脸的目标视频。 它还包含 2 个文件夹，这些文件夹是在从提取的帧中提取人脸后创建的：“aligned” 包含人脸图像（内嵌了人脸特征点数据，用于生成原始遮罩） “aligned_debug” 包含原始帧，这些帧画出了人脸特征点，用于标识检验人脸提取是否正确（并且不参与训练或合成过程）。清理完数据集后，可以将其删除以节省空间。</li>
<li>“<strong>data_src</strong>“ 是一个文件夹，用于保存从 data_src.mp4 文件提取的帧（可以是采访，电影，预告片等），也可以在其中放散装图片 - 就是您要在换到视频上的人。 “aligned” 包含人脸图像（内嵌了人脸特征点数据，用于生成原始遮罩） “aligned_debug” 包含原始帧，这些帧画出了人脸特征点，用于标识检验人脸提取是否正确（并且不参与训练或合成过程）。清理完数据集后，可以将其删除以节省空间。</li>
</ul>
<p>为方便国内朋友理解，我举个例子：我要做一个马保国替换叶问中甄子丹的换脸视频。 那么马保国就是 <strong>data_src</strong>，叶问就是 <strong>data_dst</strong>。</p>
<p>但是，在提取脸部之前，必须先从以下对象中提取脸部：</p>
<ul>
<li>对于 data_dst，您应该准备目标（目标）视频并将其命名为 data_dst.mp4</li>
<li>对于 data_src，您应该准备源视频（如上例所示）并将其命名为 data_src.mp4，或者准备 jpg 或 png 格式的图像。</li>
</ul>
<p>从视频中提取帧的过程也称为提取，因此在本指南的其余部分中，我将这两个过程都称为 “面部提取” 和 “帧提取”。</p>
<p>如开头所述，所有这些数据都存储在 “workspace” 文件夹中。data_dst.mp4 和 data_src.mp4 放在 workspace 文件夹目录；data_src 文件夹和 data_dst 文件夹用于放置分解视频得到的原始画面帧或散装图片。运行面部提取后，会在其中自动生成存储人脸的 “ aligned” 文件夹。</p>
<h2 id="1-工作区清理"><a href="#1-工作区清理" class="headerlink" title="1. 工作区清理"></a>1. 工作区清理</h2><p><strong>1) Clear Workspace -</strong> 删除 workspace 下所有内容。别手贱点他。</p>
<h2 id="2-从-src-视频中提取画面-data-src-mp4"><a href="#2-从-src-视频中提取画面-data-src-mp4" class="headerlink" title="2. 从 src 视频中提取画面 (data_src.mp4)"></a>2. 从 src 视频中提取画面 (data_src.mp4)</h2><p><strong>2) Extract images from video data_src -</strong> 从 data_src.mp4 视频中提取帧并将其放入自动创建的 “data_src” 文件夹中，可用选项：-FPS - 跳过视频的默认帧速率，输入其他帧速率的数值（例如，输入 5 将仅以每秒 5 帧的速度呈现视频，这意味着将提取较少的帧） -JPG / PNG - 选择提取帧的格式，jpg 较小，通常质量足够好，因此建议使用，png 较大，不能提供明显更高的质量，但是可以选择。</p>
<h2 id="3-视频切割-（可选环节）"><a href="#3-视频切割-（可选环节）" class="headerlink" title="3. 视频切割 （可选环节）"></a>3. 视频切割 （可选环节）</h2><p><strong>3) cut video (drop video on me)</strong> - 通过将视频拖放到该.bat 文件中，可以快速将视频剪切为所需的长度。 如果您没有视频编辑软件并且想快速剪切视频，则很有用，可以选择以下选项： 从时间开始 - 视频开始 结束时间 - 视频结束 音轨 - 保留默认设置 比特率 - 让我们更改视频的比特率（质量）- 最好保留默认设置</p>
<h2 id="3-从目标视频中提取画面-data-dst-mp4"><a href="#3-从目标视频中提取画面-data-dst-mp4" class="headerlink" title="3. 从目标视频中提取画面 (data_dst.mp4)"></a>3. 从目标视频中提取画面 (data_dst.mp4)</h2><p><strong>3) extract images from video data_dst FULL FPS -</strong> 从 data_dst.mp4 视频文件中提取帧并将其放入自动创建的 “data_dst” 文件夹中，可用选项： - JPG/PNG - 同 2)</p>
<h2 id="4-提取-Data-src-中的人脸"><a href="#4-提取-Data-src-中的人脸" class="headerlink" title="4. 提取 Data_src 中的人脸"></a>4. 提取 Data_src 中的人脸</h2><p>准备源数据集的第一步是对齐人脸（把人脸都摆正了），并从位于 “data_src” 文件夹中的提取帧中生成 512x512 面部图像。</p>
<p>有 2 个选项：</p>
<p><strong>4) data_src faceset extract MANUAL</strong> - 手动提取器，用法请参见 5.1。</p>
<p><strong>4) data_src faceset extract</strong> - 使用 S3FD 算法的自动提取</p>
<p>S3FD 和 MANUAL 提取器的可用选项包括： - 根据要训练的模型的面部类型选择提取的覆盖区域：</p>
<p><strong>a) full face</strong> (简称 F 脸，额头部分有些许被裁到)</p>
<p><strong>b) whole face</strong> (简称 WF 脸，范围更大，整个额头都取了，兼容 WF 和 F 脸模型)</p>
<p><strong>c) head</strong> (不常用，给高玩做 avatar 用，萌新用不到) - 选择用于面部提取 / 对齐过程的 GPU（或 cpu） - 选择是否生成 “ aligned_debug” 文件夹</p>
<h2 id="4-Data-src-整理"><a href="#4-Data-src-整理" class="headerlink" title="4. Data_src 整理"></a>4. Data_src 整理</h2><p>完成此操作后，下一步是清理错误 faceset / 数据集 / 不正确对齐的 faces，有关详细信息，请检查以下帖子：<a target="_blank" rel="noopener" href="https://mrdeepfakes.com/forums/thread-guide-celebrity-faceset-dataset-creation-how-to-create-celebrity-facesets">https://mrdeepfakes.com/forums/thread-gu…y-facesets</a></p>
<p><strong>4.1) data_src view aligned result</strong> - 不常用</p>
<p><strong>4.2) data_src sort</strong> - 给图片排序，方便你筛选错误图片</p>
<ul>
<li>blur 模糊程度</li>
<li>face yaw direction 俯仰角度</li>
<li>face pitch direction 左右角度</li>
<li>face rect size in source image 人脸在原图中的大小</li>
<li>histogram similarity 颜色直方图相似度</li>
<li>histogram dissimilarity 颜色直方图不相似度</li>
<li>brightness 亮度</li>
<li>hue 颜色色相</li>
<li>amount of black pixels 黑色像素的数量（常用于筛选异常人脸提取结果）</li>
<li>original filename 源文件名字</li>
<li>one face in image 是否是画面中唯一人脸</li>
<li>absolute pixel difference 绝对的像素差异</li>
<li>best faces 筛选最佳的人脸</li>
<li>best faces faster 更快的筛选最佳的人脸</li>
</ul>
<p><strong>4.2) data_src util add landmarks debug images</strong> - 重新生成 debug 文件夹</p>
<p><strong>4.2) data_src util faceset enhance</strong> - 用 AI 算法提升素材清晰度</p>
<p>另一个可选的提升 SRC 的素材的方法，可用 DFDNET ，谷歌 colab 连接如下： <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/rocketsvm/DFDNet/blob/master/DFDNet_DFL_Colab.ipynb#scrollTo=OZ7-E-vRSYar">https://colab.research.google.com/github…7-E-vRSYar</a></p>
<p><strong>4.2) data_src util faceset metadata restore and 4.2) data_src util faceset metadata save</strong> - 让我们从源面集 / 数据集中保存和还原嵌入的对齐数据，以便在提取某些面部图像（例如将它们锐化，编辑眼镜，皮肤瑕疵，颜色校正）后可以对其进行编辑，而不会丢失对齐数据。如果不按此步骤编辑 “已对齐” 文件夹中的任何图像，则将不会再使用对齐数据和这些图片进行训练，因此，在保持名称相同的情况下，不允许翻转 / 旋转，仅是简单的编辑，例如彩色 。</p>
<p><strong>4.2) data_src util faceset pack and 4.2) data_src util faceset unpack -</strong> 将 “aligned” 文件夹中的所有面孔打包 / 解压缩到一个文件中。 主要用于准备自定义的预训练数据集或更易于共享为一个文件。</p>
<p><strong>4.2.other) data_src util recover original filename -</strong> 将面部图像的名称恢复为原始顺序 / 文件名（排序后）。 可选，无论 SRC face 文件名如何，训练和合成都能正确运行。</p>
<h2 id="5-Data-dst-数据准备"><a href="#5-Data-dst-数据准备" class="headerlink" title="5. Data_dst 数据准备"></a>5. Data_dst 数据准备</h2><p>这里的步骤与源数据集几乎相同，除了少数例外，让我们从面部提取 / 对齐过程开始。 我们仍然有 Manual 和 S3FD 提取方法，但是还有一种结合了这两种方法和一种特殊的手动提取模式，始终会生成 “aligned_debug” 文件夹。</p>
<p><strong>5) data_dst faceset extract MANUAL RE-EXTRACT DELETED ALIGNED_DEBUG</strong> - 从 “aligned_debug” 文件夹中删除的帧进行手动重新提取。 有关更多信息，请参见 5. Data_dst 清理。 以下步骤 5.1 中的用法。</p>
<p><strong>5) data_dst faceset extract MANUAL</strong> - 纯手动模式</p>
<p><strong>5) data_dst faceset extract + manual fix</strong> - 半自动，机器识别不了的会切手动</p>
<p><strong>5) data_dst faceset extract</strong> - 纯自动提取</p>
<p>选项和 src 的一样，不重复说了</p>
<h2 id="5-1-手动人脸提取的操作说明"><a href="#5-1-手动人脸提取的操作说明" class="headerlink" title="5.1 手动人脸提取的操作说明"></a>5.1 手动人脸提取的操作说明</h2><p>启动手动提取器或重新提取器后，将打开一个窗口，您可以在其中手动找到要提取 / 重新提取的脸部：</p>
<ul>
<li>使用鼠标定位脸部</li>
<li>使用鼠标滚轮更改搜索区域的大小</li>
<li>确保所有或至少是大多数地标（在某些情况下，取决于角度，照明或当前障碍物，可能无法精确对齐所有地标，因此，请尝试找到一个最能覆盖所有可见位并且是 “t 太不对准）落在重要的部位，例如眼睛，嘴巴，鼻子，眉毛上，并正确遵循面部形状，向上箭头指示您面部的 “向上” 或 “顶部” 在哪里</li>
<li>使用键 A 更改精度模式，现在地标不会对检测到的面部 “粘” 太多，但您可能能够更正确地定位地标</li>
<li>用户 &lt;和&gt; 键（或，和。）来回移动，以确认检测到鼠标左键单击并移至下一个或按 Enter</li>
<li>鼠标右键，用于检测无法检测到的正面或非人脸（需要应用 xseg 进行正确的遮罩）</li>
<li>q 跳过剩余的面孔并退出提取器（到达最后一张面孔并确认时也会关闭）</li>
</ul>
<h2 id="5-2-Data-dst-数据整理"><a href="#5-2-Data-dst-数据整理" class="headerlink" title="5.2 Data_dst 数据整理"></a>5.2 Data_dst 数据整理</h2><p>对齐 data_dst 面后，我们必须清理它们，类似于我们使用源 faceset /dataset 进行处理时，我们将选择一些排序方法，由于它们的工作方式与 src 完全相同，因此我将不作解释。 但是清理目标数据集与源数据集有所不同，因为我们要使所有存在的帧的所有面对齐（包括可以在 XSeg 编辑器中标记的受遮挡的面），然后训练 XSeg 模型以将其遮盖 - 有效地使障碍物在学到的面孔上清晰可见，更多的是在下面的 XSeg 阶段。</p>
<p>这块做法和 data_src 类似，区别在于，最后合成时是根据 dst 中 aligned 文件数量来合成。删掉的 dst 人脸数据对应的画面就不会换脸</p>
<h2 id="5-3-XSeg-model-的训练和使用（画遮罩）"><a href="#5-3-XSeg-model-的训练和使用（画遮罩）" class="headerlink" title="5.3 XSeg model 的训练和使用（画遮罩）"></a>5.3 XSeg model 的训练和使用（画遮罩）</h2><p>这章比较复杂，晚点翻译。萌新先不要使用遮罩。不用遮罩正常也能训练</p>
<h2 id="6-训练"><a href="#6-训练" class="headerlink" title="6. 训练"></a>6. 训练</h2><p>有两种模式可以选择：</p>
<p><strong>SAEHD (6GB+)：高质量模型，至少 6GB 显存</strong></p>
<p>特点 / 设置</p>
<p>最高 640x640 分辨率，</p>
<p>可支持 half face, mid-half face, full face, whole face and head face 5 中人脸尺寸类型</p>
<p>8 种模型结构：DF, LIAE, 每种 4 个变种 - regular, -U, -D and -UD</p>
<p>可调节的批大小（batchsize）</p>
<p>可调节的模型各层维度大小</p>
<p>Auto Backup feature 自动备份</p>
<p>Preview History 预览图存档</p>
<p>Adjustable Target Iteration 目标迭代次数</p>
<p>Random Flip (yaw) 随机水平翻转</p>
<p>Uniform Yaw 按角度顺序来训练</p>
<p>Eye Priority 眼神训练优先</p>
<p>Masked Training 带遮罩训练</p>
<p>GPU Optimizer 优化器放 GPU 上</p>
<p>Learning Dropout 学习率自动下降</p>
<p>Random Warp 随机扭曲</p>
<p>GAN Training Power 使用 GAN</p>
<p>True Face Training Power 提高人脸相似度</p>
<p>Face and Background Style Power 提高颜色相似度</p>
<p>Color Transfer modes 变对素材变色</p>
<p>Gradient Clipping 梯度裁剪 - Pretrain Mode 使用预训练模式</p>
<p><strong>Quick96 (2-4GB)：低配电脑可用</strong></p>
<p>特点：</p>
<ul>
<li>96x96 分辨率</li>
<li>只支持 Full Face</li>
<li>Batch size 4</li>
<li>默认 DF-UD 结构</li>
</ul>
<p><strong>6) train SAEHD</strong></p>
<p><strong>6) train Quick96</strong></p>
<p>由于 Quick96 不可调节，因此您将看到命令窗口弹出并仅询问一个问题 - CPU 或 GPU（如果您有更多问题，它将选择其中之一或同时进行训练）。 但是，SAEHD 将为您提供更多调整选项。</p>
<p>在这两种情况下，首先都会出现一个命令行窗口，您可以在其中输入模型设置。 初次使用时，您将可以访问以下说明的所有设置，在使用已经受过训练的模型进行训练并在 “模型” 文件夹中显示该模型时，您还将收到提示，您可以在其中选择要训练的模型（ （如果您的 “模型” 文件夹中存在多个模型文件）。 您还将始终提示您选择要在其上运行培训器的 GPU 或 CPU。</p>
<p>启动后将看到的第二件事是预览窗口，如下所示：</p>
<p><a target="_blank" rel="noopener" href="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E9%A2%84%E8%A7%88%E7%AA%97%E5%8F%A3.jpg"><img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E9%A2%84%E8%A7%88%E7%AA%97%E5%8F%A3.jpg" alt="预览窗口.jpg">预览窗口.jpg</a></p>
<p>这是所有功能的更详细说明，以便在开始训练新模型时将其呈现给用户：</p>
<p>请注意，由于这些模型的工作方式，其中一些已锁定，一旦开始训练就无法更改，例如，稍后无法更改的示例如下：</p>
<ul>
<li>model resolution 模型分辨率</li>
<li>model architecture 模型结构</li>
<li>models dimensions (dims settings) 模型维度参数</li>
<li>face type 人脸类型</li>
</ul>
<p><strong>Autobackup every N hour ( 0..24 ?:help )</strong> ：自动备份频率，0 不备份</p>
<p><strong>Target iteration :</strong> 将在达到一定的迭代次数后停止训练，例如，如果要将模型训练为仅进行 100.000 次迭代，则应输入值 100000。将其保留为 0 将使其运行，直到您手动将其停止为止。 默认值为 0（禁用）。</p>
<p><strong>Flip faces randomly ( y/n ?:help ) :</strong> 基本不开。在您没有要交换到目标的人脸（源数据集）的所有必要角度的情况下的有用选项。 例如，如果您有一个目标 / 目标视频，人物直视向右，而您的源只具有直视向左的脸，则应启用此功能，但请记住，由于没有人脸对称，结果看起来可能不太像 src 以及来源面部的特征（例如美容标记，疤痕，痣等）都会被镜像。 默认值为 n（禁用）。</p>
<p><strong>Batch_size ( ?:help ) :</strong> 批处理大小设置会影响每次迭代中相互比较的面孔数。 最低值是 2，您可以提高到 GPU 允许的最大值，受 GPU 影响。 模型分辨率，尺寸越高，启用的功能越多，将需要更多的显存，因此可能需要较小的批处理大小。 建议不要使用低于 4 的值。批量越大，质量越好，但训练时间越长（迭代时间越长）。 对于初始阶段，可以将其设置为较低的值以加快初始训练的速度，然后将其升高。 最佳值为 6-12。 如何猜测要使用的批量大小？ 您可以使用试错法，也可以通过查看 DFL 2.0 电子表格来了解其他人在他们的 GPU 上可以实现什么，以帮助自己。<a target="_blank" rel="noopener" href="https://mrdeepfakes.com/forums/thread-dfl-2-0-user-model-settings-spreadsheet">https://mrdeepfakes.com/forums/thread-dfl-2-0-user-model-settings-spreadsheet</a></p>
<p><strong>Resolution ( 64-640 ?:help ) :</strong> 在这里，您可以设置模型的分辨率，请记住，在训练过程中不能更改此选项。 它会影响交换的面部的分辨率，模型的分辨率越高 - 学习的面部越详细，但训练的负担也将越来越长。 分辨率可以从 64x64 增至 640x640，其增量为： 16（对于常规和 - U 体系结构变体） 32（用于 - D 和 - UD 体系结构变体） 更高的分辨率可能需要增加模型尺寸（尺寸）。</p>
<p><strong>Face type ( h/mf/f/wf/head ?:help ) :</strong> 此选项使您可以设置要训练的脸部区域，共有 5 个选项 - 半脸，半脸，全脸，全脸和头部： a）H 半脸 - 仅从嘴巴到眉毛训练，但在某些情况下可以割破脸部的顶部或底部（眉毛，下巴，嘴巴）。 b）MF 中半脸 - 旨在解决此问题，方法是遮盖脸部比半脸大 30％，这应该可以防止大多数不希望的割伤的发生，但仍然可以发生。 c）F 全脸 - 覆盖除额头以外的大部分脸部区域，有时会割掉一点下巴，但是这种情况很少发生 - 当 SRC 和 / 或 DST 的额头覆盖头发时，最推荐使用此方法。 d）WF 整脸 - 扩大该区域以覆盖几乎整个脸部，包括额头，甚至一点点头发，但是当我们要交换整个脸部（不包括头发）时，应使用此模式。该脸部类型的另一个选项是 masked_training，它使您可以优先确定学习脸部的整个脸部的优先级，然后（禁用之后）让模型学习像额头一样的脸部其余部分。 e）头 - 用于交换整个头，不适合长发的对象，如果源面组 / 数据集来自单个源并且 SRC 和 DST 都短发或不变，则效果最好形状取决于角度。此脸型的最低建议分辨率为 224。<a target="_blank" rel="noopener" href="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E8%84%B8%E5%9E%8B.jpg"><img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E8%84%B8%E5%9E%8B.jpg" alt="脸型.jpg"></a></p>
<p><a target="_blank" rel="noopener" href="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E8%84%B8%E5%9E%8B.jpg">脸型.jpg</a></p>
<p><strong>whole face 案例</strong></p>
<p><a target="_blank" rel="noopener" href="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E6%95%B4%E8%84%B8%E6%A1%88%E4%BE%8B.png"><img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E6%95%B4%E8%84%B8%E6%A1%88%E4%BE%8B.png" alt="整脸案例.png">整脸案例.png</a></p>
<p>整脸案例.png</p>
<p><strong>AE architecture (df/liae/df-u/liae-u/df-d/liae-d/df-ud/liae-ud ?:help ) :</strong> 此选项使您可以在 2 种主要的体系结构 DF 和 LIAE 及其 - U，-D 和 - UD 变体之间进行选择。</p>
<p>DF 和 LIAE 体系结构是基础体系结构，两者均提供高质量和良好的性能。 DF-U，DF-UD，LIAE-U 和 LIAE-UD 是其他体系结构变体。</p>
<p><strong>DF:</strong> 此模型体系结构提供了更直接的人脸交换，不使人脸变形，但要求源和目标 / 目标人脸 / 头部具有相似的人脸形状。 该模型在正面拍摄时效果最好，并且要求您的源数据集具有所有所需的角度，在侧面轮廓上可能会产生较差的结果。</p>
<p><strong>LIAE:</strong> 当涉及源和目标 / 目标之间的面部 / 头部形状相似性时，此模型体系结构没有那么严格，但是该模型确实使面部变形，因此建议使实际面部特征（眼睛，鼻子，嘴巴，整体面部结构）相似 在源和目标 / 目标之间。 该模型与源头镜头的源相似性较差，但可以更好地处理侧面轮廓，并且在源源面集 / 数据集方面更宽容，通常可以产生更精致的人脸替换，并具有更好的颜色 / 照明匹配度。 <strong>-U:</strong> 此变体旨在提高训练结果面与 SRC 数据集的相似性 / 相似性。 <strong>-D:</strong> 此变体旨在提高性能，让您以两倍的分辨率训练模型，而无需额外的计算成本（VRAM 使用）和类似的性能，例如以与 128 分辨率相同的 VRAM 使用和速度（迭代时间）训练 256 分辨率模型</p>
<p><strong>-UD:</strong> 结合 U 和 D</p>
<p>接下来的 4 个选项控制模型神经网络的尺寸，这些尺寸会影响模型的学习能力，对其进行修改可能会对所学面孔的性能和质量产生重大影响，因此应将其保留为默认值。</p>
<p><strong>AutoEncoder dimensions ( 32-1024 ?:help ) :</strong> 自动编码器中间层维度大小</p>
<p><strong>Encoder dimensions ( 16-256 ?:help ) :</strong> 编码器尺寸设置会影响模型学习面孔总体结构的能力。</p>
<p><strong>Decoder dimensions ( 16-256 ?:help ) :</strong> 解码器尺寸设置会影响模型学习细节的能力。</p>
<p><strong>Decoder mask dimensions ( 16-256 ?:help ) :</strong> 遮罩解码器的尺寸设置会影响学习到的遮罩的质量。</p>
<p>更改每个设置时的性能变化可能会对性能产生不同的影响，如果没有大量的培训，就无法衡量每个参数对性能和质量的影响。 每个设置为某个默认值，该默认值应提供最佳结果，并在训练速度和质量之间取得良好的折衷。</p>
<p>同样，在更改一个参数时，也应更改其他参数，以保持它们之间的关系相似（例如，如果将 “编码器” 和 “解码器” 的尺寸从 64 降低到 48，则还可以将 “自动编码器” 的尺寸从 256 降低到 192-240）。 随意尝试各种设置。 如果要获得最佳结果，请将其保留为默认值，或者对于高分辨率型号，将其略微提高。</p>
<p><strong>Eyes priority ( y/n ?:help ) :</strong> 试图通过强制神经网络训练优先级更高的眼睛来解决眼睛训练问题。 请记住，它不能保证正确的眼睛方向，它只会影响眼睛的细节和周围区域。 示例（之前和之后）：</p>
<p><a target="_blank" rel="noopener" href="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%BC%E7%9D%9B%E4%BC%98%E5%85%88.jpg"><img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%BC%E7%9D%9B%E4%BC%98%E5%85%88.jpg" alt="眼睛优先.jpg">眼睛优先.jpg</a></p>
<p>眼睛优先.jpg</p>
<p><strong>Place models and optimizer on GPU ( y/n ?:help ) :</strong> 启用 GPU 优化器会将所有负载都放在 GPU 上，这将大大提高性能（迭代时间），但会导致更高的 VRAM 使用率，禁用此功能会将优化器的某些工作分担给 CPU，从而减少了 GPU 和 VRAM 使用率的负载，从而使您可以实现 批处理量更大或以更高的迭代时间为代价运行更苛刻的模型。 如果您收到 OOM（内存不足）错误，并且不想减小批量大小或禁用某些功能，则应禁用此功能，这样一来，一些工作将被卸载到 CPU 上，而某些数据将从 GPU VRAM 转移到系统 RAM 中 - 您将能够以较低速度为代价运行模型而不会出现 OOM 错误。 默认值为 y（启用）。</p>
<p><strong>Use learning rate dropout ( y/n/cpu ?:help ) :</strong> LRD 被用于加速面部的培训，并减少相比，如果没有启用它，使用它较少的迭代子像素抖动：</p>
<ul>
<li>在禁用 RW 之前和运行其他选项之后一次。</li>
<li>禁用 RW 之后（也是 LRD）第二次使用其他选项（均匀偏航，样式效果，真面，眼睛优先），但在启用 GAN 之后。</li>
</ul>
<p>此选项会影响 VRAM 的使用，因此，如果遇到 OOM 错误，则可以在 CPU 上运行它，但需要花费 20％的迭代时间或降低批处理大小。</p>
<p>有关 LRD 的详细说明以及在培训期间启用主要功能的顺序，请参阅 FAQ 问题 8（此主题的第 3 个帖子）：”When should I enable or disable random warp, GAN, True Face, Style Power, Color Transfer and Learning Rate Dropout?”</p>
<p><strong>Enable random warp of samples ( y/n ?:help ) :</strong> 随机扭曲用于概括模型，以便它可以正确学习所有基本形状，面部特征，面部结构，表情等，但是只要启用该模型，学习精细细节就可能会遇到麻烦 - 因为它 建议您只要您的脸部仍在改善中（通过查看减少的损耗值和预览窗口）就启用此功能，一旦对脸部进行了全面训练并想要获得更多详细信息，则应禁用它并进行数千次迭代 应该会开始看到更多详细信息，并且禁用此功能后，您将继续进行培训。 默认值为 y（启用）。 <strong>Uniform_yaw ( y/n ?:help ) :</strong> 有助于训练轮廓脸部，迫使模型根据其偏航角在所有面孔上均匀地训练，并优先考虑轮廓脸部，可能会导致正面脸部的训练速度变慢，这在预训练期间默认启用，可与随机变形类似地使用（在开始时 （训练过程）或在禁用或禁用 RW 后启用（当您对面部进行或多或少的训练，并且您希望轮廓脸部看起来更好且更少模糊时）。 当您的源数据集没有很多轮廓照片时很有用。 可以帮助降低损失值。 默认值为 n（禁用）。</p>
<p><strong>GAN power ( 0.0 .. 10.0 ?:help ) :</strong> GAN 代表 Generative Adversarial Network，在 DFL 2.0 的情况下，它是作为获得更详细 / 更清晰面孔的一种额外培训方式而实施的。 此选项的调整范围是 0.0 到 10.0，只有在模型或多或少地完成训练后（禁用样本随机扭曲并启用 LRD 之后），才应启用该选项。 建议从低值 0.1 开始，该值在大多数情况下也是建议值，一旦启用，就不应禁用它，请确保对模型进行备份，以防不满意结果。 默认值为 0.0（禁用）。</p>
<p>用 GAN 训练 0.1 的面部进行 40k 迭代之前 / 之后的示例：</p>
<p><a target="_blank" rel="noopener" href="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_GAN.png"><img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_GAN.png" alt="GAN.png">GAN.png</a></p>
<p>GAN.png</p>
<p><strong>‘True face’ power. ( 0.0000 .. 1.0 ?:help ) :</strong> 使用可变功率设置的真实面部训练，让您将模型鉴别器设置为更高或更低的值，这样做是为了使最终面孔看起来更像 src，而对于 GAN，只有在禁用了随机扭曲后，才应启用此功能 并且模型训练有素。 在启用此功能之前，请考虑进行备份。 切勿使用较高的值，典型值为 0.01，但可以使用较低的值，例如 0.001。 设置越高，结果面将越像源数据集中的面，这可能导致颜色匹配问题，并导致出现伪影，因此重要的是不要使用高值。 它对性能的影响很小，可能会导致 OOM 错误发生。 默认值为 0.0（禁用）。</p>
<p><a target="_blank" rel="noopener" href="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%9F%E5%AE%9E%E9%9D%A2%E9%83%A8%E6%9D%83%E9%87%8D.png"><img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E7%9C%9F%E5%AE%9E%E9%9D%A2%E9%83%A8%E6%9D%83%E9%87%8D.png" alt="真实面部权重.png">真实面部权重.png</a></p>
<p>真实面部权重.png</p>
<p><strong>Face style power ( 0.0..100.0 ?:help ) and Background style power ( 0.0..100.0 ?:help ) :</strong> 此设置控制图像的面部或背景部分的样式转移，用于将目标 / 目标面孔（data_dst）的样式转移到最终学习的面孔，这样可以提高质量和合并后最终结果的外观，但是 高值可能导致学习的人脸看起来更像 data_dst，而不是 data_src。 它将从 DST 传输一些颜色 / 照明信息到结果脸部。 建议不要使用大于 10 的值。从 0.001-0.01 之类的小值开始。 此功能对性能有很大影响，使用它会增加迭代时间，并且可能需要您减小批处理大小，禁用 gpu 优化器或在 CPU 上运行 LRD。 在启用此功能之前，请考虑进行备份。 默认值为 0.0（禁用）。</p>
<p><strong>Color transfer for src faceset ( none/rct/lct/mkl/idt/sot ?:help ) :</strong> 此功能用于将 data_src 的颜色与 data_dst 进行匹配，以使最终结果具有与 data_dst 相似的肤色 / 色调，并且训练后的最终结果不会在人脸移动时改变颜色（如果脸部不同，可能会发生这种情况 角度是从包含不同光照条件或颜色分级不同的各种光源获取的。 有以下几种选择：</p>
<p><strong>rct</strong> (reinhard color transfer)（我常用，滚石注）：基于 <a target="_blank" rel="noopener" href="https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf">https://www.cs.tau.ac.il/~turkel/imagepapers/ColorTransfer.pdf</a></p>
<p><strong>lct</strong> (linear color transfer)：使用线性变换将目标图像的颜色分布与源图像的颜色分布匹配。</p>
<p><strong>mkl</strong> (Monge-Kantorovitch linear)：基于 <a target="_blank" rel="noopener" href="http://www.mee.tcd.ie/~sigmedia/pmwiki/uploads/Main.Publications/fpitie07b.pdf">http://www.mee.tcd.ie/~sigmedia/pmwiki/uploads/Main.Publications/fpitie07b.pdf</a></p>
<p><strong>idt</strong> (Iterative Distribution Transfer)：基于 <a target="_blank" rel="noopener" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.1052&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.1052&amp;rep=rep1&amp;type=pdf</a></p>
<p><strong>sot</strong> (sliced optimal transfer)：基于 <a target="_blank" rel="noopener" href="https://dcoeurjo.github.io/OTColorTransfer/">https://dcoeurjo.github.io/OTColorTransfer/</a></p>
<p><strong>Enable gradient clipping ( y/n ?:help ) :</strong> 梯度裁剪。实现此功能是为了防止在使用 DFL 2.0 的各种功能时可能发生的所谓的模型崩溃 / 损坏。 它对性能的影响很小，因此，如果您真的不想使用它，则必须启用自动备份，因为崩溃后的模型无法恢复，必须将其废弃，并且必须从头开始进行培训。 默认值为 n（禁用），但是由于对性能的影响非常低，并且如果保持启用状态，可以防止模型崩溃而节省大量时间。 使用 Style Powers 时最容易发生模型崩溃，因此强烈建议您启用渐变裁剪或备份（也可以手动进行）。</p>
<p><strong>Enable pretraining mode ( y/n ?:help ) :</strong> 启用预训练过程，该过程使用随机人脸数据集对模型进行初始训练，将其训练约 200k-400k 次迭代后，可以在开始使用要训练的实际 data_src 和 data_dst 进行训练时使用此类模型，因为您可以节省时间不必每次都从 0 开始全面训练（模型将 “知道” 面孔的外观，从而加快初始训练阶段）。可以随时启用 pretrain 选项，但建议在开始时仅对模型进行一次预训练。您还可以使用自己的自定义面集进行预训练，您要做的就是创建一个（可以是 data_src 或 data_dst），然后使用 4.2）data_src（或 dst）util faceset pack .bat 文件打包成一个文件，然后将其重命名为 faceset.pak 并替换（备份旧的）“ …  _ internal  pretrain_CelebA” 文件夹中的文件。默认值为 n（禁用）。但是，如果要节省一些时间，可以去论坛找别人训练好的模型。</p>
<p>共享模型：<a target="_blank" rel="noopener" href="https://mrdeepfakes.com/forums/thread-dfl-2-0-pretrained-models-general-thread-for-user-made-models-and-requests">https://mrdeepfakes.com/forums/thread-dfl-2-0-pretrained-models-general-thread-for-user-made-models-and-requests</a></p>
<p>要使用共享的预训练模型，只需下载它，将所有文件直接放入模型文件夹中，开始训练，在选择要训练的模型（如果在模型文件夹中有更多内容）和用于训练的设备后 2 秒钟内按任意键 （GPU / CPU）来覆盖模型设置，并确保禁用预训练选项，以便您开始正确的训练；如果您启用了预训练选项，则模型将继续进行预训练。 请注意，该模型会将迭代计数恢复为 0，这是预训练模型的正常行为。</p>
<h2 id="7-Merging-合成"><a href="#7-Merging-合成" class="headerlink" title="7. Merging 合成:"></a>7. Merging 合成:</h2><p>训练完模型后，该将学习的人脸合并到原始帧上以形成最终视频了（转换）。</p>
<p>为此，我们有 2 个对应于 2 种可用型号的转换脚本：</p>
<p><strong>7) merge SAEHD</strong></p>
<p><strong>7) merge Quick96</strong></p>
<p>选择其中任何一个后，命令行窗口将出现，并带有多个提示。 第一个将询问您是否要使用带交互界面的转化器，默认值为 y（启用），除非你受虐狂，不然就好好开着吧，边调参数边预览</p>
<p><strong>Use interactive merger? ( y/n ) :</strong></p>
<p>第二个将询问您要使用哪种模型：</p>
<p><strong>Choose one of saved models, or enter a name to create a new model.</strong></p>
<p>[r] ：rename</p>
<p>[d] ：delete</p>
<p>[0] ：df192 - latest</p>
<p>:</p>
<p>第 3 个会问您要在合并（转换）过程中使用哪个 GPU / GPU：</p>
<p><strong>Choose one or several GPU idxs (separated by comma).</strong></p>
<p>[CPU] ：CPU</p>
<p>[0] ：GeForce GTX 1070 8GB</p>
<p>[0] Which GPU indexes to choose?</p>
<p>:</p>
<p>按 Enter 将使用默认值 [0]。</p>
<p>完成之后，您将看到一个带有当前设置的命令行窗口以及一个预览窗口，其中显示了操作交互式转换器 / 合并程序所需的所有控件。</p>
<p>这是命令行窗口和转换器预览窗口的快速浏览：</p>
<p><a target="_blank" rel="noopener" href="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E5%BF%AB%E9%80%9F%E6%B5%8F%E8%A7%88.png"><img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_11_21_%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E_%E5%BF%AB%E9%80%9F%E6%B5%8F%E8%A7%88.png" alt="快速浏览.png">快速浏览.png</a></p>
<p>快速浏览.png</p>
<p>Converter 具有许多选项，可用于更改遮罩类型，大小，羽化 / 模糊，还可以添加其他颜色转移并进一步锐化 / 增强最终训练的脸部。</p>
<p>这是解释的所有合并 / 转换器功能的列表：</p>
<p><strong>1. Main overlay modes:</strong></p>
<ul>
<li><strong>original:</strong> 显示原始画面而没有换脸</li>
<li><strong>overlay:</strong> 简单地将学习到的脸覆盖在框架上 （推荐用这个，滚石注）</li>
<li><strong>hist-match:</strong> 根据直方图叠加学习的面部和试图以使其匹配（具有 2 种模式：正常模式和可通过 Z 切换的蒙版）</li>
<li><strong>seamless:</strong> 使用 opencv 泊松无缝克隆功能在原始帧的头部上方融合新学习的面部</li>
<li><strong>seamless hist match:</strong> 结合了直方图匹配和无缝匹配。</li>
<li><strong>raw-rgb:</strong> 覆盖原始学习过的脸部而没有任何遮罩</li>
</ul>
<p>注意：无缝模式可能导致闪烁。</p>
<p><strong>2. Hist match threshold:</strong> 在直方图匹配和无缝直方图覆盖模式下控制直方图匹配的强度。</p>
<p>Q - 增加值</p>
<p>A - 减小值</p>
<p><strong>3. Erode mask:</strong> 控制遮罩的大小。</p>
<p>W - 增加遮罩腐蚀（较小的遮罩）</p>
<p>S - 减少遮罩腐蚀 (较大的遮罩)</p>
<p><strong>4. Blur mask</strong>：使遮罩边缘模糊 / 羽化，以实现更平滑的过渡</p>
<p>E - 增加值</p>
<p>D - 减小值</p>
<p><strong>5. Motion blur:</strong> 动态模糊。输入初始参数（转换器模式，模型，GPU / CPU）后，合并将加载所有帧和 data_dst 对齐的数据，同时，它会计算用于创建此设置控制的运动模糊效果的运动矢量，让您 将其添加到人脸移动的地方，但是即使移动很小，高值也可能使人脸模糊。 该选项仅在 “data_dst /aligned” 文件夹中存在一组面孔时才有效 - 如果在清理过程中某些面孔带有_1 前缀（即使只有一个人的面孔），效果将不起作用，同样 如果有一个可以反射目标人员面部的镜子，在这种情况下，您将无法使用运动模糊，并且添加该模糊的唯一方法是分别训练每组面部。</p>
<p>R - 增加 motion blur</p>
<p>F - 减少 motion blur</p>
<p><strong>6. Super resolution:</strong> 超分辨率使用与 data_src 数据集 / 面部设置增强器类似的算法，它可以为牙齿，眼睛等区域添加更多定义，并增强所学面部的细节 / 纹理。</p>
<p>T - 增加细节 the enhancement effect</p>
<p>G - 减少细节</p>
<p><strong>7. Blur/sharpen:</strong> 使用方块或高斯方法模糊或锐化所学的面部。</p>
<p>Y - sharpens the face</p>
<p>H - blurs the face</p>
<p>N - box/gaussian mode switch</p>
<p><strong>8. Face scale:</strong> 缩放人脸</p>
<p>U - scales learned face down</p>
<p>J - scales learned face up</p>
<p><strong>9. Mask modes:</strong> 6 种遮罩计算方式，效果自己试一遍就知道了</p>
<p>dst: uses masks derived from the shape of the landmarks generated during data_dst faceset/dataset extraction.</p>
<p>learned-prd: uses masks learned during training. Keep shape of SRC faces.</p>
<p>learned-dst: uses masks learned during training. Keep shape of DST faces.</p>
<p>learned-prd*dst: combines both masks, smaller size of both.</p>
<p>learned-prd+dst: combines both masks, bigger size of both.</p>
<p>XSeg-prd: uses XSeg model to mask using data from source faces.</p>
<p>XSeg-dst: uses XSeg model to mask using data from destination faces.</p>
<p>XSeg-prd*dst: combines both masks, smaller size of both.</p>
<p>learned-prd<em>dst</em>XSeg-dst*prd: combines all 4 mask modes, smaller size of all.</p>
<p><strong>10. Color transfer modes:</strong> 与训练过程中的颜色转移类似，您可以使用此功能将学习到的脸部的肤色与原始帧更好地匹配，以实现更加无缝和逼真的脸部交换。 有 8 种不同的模式：</p>
<p>RCT</p>
<p>LCT</p>
<p>MKL</p>
<p>MKL-M</p>
<p>IDT</p>
<p>IDT-M</p>
<p>SOT - M</p>
<p>MIX-M</p>
<p><strong>11. Image degrade modes:</strong> 您可以使用 3 种设置来影响原始帧的外观（不影响换面）：</p>
<p>Denoise - denoises image making it slightly blurry (I - increases effect, K - decrease effect)</p>
<p>Bicubic - blurs the image using bicubic method (O - increases effect, L - decrease effect)</p>
<p>Color - decreases color bit depth (P - increases effect, ; - decrease effect)</p>
<p>附加控件：</p>
<p><strong>TAB button</strong> - 在主预览窗口和帮助屏幕之间切换。</p>
<p>请记住，您只能在主预览窗口中更改参数，按帮助屏幕上的任何其他按钮都不会更改它们。</p>
<p><strong>-/_ and =/+</strong> buttons are used to scale the preview window.</p>
<p>Use caps lock to change the increment from 1 to 10 (affects all numerical values).</p>
<p>要保存 / 覆盖当前一帧中所有下一帧的设置 <strong>shift + /</strong></p>
<p>要保存 / 覆盖当前一帧中所有先前帧的设置 <strong>shift + M</strong></p>
<p>要开始合并所有帧，请按 <strong>shift + &gt;</strong></p>
<p>要返回第一帧，请按 <strong>shift + &lt;</strong></p>
<p>要仅转换下一帧，请按 <strong>&gt;</strong></p>
<p>要返回上一帧，请按 <strong>&lt;</strong></p>
<h2 id="8-把转化好的帧合成为视频"><a href="#8-把转化好的帧合成为视频" class="headerlink" title="8. 把转化好的帧合成为视频"></a>8. 把转化好的帧合成为视频</h2><p>合并 / 转换所有面部之后，“data_dst” 文件夹中将有一个名为 “ merged” 的文件夹，其中包含构成视频的所有帧。 最后一步是将它们转换回视频，并与 data_dst.mp4 文件中的原始音轨合并。</p>
<p>为此，您将使用提供的 4 个.bat 文件之一，这些文件将使用 FFMPEG 将所有帧组合成以下格式之一的视频 - avi，mp4，lessless mp4 或 lossless mov：</p>
<p><strong>8) merged to avi</strong></p>
<p><strong>8) merged to mov lossless 无损 mov</strong></p>
<p><strong>8) merged to mp4 lossless 无损 MP4</strong></p>
<p><strong>8) merged to mp4</strong></p>
<h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><p><strong>1. Q：1.0 和 2.0 有什么区别？</strong></p>
<p>答：2.0 是经过改进和优化的版本，由于进行了优化，它提供了更好的性能，这意味着您可以训练更高分辨率的模型或更快地训练现有模型。 合并和提取也明显更快。 问题在于 DFL 2.0 不再支持 AMD GPU / OpenCL，唯一的使用方法是与 Nvidia GPU（需要最少 3.0 CUDA 计算级别的 GPU 支持）或 CPU 一起使用。 请记住，在 CPU 上进行训练的速度要慢得多，其他所有步骤（例如提取和合并（以前称为转换））也要慢得多。 此外，新版本仅提供 2 种型号 - SAEHD 和 Quick 96，没有 H128 / H64 / DF / LIAEF / SAE 型号。 同样，从 1.0 开始的所有训练 / 预训练模型（SAE / SAEHD）与 2.0 不兼容，因此您需要训练新模型。 在上面的主要指南中，您还可以了解其他一些更改。</p>
<p><strong>2. Q：做一次换脸需要多长时间？</strong></p>
<p>A：根据目标（DST）视频的时长，SRC 数据集 / 面集的大小以及用于训练的硬件（GPU）的模型的类型。 如果您是从头开始训练模型或制作更长的 deepfake，则在经过预训练的模型上进行简单，短暂的可能需要半天到 5 到 7 天的时间，特别是如果高分辨率全脸型需要额外的时间 培训 XSeg 模型，甚至在合并视频编辑软件后进行一些工作。 这也取决于您拥有的硬件，如果您的 GPU 的 VRAM 较少（4-6 GB），则需要花费更长的时间来训练具有更强大 GPU（8-24GB）的模型。 它还取决于您的技能，查找 SRC 数据集 / 面容的原始资料的速度，可以找到合适的目标（DST）视频的速度以及为训练准备两个数据集的速度有多快。</p>
<p><strong>3. Q：你能用几张照片制作一个 Deepfake 视频吗？</strong></p>
<p>A：通常，答案是 “否”。 推荐使用面部表情制作像样的 Deepfake 的方法是使用视频。角度和面部表情越多越好。 当然，您可以尝试只用几百张照片制作一个 Deepfake 视频，它可以正常工作，但结果却不那么令人信服。</p>
<p><strong>4. Q：理想的人脸数据数量是多少？</strong></p>
<p>A：对于 data_src（名人）脸部，建议至少拥有 4000-6000 张不同的图像。当然，您可以拥有更多但通常 10.000-15.000 张图像就足够了，只要数据集中有多种（不同的面部角度和表情）。 最好将它们用于尽可能少的源，使用的源越多（尤其是当不同源之间相似的表达式 / 角度 “重叠” 时），将模型变形为 DST 的可能性越高，看起来更像是 SRC，可能需要运行 TF 或保持启用 RW 的模型训练时间更长。 有关制作源面集 / 数据集的更多详细指南，请查看以下指南： <a target="_blank" rel="noopener" href="https://mrdeepfakes.com/forums/thread-gu...y-facesets">https://mrdeepfakes.com/forums/thread-gu...y-facesets</a></p>
<p><strong>5. Q：为什么我的 Deepfake 变得模糊？</strong></p>
<p>A：面孔模糊的原因很多。最常见的原因包括 - 训练时间不够长，源数据集中缺少必要的角度，提取的源或目标面集 / 数据集的对齐方式不正确，训练或合并过程中的设置不正确，源面或目标面集 / 数据集中的人脸模糊。 如果您想知道做些什么以及在做一次深度仿真时应避免什么，请阅读本主题（指南部分）的第一篇文章。</p>
<p><strong>6. Q：为什么我的结果脸没有眨眼 / 眼睛看起来不对 / 眼睛交叉？?</strong></p>
<p>A：这很可能是由于 data_src 中缺少图像而导致的，这些图像包含闭着眼睛或以特定角度或某些角度朝特定方向注视的面孔。确保在所有可能的角度上都有相当数量的不同面部表情，以匹配目标 / 目标视频中的面部表情和角度 - 其中包括睁着眼睛并朝不同方向看的面部，而模型不知道这些 脸部的眼睛看起来应该怎样，导致眼睛无法睁开或看上去全都错了。 造成此问题的另一个原因可能是使用错误的设置或调暗设置减少了跑步训练。</p>
<p><strong>7. Q：我什么时候应该停止训练？</strong></p>
<p>A：没有正确的答案，但是普遍的共识是使用预览窗口来判断何时停止训练和转换。没有确切的迭代次数或损失值，您应在此停止训练。 如果您正在运行预训练的模型，则建议至少进行 100.000 次迭代；如果从 0 开始运行新模型，则建议至少进行 200.000 次迭代，但是该数目甚至可能高达 300.000 次迭代（取决于模型必须学习的面孔的数量和数量） ）。</p>
<p><strong>8. Q：我什么时候应该启用或禁用随机扭曲，GAN，真面，样式效果，色彩转移和学习率下降？When should I enable or disable random warp, GAN, True Face, Style Power, Color Transfer and Learning Rate Dropout?</strong></p>
<p>A： 1. 从 “随机扭曲” 开始，只要使模型能够泛化并达到 0.4-0.6 左右的损耗值，就应启用它 - 预测面（预览的第二和第四列）和最终结果面（第五列）应看起来正确，但可能仍然模糊。</p>
<p>\2. 接下来，您可以启用诸如 Uniform Yaw 之类的选项，以帮助概括轮廓面或帮助培训它们，以防您在源数据集中没有很多东西的情况下使用，但是不必总是使用 RW 来运行此选项，以后可以在启用该选项时启用禁用 RW。</p>
<p>\3. 在禁用 RW 之前，您可以启用 LRD 并对其进行更多地训练，以使其在此阶段中学习更多，接下来，您将禁用 RW 和 LRD，以便模型可以在下一阶段中正确进行训练。稍后，在禁用 RW 的情况下进行训练并有选择地为 4-9 运行一些其他选项之后，您可以第二次启用 LRD，在 LRD 之后，您不应启用 GAN 以外的任何其他选项。</p>
<p>\4. 可以在禁用 “随机扭曲” 之前或之后使用 “ True Face and Style Power”，但是建议在禁用 RW 之后以及启用 LRD 之前使用它们。</p>
<p>5.GAN 应该最后启用，并且在同时启用 LRD 时也必须启用。</p>
<p>\6. 可以从培训开始或结束时就启用颜色转移，这完全取决于您的 SRC 面部表情 / 数据集在色彩方面与 DST / 目标视频的匹配程度。</p>
<p>\7. 仅当源面集 / 数据集缺少某些角度时才应启用随机翻转 - 注意使用随机翻转可能会导致某些问题，并且如果面部具有一些不对称的特征 - 它们将被镜像，建议您在启动随机翻转时 RW 仍处于启用状态。</p>
<p>\8. 对于大多数训练，应该启用蒙版训练（仅全脸），因为它有助于训练模型需要学习的内容，即所应用的 XSeg 遮罩（如果 XSeg 遮罩为未应用），但是如果您打算在合并过程中腐蚀蒙版（如果源面比 DST 大 / 宽，并且您不希望它的形状被蒙版剪切），则可以禁用它（如果这样做）确保在启用 GAN 之前先执行此操作。</p>
<p>\9. 禁用 RW 之后但在 LRD 和 GAN 之前，应启用眼睛优先级，因为其他选项可能会引起问题（真面目，样式效果）。在某些情况下，当仍在使用 RW 时，用户在运行它时报告了良好的效果，但请记住，启用它会优先考虑眼睛区域和面部其余部分，其学习速度不会像平常那样快。</p>
<p><strong>9. Q：DFL 根本不起作用（提取，培训，合并）和 / 或我收到错误消息。</strong></p>
<p>您的 GPU 可能不受支持，您正在尝试在较新版本的 DFL 上运行旧模型（或以其他方式），或者软件或 PC 出现问题。</p>
<p>首先检查您的 GPU 是否受支持，DFL 需要 CUDA 计算能力为 3.0： <a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/CUDA#GPUs_supported">https://zh.wikipedia.org/wiki/CUDA#GPUs_supported</a> 然后查看是否拥有 DFL 的最新版本：GOOGLE DRIVE TORRENT 然后检查您要运行的模型是否仍然兼容，最简单的方法是尝试运行具有相同参数的新模型（将批次大小调整为较低的值，例如 2-4，以进行测试） 如果仍然有问题，请检查您的 PC，GPU 驱动程序过时和 Windows 更新等待中之类的事情可能会引起一些问题。 如果仍然无法运行，则可以在问题部分中创建一个新线程，但是在执行此操作之前，请检查 github 上的问题选项卡，以查看其他用户是否没有相同的问题 / 错误：DFL 2.0 GITHUB 如果找不到任何东西，并且您已在论坛上搜索类似的问题，请在此处创建新主题：<a target="_blank" rel="noopener" href="https://mrdeepfakes.com/forums/forum-questions">https://mrdeepfakes.com/forums/forum-questions</a> 或在此处发布有关信息。</p>
<p><strong>10. 训练 SAEHD 时出现 OOM / 内存不足错误。</strong> 如果遇到 OOM 错误，则意味着您的 VRAM 即将用尽，可以更改各种设置来解决此问题：</p>
<p>a）减少批次大小 - 较低的批次大小意味着模型在较少的图像上进行训练，因此使用较少的 VRAM，但这意味着与达到较高的批次大小相比，您将需要更长的训练时间才能达到相同的结果，例如 2-4 的极低批次大小也会导致结果准确性降低。</p>
<p>b）更改优化器设置（models_opt_on_gpu）- 设置为 True 时，优化器和模型均由 GPU 处理，这意味着更快的迭代时间 / 性能 / 更快的训练，但 VRAM 使用率更高；如果设置为 False，则将处理运行网络优化器的职责通过 CPU 进行操作，这意味着较少的 VRAM 使用量，可能没有 OOM，甚至可能的批处理量更大，但是由于迭代时间较长，因此训练速度会变慢。</p>
<p>c）关闭其他功能，例如面部和 bg 样式转换，TrueFace 训练，GAN 训练或性能沉重的 CT 方法（如 SOT-M）- 使它们增加迭代 / 训练时间并使用更多的 VRAM。</p>
<p><strong>11. Q：I’ve turned off all additional features and the training is still giving me OOM errors even at low batch size.</strong></p>
<p>A：在这种情况下，您甚至可以更改更多设置，但由于您只能设置一次，因此将需要您开始新的模型训练：</p>
<p>a）以降低的分辨率运行模型 - 即使可以进行所有优化并禁用各种功能，仍然可能无法运行所需的分辨率，只需降低分辨率直到可以运行（分辨率为 16 倍）</p>
<p>b）减小自动编码器尺寸，编码器尺寸，解码器尺寸，解码器蒙版尺寸。 这些设置控制着模型的尺寸，因此更改它们可以对模型学习面部和表情特征的能力产生巨大影响。 将该值设置为低值可能意味着该模型不会闭上眼睛或无法学习某些面部特征。 如果您无能为力，请仅更改它们。 有关他们做什么的更多信息，请查看指南。</p>
<p>c）购买具有更多 VRAM 的 GPU。</p>
<p><strong>12. Q：我的源数据集中有太多相似的面孔，有没有可以用来删除它们的工具？</strong></p>
<p>A：是的，您可以使用内置的 DFL 排序方法，也可以使用 VisiPics 之类的应用来检测源数据集中的相似外观并将其删除。</p>
<p><strong>13. Q：我正在训练已经进行了数千次迭代的模型，但是预览窗口中的面孔突然变成黑 / 白 / 看起来很怪异，我的损失值上升 / 为零。</strong></p>
<p>A：您的模型已经崩溃，这意味着您不能再使用它，必须重新开始，如果有备份，请使用它们。为防止模型崩溃，请使用渐变裁剪或仅启用备份，通常除非使用样式功能（否则强烈建议启用渐变裁剪），否则模型不会崩溃（但强烈建议您启用渐变裁剪），即使您担心即使没有启用任何其他功能也要崩溃 您可以始终在不影响性能的情况下始终启用它（可能很难注意到，根据我的测试，最多最多可以增加 50-100 毫秒）。</p>
<p><strong>14. Q：如果我训练 Celeb A 的模型（data_src）并使用 Celeb B（data_dst）作为目的地，是否可以使用相同的 Celeb A 模型替换新的 Celeb C？ 我可以重用模型吗？</strong></p>
<p>A：是的，实际上，如果您打算对同一来源进行更多的伪造，甚至在使用同一数据集时，也建议重用您的模型。 当使用完全不同的源和目标 / 目标时，您也可以重新使用模型。</p>
<p><strong>15. Q：我应该预训练我的模型吗？</strong></p>
<p>A：与重用一样，是的，您应该进行预训练。使用 DFL 内置的预训练功能，您可以在启动模型时选择该功能。 这是预训练模型，在 200k 至 400k 迭代中的任何位置运行此功能并在要完成预训练后将其关闭的正确方法。</p>
<p><strong>16. Q：我遇到错误：不是在 DeepFaceLab 中训练所需的 dfl 图像文件 error: is not a dfl image file required for training in DeepFaceLab</strong></p>
<p>A：这意味着 data_src /aligned 和 / 或 data_dst 内部的图片对于 DFL 训练无效。 这可能是由以下几方面引起的：</p>
<p>\1. 您正在使用名人的共享数据集之一，尽管它们看起来像对齐的面孔（256x256 图像）可能只是从中提取的图片，但它们是由不同于 DFL 的软件或较旧版本制作的。以不同方式存储界标 / 路线数据的不同应用。要修复它们，您只需要对它们运行对齐过程，只需将它们放入 “data_src” 文件夹（而不是其中的 “ aligned” 文件夹），然后使用 4 重新对齐它们即可。data_src 提取面 S3FD</p>
<p>\2. 对齐后，您在 gimp /photoshop 中的 data_src 或 data_dst 的对齐文件夹中编辑了脸部 / 图像。 编辑这些图像时，将覆盖存储在其中的地标 / 路线数据。 如果要编辑这些图像，请首先运行 4.2）data_src util faceset 元数据保存以将对齐信息保存在单独的文件中，然后编辑图像并运行 4.2）data_src util faceset 元数据恢复以还原该数据。 仅允许编辑 AI 放大 / 增强功能（您现在也可以使用 4.2 进行操作）data_src 实用面部设置增强功能，而不是使用外部应用程序（例如 Gigapixel），颜色校正或对不改变其形状的面部进行编辑（例如删除或添加内容），则不允许翻转 / 镜像或旋转。</p>
<p>\3. 您的 “data_src /dst” 或 “ aligned” 文件夹中有未提取的未对齐的常规图像。</p>
<p>\4. 您的 “data_src /aligned” 文件夹中有_debug 面。删除它们。</p>
<p><strong>17. Q：转换时出现错误：找不到 XYZ.jpg/png 的人脸，没有人脸地复制。I’m getting errors during conversion: no faces found for XYZ.jpg/png, copying without faces.</strong></p>
<p>A：这意味着对于 “data_dst” 文件夹中的 XYZ 帧，没有脸部被提取到 “ aligned” 文件夹中。 这可能是因为在该帧中实际上没有可见的脸部（正常），或者它们是可见的，但是由于它们所处的角度或障碍物而无法检测到。 要解决此问题，您需要手动提取这些面孔。 查看主要指南，尤其是有关清理 data_dst 数据集的部分。</p>
<p>总体而言，在开始训练之前，应确保已对齐并正确提取了多少张面孔。 并记住，在训练之前应该清理两个数据集，以了解更多信息，请查阅第一篇文章（指南），并阅读以下有关准备源数据集以供训练使用并在我们的论坛上共享的主题</p>
<p><strong>18. Q：转换期间 警告：检测到多张脸。 强烈建议分开对待它们，并警告：检测到多张面孔。 将不使用定向模糊。 I’m getting errors: Warning: several faces detected. Highly recommended to treat them separately and Warning: several faces detected. Directional Blur will not be used. during conversion</strong></p>
<p>A：这是由 data_dst /aligned 文件夹中的多个面孔引起的。 提取过程试图不惜一切代价检测每帧中的面部。 如果它确实检测到多个面部或一个真实的面部并错误地将其他事物检测为面部，则会为每个帧创建多个文件，如下所示：0001_0.jpg 0001_1.jpg 0001_2.jpg（如果检测到 3 张面部）。</p>
<p><strong>19. Q：合并后，我在某些或所有合并帧中看到原始 / DST 面。</strong></p>
<p>A：确保将转换器模式设置为叠加模式或 “原始” 模式以外的任何其他模式，并确保已对齐 data_dst.mp4 文件所有帧中的面。 如果您仅在某些帧上看到原始人脸，那是因为未从相应的帧中检测到它们 / 将它们对齐，则可能由于各种原因而发生这种情况：难以看到人脸的极端角度，模糊 / 运动模糊，障碍物等。 总体而言，您希望始终将 data_dst.mp4 中的所有面孔对齐。</p>
<p><strong>20. Q：训练时那些 0.2513 0.5612 代表什么意思？</strong></p>
<p>A：这些是损失值。 它们表明模型的训练程度。但是，除非它们已经稳定在某个值附近（假设您没有更改任何模型参数），否则您不应该关注它们，除非看到它们的值突然上升（上下波动），而应关注预览窗口并查找诸如 牙齿分离，美容痕迹，鼻子和眼睛，如果它们锋利且看起来不错，那么您就不必担心任何事情。 如果您发现损耗值由于某些原因尽管没有改变但仍因某些原因而上升，请考虑停止训练并通过梯度裁剪恢复它，或者禁用一些其他选项，这些选项可能是您在错误的设置下启用的，这现在会引起问题。</p>
<p><strong>21. Q：理想损耗值是多少，低 / 高损耗值应为多少？</strong></p>
<p>A：这完全取决于设置，数据集和各种不同的因素。通常，您希望在禁用所有功能的情况下开始训练，但样本的随机扭曲（以及可选的梯度裁剪，以防止模型崩溃和万一您的源数据集缺少某些面部 / 头部角度的情况下进行随机翻转）损失到 0.4-0.5 以下（取决于 模型架构，以及是启用了屏蔽训练的全脸模型还是启用了屏蔽训练的全脸 / 头部模型，以及模型分辨率或模型尺寸。 禁用随机扭曲模型后，应该能够达到 0.15 到 0.25 之间的损失值。 在某些情况下，您的模型可能会陷入某些损失值，或者永远无法达到较低的损失值。</p>
<p><strong>22. Q：我的模型崩溃了，我能以某种方式恢复它吗？</strong></p>
<p>A：不，您需要重新开始，或者如果使用了备份，请使用备份。</p>
<p><strong>23. Q：如果您使用名人面部表情训练并且想要向其中添加更多面部 / 图像 / 框架怎么办？ 如何为现有 src /source/celebrity 数据集增加更多种类？</strong></p>
<p>A：最安全的方法是将整个 “data_src” 文件夹的名称更改为其他名称，或将其临时移动到其他位置，然后从新的 data_src.mp4 文件中提取帧，或者如果您已经提取了帧并准备好一些图片，则创建一个 新文件夹 “ data_src”，将其复制到其中并运行 data_src 提取 / 对齐过程，然后将对齐的图像从旧的 data_src /aligned 文件夹复制到新文件夹中，并在 Windows 要求替换或跳过时，选择重命名选项 文件，因此您可以保留所有文件，而不会最终用新文件替换旧文件。</p>
<p><strong>24. Q：dst faceset /data_dst.mp4 是否也需要清晰且高质量？ dst faceset /dataset/data_dst 中的某些面孔会有点模糊 / 有阴影等吗？ 我的 data_dst /aligned 文件夹中的面孔模糊怎么办</strong></p>
<p>A：您希望您的 data_dst 尽可能清晰，并且没有任何运动模糊。 data_dst 中的面孔模糊会导致几个问题： - 首先是某些框架中的某些面孔将不会被检测到 - 转换 / 合并时，原始面孔将显示在这些框架上，因为在提取过程中无法正确对齐它们，因此您必须手动提取。 - 其次是其他人可能未正确对齐 - 这将导致该帧上的最终脸部旋转 / 模糊，并且看起来很不正确，并且与其他模糊脸部相似，必须手动对齐才能用于训练和转换。 - 第三 - 即使在某些情况下使用手动对齐，也可能无法正确检测 / 对齐面部，这又将导致原始面部在相应的帧上可见。 - 包含运动模糊或正确对齐的模糊（不清晰）的面部可能仍会产生不良结果，因为用于训练的模型无法理解运动模糊，模糊时面部的某些部分（如嘴巴）可能显得更大 / 更宽或只是不同而已，模型会将其解释为该部分的形状 / 外观发生了变化，因此，预测的假面和最终的假面都将看起来不自然。 您应该从训练数据集（data_dst /aligned 文件夹）中删除那些模糊的面部，然后将它们放在其他地方，然后再将它们复制回 data_dst /aligned 文件夹，然后再进行转换，这样我们可以将交换的面部显示在与那些模糊的面部相对应的帧上。 要消除运动中的怪异表情，您可以在合并中使用运动模糊（但不适用于 “data_dst /aligned” 文件夹中只有一组面孔且所有文件都以_0 前缀结尾的情况）。</p>
<p>您希望 SRC 数据集和 DST 数据集都尽可能清晰和高质量。 某些帧上的少量模糊不应该引起很多问题。至于阴影，这取决于我们在谈论多少阴影，可能看不见小的浅色阴影，您可以在脸上带有阴影的情况下获得良好的效果，但在很大程度上也会看起来很糟糕，您希望将脸部照亮均匀分布，并尽可能减少刺眼 / 尖锐和深色阴影。</p>
<p><strong>25. Q：当我尝试将 Deepfake 转换回 8）转换为 mp4 的 mp4 时，找不到错误 reference_file。</strong></p>
<p>A：您在 “工作区” 文件夹中缺少 data_dst.mp4 文件，请检查是否未删除该文件： 之所以需要它，是因为即使使用 3）从视频 data_dst FULL FPS 提取图像将其分成单独的帧，“ data_dst” 文件夹中的所有内容也只是视频的帧，您还需要声音，该声音取自 原始的 data_dst.mp4 文件。</p>
<p><strong>26. Q：我不小心删除了 data_dst.mp4 文件，无法恢复，仍然可以将合并 / 转换后的帧转换为 mp4 视频吗？</strong></p>
<p>A：是的，如果您已经永久删除了 data_dst.mp4，并且无法恢复它或呈现相同的文件，您仍然可以使用 ffmpeg 和适当的命令将其手动转换回 mp4（尽管没有声音）：</p>
<p>- start by going into folder …:_internalfmpeg and copy ffmpeg.exe - paste it into the merged folder - open up command line by pressing windows key + r (run) and typing cmd or searching it up after pressing windows key and typing cmd/cmd.exe - copy address of your merged folder (example: DFLworkspacedata_dstmerged) - in the command line type the letter of your drive, as in example above that would be “d:” (without quotation marks) and press enter - line D:&gt; should appear, next type “cd: FULL_ADDRESS”, example: “cd: D:workspacedata_dstmerged” - you should now see your entire address like this: DFLworkspacedata_dstmerged&gt; - enter this command:</p>
<p>ffmpeg -r xx -i %d.jpg -vcodec libx264 -crf 20 -pix_fmt yuv420p result.mp4</p>
<p>- xx is framerate - d is a number representing amount of numbers in the file name so if your merged frames have names like 15024.jpg that would be 5, if it’s 5235.jpg it is 4, etc. If your images are pngs, change .jpg to .png - crf is quality setting, best to be left at 20. If your merged file names have some letters in front like out12345.jpg add “out” before the % sign.</p>
<p>Example command for converting frames named “out_2315.png” into an 30 fps .mp4 file named “deepfake”.</p>
<p>ffmpeg -r 30 -i out%4.png -vcodec libx264 -crf 20 -pix_fmt yuv420p deepfake.mp4</p>
<p>If you want to use x265 encoding change libx264 to libx265.</p>
<p><strong>27. Q：您可以暂停合并，然后再恢复吗？ 您可以保存合并设置吗？ 我的合并失败 / 合并时出现错误，并且卡在％处，我可以再次启动它，然后从上一个成功合并的帧开始合并吗？</strong></p>
<p>A：是的，默认情况下，交互式转换器 / 合并在 “模型” 文件夹中创建会话文件，该文件同时保存进度和设置。</p>
<p>如果您只想暂停训练，则可以单击 &gt;，它将暂停。 但是，如果您需要将其完全关闭 / 重新启动 PC 等，则退出了与 esc 的合并，并等待它保存进度，下次选择合并 / 转换器（Y / N）后，下次启动合并时 - 是，您会 将会提示您是否要使用保存 / 会话文件并恢复进度，合并将在正确的框架处以正确的设置加载。</p>
<p>如果合并失败并且没有保存进度，则必须手动恢复它，方法是先备份 “data_dst” 文件夹，然后删除 data_dst 中所有提取的帧以及 “ aligned” 文件夹中的所有图像 在 “ data_dst” 内部，对应于已在文件夹 “ merged” 中转换 / 合并的帧。 然后，只需启动合并 / 转换器，输入之前使用的设置，然后转换其余帧，然后从备份的 “ data_dst” 文件夹中将新合并的帧与旧的合并，并照常转换为.mp4 即可。</p>
<p><strong>28. Q：训练期间预览中的面孔看起来不错，但转换后看起来很糟。 我看到了原始脸的一部分（下巴，眉毛，双脸轮廓）。</strong></p>
<p>A：预览中的面孔是 AI 的原始输出，然后需要在原始素材上进行合成。因此，当人脸形状不同或稍小 / 较大时，您可能会在 DFL 合并创建的蒙版周围 / 外部看到原始人脸的一部分。 要解决此问题，您需要更改转换设置，方法是：</p>
<p>- 调整遮罩类型</p>
<p>- 调整遮罩腐蚀（大小）和模糊（羽化，使边缘平滑）</p>
<p>- 调整脸部大小（比例）</p>
<p>注意：负腐蚀会增加面罩的尺寸（覆盖更多），正腐蚀会减小面罩的尺寸。</p>
<p><strong>29. Q：使用 “无缝” 模式时，最终结果 / 深层伪影具有怪异的伪影，面部变化的颜色，背景的颜色渗出，并使其在角落 / 边缘的颜色闪烁 / 变暗 / 更改颜色。</strong></p>
<p>A：您正在使用无缝 / 历史 / 无缝 + 历史叠加模式，或者使用具有变化光照条件的源数据集 / 面集训练了模型，并且在训练过程中未使用任何颜色转移。 - 使用覆盖或除无缝 / 历史 / 无缝 + 历史之外的任何其他模式 - 如果要使用无缝： - 减小遮罩 / 脸部的大小，以使其不会 “触摸” 外部区域，因此不会通过增加 “侵蚀遮罩” 值来获得脸部 / 头部外部的背景 / 区域的颜色。 - 或通过增加 “模糊蒙版”（Blur Mask）值来平滑蒙版 / 脸部的边缘，这可能会掩盖某些颜色变化，还有助于使脸部看起来更…“无缝”（当您减小蒙版大小时）。 如果仍然持续使用上述简单叠加模式，则这两种方法都可能会或可能不会解决问题。</p>
<p>如果您的源数据集包含具有不同光照条件的面部图像并且未使用颜色转移，则可能需要返回并继续启用颜色转移进行更多训练。 万一打开它会严重洗掉颜色或以不好的方式影响训练数据 / 面部的颜色（褪色，错误的颜色，过度饱和的颜色，噪点）或使学习的面部模糊（由于模型的变化太大）必须全面学习，好像源数据集和目标数据集中有新面孔一样），您可能需要保存界标数据并编辑源数据集颜色以更好地匹配目标数据集，并且变化较少。</p>
<p>我建议除非绝对必要，否则不要使用无缝模式，即使如此，我还是建议在每个主要角度和相机移动 / 光线变化时停下来看看它是否不会引起那些伪影。</p>
<p><strong>30. Q：半脸，半脸，全脸和全脸 face_type 模式有什么区别？</strong></p>
<p>A：全脸是覆盖整个脸部 / 头部的新模式，这意味着它也覆盖了整个额头，甚至覆盖了某些头发和其他特征，这些特征可能会被全脸模式剪切掉，并且在使用一半或一半时绝对不会出现脸部模式。它还在训练过程中带有新选项，让您训练称为 masked_training 的额头。首先，先启用它，然后将训练蒙版剪切到整个脸部区域，一旦对脸部进行了足够的训练，就禁用它并训练整个脸部 / 头部。此模式需要在后期手动屏蔽或训练自己的 XSeg 模型：<a target="_blank" rel="noopener" href="https://mrdeepfakes.com/forums/thread-gu...g-tutorial">https://mrdeepfakes.com/forums/thread-gu...g-tutorial</a></p>
<p>建议使用全脸 face_type 模式，以尽可能多地遮盖脸部，而无需多余的东西（发际线，额头和头部其他部位） 半脸模式是 H64 和 H128 模型中的默认 face_type 模式。它只覆盖一半的脸（从嘴到眉毛以下） 半脸是覆盖半脸约 30％区域的一种模式。</p>
<p><strong>31. Q：什么是最适合深度伪造的 GPU？ 我想升级我的 GPU，我应该得到哪一个？</strong></p>
<p>A：20 系最好，30 系等更新</p>
<p><strong>32. Q：AutoEncoder，Encoder，Decoder 和 D_Mask_Decoder 维度设置有什么作用？ 更改它们有什么作用？</strong></p>
<p>A：可以更改它们以提高性能或质量，将它们设置为高将使模型真的很难训练（缓慢，高使用 vram），但会提供更准确的结果和更多的 src，如外观，将其设置为低将提高性能但是结果将不太准确，并且模型可能无法学习人脸的某些特征，从而导致通用输出看起来更像 dst 或什么都不像 dst 或 src。 自动编码器尺寸（32-1024？：help）：这是学习的整体模型能力。 价值太低，将无法学习所有内容 - 更高的价值将使模型能够学习更多表达式，并且以性能为代价更加准确。</p>
<p>编码器尺寸（16-256？：help）：这会影响模型学习不同表情，面部状态，角度，照明条件的能力。 值太低，模型可能无法学习某些表达式，模型可能无法闭上眼睛，嘴巴，某些角度的细节可能不够准确，较高的值将导致模型更加准确和富有表现力，前提是 AE 昏暗程度会相应提高。性能成本。</p>
<p>解码器尺寸（16-256？：help）：这会影响模型学习精细细节，纹理，牙齿，眼睛的能力，这些微小的事物会使人的面部变得细腻且可识别。 值太低将导致无法学习某些细节（例如牙齿和眼睛看起来模糊，缺少纹理），也可能无法正确学习一些微妙的表情和面部特征 / 纹理，从而导致像面部表情一样的 src 更少，价值更高将使面部更加细化，模型将能够以性能为代价选择更多这些细微的细节。</p>
<p>解码器蒙版尺寸（16-256？：help）：在启用学习蒙版的情况下进行训练时，会影响学习的蒙版的质量。不影响培训质量。</p>
<p><strong>33. Q：推荐的批量大小是多少？ 我应该设置多大的批量大小？ 批量大小可以设置多低？</strong></p>
<p>A：没有建议的批量大小，但是合理的值在 8-12 之间，其中 16-22 以上的值非常好，最小 4-6 的值。 批次大小 2 不足以正确训练模型，因此建议的最小值为 4，值越大越好，但是在某些时候批次大小可能不利，尤其是在迭代时间开始增加或您有禁用 models_opt_on_gpu - 从而在 CPU 上强制优化器，这会减慢训练速度 / 增加迭代时间。 您可以通过将迭代时间除以批次大小来计算何时增加批次大小变得效率较低。选择可以在给定的迭代时间内为每个批次降低 ms 值的批次大小，例如：</p>
<p>批次 8-1000/8 = 1000 时 1000 ms 批处理 1500 毫秒 10-1500/10 = 150</p>
<p>在这种情况下，与批次 10 相比，在批次 8 中运行将在给定时间内提供更多的数据模型，但是差异很小。如果说我们要使用批处理 12，但得到一个 OOM - 因此我们禁用 models_opt_on_gpu，它现在看起来可能像这样： 批次 12 时为 2300 毫秒（CPU 上的 Optimizer）-2300/12 = 191 毫秒，这比批次 8 和迭代时间为 1000 毫秒的 128 毫秒长得多。</p>
<p>启动模型时，最好使用较小的批处理大小 - 较长的迭代时间，然后在禁用随机扭曲时增加它。</p>
<p><strong>34. Q：如何使用预训练模型？</strong></p>
<p>A：只需下载它，然后将所有文件直接放入模型文件夹即可。 开始训练，在选择要训练的模型（如果文件夹中还有更多）和要训练的设备（GPU / CPU）之后的 2 秒钟内按任意键，以覆盖模型设置并确保禁用预训练选项，以便正确启动 训练中，如果您启用了预训练选项，则模型将继续进行预训练。 请注意，模型会将迭代计数还原为 0，这是预训练模型的正常行为，这与不使用预训练功能而只是在随机面孔上训练的模型不同。</p>
<p><strong>35. Q：我的 GPU 使用率非常低，尽管选择了 GPU 进行训练 / 合并，也没有使用 GPU。</strong></p>
<p>A：它可能正在使用中，但是 Windows 不仅报告 CUDA 使用情况（这是您应该查看的），而且 GPU 的总使用情况可能会更低（大约 5-10％）。 要在培训期间（在 Windows 10 中）查看 CUDA / GPU 的真实使用情况，请进入任务管理器 -&gt; 性能 -&gt; 选择 GPU-&gt; 将 4 个较小的图形之一更改为 CUD。</p>
<p>如果您使用的是其他版本的 Windows，请下载外部监视软件（例如 HWmonitor 或 GPU-Z），或者查看 VRAM 的使用情况，该使用率应接近培训期间的最大值。</p>
<h1 id="转载来源"><a href="#转载来源" class="headerlink" title="转载来源"></a>转载来源</h1><p><a target="_blank" rel="noopener" href="http://dfldata.xyz/forum.php?mod=viewthread&tid=116&page=1&extra=#pid1515">【必看】DFL 官方使用说明【已汉化】 - 软件下载与教程 - Discuz! Board - Powered by Discuz!</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/01/25/%E8%AF%AD%E9%9F%B3%E5%AF%B9%E6%8A%97%E9%BB%91%E7%9B%92%E6%80%BB%E7%BB%93/" rel="prev" title="语音对抗黑盒方法">
                  <i class="fa fa-chevron-left"></i> 语音对抗黑盒方法
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/02/07/adversarial%20competition/adversarial%20competition/adversarial-competition/" rel="next" title="天池对抗鲁棒">
                  天池对抗鲁棒 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiqiang Gao</span>
</div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  <script src="/js/local-search.js"></script>















  








  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
